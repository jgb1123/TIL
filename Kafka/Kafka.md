# Kafka

## Kafka?
* Kafka는 LinkedIn에서 개발되었고, Apache Software Foundation에 기증되어 Apache Software Foundation에서 관리되고 있다.
* Kafka는 대량의 데이터를 안정적으로 신속하게 처리할 수 있는 분산 스트리밍 플랫폼으로 사용된다.
* Kafka는 Topic이라는 개념을 사용하여 데이터를 구조화하고, Producer가 데이터를 특정 토픽에 전송하면 Consummer가 해당 토픽에서 데이터를 소비할 수 있다.
* 이러한 특징을 통해 데이터의 효율적인 분산 처리와 확장이 가능하다.

### vs RabbitMQ
* Kafka와 RabbitMQ 모두 메시지 브로커 시스템으로, 분산 시스템에서 메시지를 안정적으로 처리하고 전달하는데 사용한다.
* RabbitMQ는 AMQP(Advanced Message Queuing Protocol)를 준수하는 메시지 지향 미들웨어로 메시지를 안적적으로 전달하고 처리하기 위해 설계되었다.
* 그에 반면 Kafka는 대량의 데이터를 실시간으로 처리하고 분산 시스템에서 데이터를 Pub/Sub 하는데 중점을 두었다.
* 또한 RabbitMQ는 메시지를 큐에 저장하고 컨슈머에게 전달하여 처리한 후 해당 메시지를 삭제한다. (메시지는 일반적으로 디스크에 영속적으로 저장되지 않으며 설정은 가능하다)
* Kafka는 메시지를 처리한 후에도 메시지를 영구적으로 유지하기 떄문에, 대용량 분산 시스템에서 안정적으로 데이터를 처리할 수 있다.

## Kafka 활용
### 데이터 스트리밍
* 대용량의 데이터를 실시간으로 처리하며, 이벤트 기반 아키텍처를 구현할 때 사용된다.

### 로그 수집
* 여러 소스에서 발생한 로그 데이터를 중앙집계하고 저장하기 위해 사용된다.

### 메시지 큐
* 분산 시스템 간의 통신을 위해 사용된다.
* Producer가 메시지를 생성하고 Consumer가 메시지를 소비하는 메시지 큐 시스템으로 활용된다.

### 이벤트 소싱
* 시스템 내에서 발생하는 모든 이벤트를 저장하고 분석하기 위해 사용된다.

## Cluster
* Kafka Cluster는 Apache Kafka의 분산형 메시징 시스템을 구성하는 서버 집합이다.
* Cluster는 여러 대의 서버로 구성되어 있고, 데이터를 안정적으로 저장하고 처리하며 확장성을 제공한다.
* Kafka는 고가용성을 위해 여러개의 Kafka 서버와 Zookeeper로 구성된 클러스토 구조로 사용된다.
  * 일반적으로 3개 이상의 Kafka서버로 구성하며, Broker에 저장된 메시지를 다른 Broker에게 공유하고 하나의 브로커에 문제가 생겼을 때 다른 브로커로 그 역할을 대체해서 시스템을 정상적으로 유지한다. 
* Zookeeper는 분산 애플리케이션의 데이터 관리 기능을 가지고 있고, 여러 개의 Broker들을 컨트롤해 주는 역할을 수행한다.

> KIP-500
>
> * Zookeeper로  두개의 시스템을 관리해야 했고, 설정 및 운영하는데 필요한 작업이 더 많아지며, 동시에 유지보수 해야 하며 시스템의 복잡성을 증기시킨다.
> * 또한 메타데이터를 외부에서 모으는 것은 별로 효과적이지 못하며, 카프카 클러스터만큼 많은 Zookeeper 노드를 사용해야 하는 경우도 있었다.    
> * 메타데이터를 외부에 저장하면 컨트롤러의 in memory 상태가 외부로부터 비동기화될 수 있다. (클러스터에 있는 컨트롤러의 실시간 시점은 Zookeeper의 시점과 다를 수 있음)
> * 이러한 문제들로 인해 KIP-500이 등장했으며ㅓ, KIP-500은 Kafka Improvement Proposal의 하나로, Kafka의 Zookeeper의존성을 완전히 제거하고 Kafka를 단일 시스템으로 만들기 위한 목적으로 제안되었다.

## Topic
* 토픽은 데이터를 구분하고 분류하기 위한 논리적인 단위이며, 데이터의 목적에 따라 선택된다.
* 각 토픽은 하나 이상의 파티션으로 나뉠 수 있다.
  * 각 파티션은 데이터를 분산하여 여러 브로커에게 저장하고 병렬로 처리하는데 사용된다.
* 토픽은 데이터를 생성하는 Producer와 데이터를 소비하는 Consumer 간 중간 매개체로 사용된다.
  * Producer는 데이터를 토픽으로 전송하고, Consumer는 해당 토픽에서 데이터를 소비한다.
* Kafka 클러스터의 관리자는 토픽을 생성하고 파티션 수, 복제 수 등을 설정할 수 있다.

### Partitions
* 토픽은 하나 이상의 파티션으로 나뉠 수 있다.
* 각 파티션은 일련의 순차적인 메시지를 포함하며, 이러한 파티션은 여러 브로커에 걸쳐 분산된다.
* 각 파티션은 병렬로 처리될 수 있기 때문에 데이터의 처리량과 성능을 향상시킨다.

### Replication
* 각 파티션은 여러 브로커에 복제될 수 있다.
* 고가용성을 제공하며 특정 브로커가 다운되더라도 해당 파티션의 데이터를 다른 복제본에서 읽을 수 있게 한다.

### Retention Policy
* 토픽은 보통 데이터 보존 정책을 가지고 있다.
* 데이터가 얼마나 오래 유지되어야 하는지를 나타내며, 시간 기반 또는 데이터 크기 기반으로 설정할 수 있다.

### Partition and Leader
* 각 파티션은 여러 브로커 중 하나에 Leader가 할당된다.
* 리더는 쓰기 작업을 담당하며, 다른 브로커들은 해당 리더로부터 데이터를 동기화하여 복제한다.

## Producer API
* Producer는 Kafka에서 데이터를 생성하고 Kafka 클러스터로 전송하는데 사용되는 인터페이스이다.
* Producer는 애플리케이션이 데이터를 생성하고 Kafka 토픽으로 전송하는 역할을 한다.
  * 데이터는 일반적으로 메시지, 이벤트, 레코드 등의 형태를 가지며 Kafka 클러스터에 전달된다.
* Producer는 생성한 데이터를 하나 이상의 토픽에 전송한다.
* 주로 비동기적으로 데이터를 전송하며, 데이터를 브로커에게 보내고 즉시 제어권을 반환하여 다른 작업을 수행할 수 있다.
  * 효율적인 전송을 위해 메시지를 일괄로 묶어서 전송할 수 있으며, 작은 메시지를 하나로 묶어 전송함으로써 네트워크 및 시스템 리소스를 효율적으로 사용할 수 있다.
* Producer는 메시지 전송 중 발생할 수 있는 에러를 처리할 수 있으며, 이를통해 안정적인 데이터 전송이 가능하고 메시지 손실을 최소화할 수 있다.
* Producer는 여러 브로커로 구성된 Kafka 클러스터에 대한 확장성을 지원하기 때문에 대량의 데이터를 처리하고 처리량을 증가시킬 수 있다.

## Consumer API 
* Kafka에서 데이터를 소비하고 Kafka 토픽으로부터 메시지를 읽어오는데 사용되는 인터페이스이다.
* Kafka 클러스터부터로 데이터를 가져와 애플리케이션에서 소비하거나 처리하는 역할을 한다.
* Consumer는 특정 토픽이나 토픽 그룹에서 메시지를 읽어오며 다수의 Consumer 인스턴스를 사용하여 병렬로 데이터를 처리할 수 있다.
* 데이터를 처리하는 방식은 주로 비동기적으로 이루어지며, 브로커로부터 메시지를 효과적으로 읽어오기 위해 최적화된 방식으로 동작한다.
  * 읽어온 메시지의 위치는 오프셋을 통해 추적되어 처리된 메시지를 관리할 수 있다.
* Consumer는 메시지 소비 중에 발생할 수 있는 에러를 처리할 수 있으며, 이를 통해 안정적인 데이터 소비 및 처리가 가능하다.
* Consumer는 여러 브로커로 구성된 Kafka 클러스터에 대한 확장성을 지원하기 때문에 대량의 데이터를 효과적으로 처리하고 실시간으로 데이터를 수신할 수 있다.

## Stream API
* Kafka Stream API는 Kafka를 사용하여 데이터 스트림을 처리하기 위한 라이브러리 및 API의 집합이다.
* Kafka Stream API는 이벤트 스트림을 기반으로 하며, 데이터가 순차적으로 도착하는 스트림에서 작업을 수행한다.
  * 데이터를 개별적으로 처리하고 그 결과를 다시 Kafka 토픽에 전달할 수 있다.
* Kafak Stream API는 높은 수준으로 추상화된 API를 제공하여 개발자가 쉽게 스트림처리 애플리케이션을 작성할 수 있도록 도와준다.
  * 함수형 프로그래밍 스타일을 지원하며, 필요한 경우 DSL(Domain-Specific Language)을 사용할 수도 있다.
* Kafka Stream은 내부적으로 상태 저장을 관리하여 이전 이벤트에 기반하여 실시간으로 상태를 업데이트할 수 있다.
  * 애플리케이션이 상태를 유지하고 이를 기반으로 더 복잡한 처리를 수행할 수 있도록 해준다.
* 또한 Kafka Stream은 메시지 처리에서 Exactly Once 시맨틱(Semantic)을 지원한다.
  * 데이터 처리에서 중복, 손실 또는 순서 오류를 방지하기위해 보장되는 속성을 의미한다.

## Connect API
* Kafka Connect API는 Kafka에서 제공하는 데이터 통합 서비스이다.
* Connect를 통해 Kafka와 다른 데이터 시스템 간에 데이터를 스트리밍하고, 대규모 데이터를 이동시켜주는 커넥터를 빠르게 생성할 수 있다.
  * Source Connector : Data Source의 데이터를 Kafka Topic에 보내는 역할을 하는 커넥터이다.
  * Sink Connector : Kafka Topic에 담긴 데이터를 특정 Data Source로 보내는 역할을 하는 커넥터이다.
* Connect는 데이터 및 일회성 작업을 위한 standalone mode로 실행할 수도 있고, 대규모 운영환경을 위한 distributed mode로 실행할 수도 있다.
  * distributed mode로 실행하면 워커노드의 장애 상황에서도 유연하게 대응할 수 있으므로 고가용성이 보장된다.
* 따라서 Kafka Connect를 통해 대량의 데이터를 신속하고 안정적이며 확장 가능하게 이동시킬 수 있다.

## 신뢰성
### Producer Acks 종류
#### acks = 0
* Producer는 자신이 보낸 메시지에 대해 카프카로부터 확인을 기다리지 않는다.
* 메시지가 카프카에 성공적으로 전송되었는지 확인하지 않는다.

#### acks = 1
* Producer는 자신이 보낸 메시지에 대해 카프카의 leader가 메시지를 받았는지 기다린다.
* follower들은 확인하지 않고, leader가 확인 응답을 보내고 follower에게 복제가 되기 전 leader가 실패하면 해당 메시지는 손실될 수 있다.

#### acks = all(-1)
* Producer는 자신이 보낸 메시지에 대해 카프카의 leader와 follower까지 받았는지 기다린다.
* 최소 하나의 복제본까지 처리가 된 것을 확인하기 때문에 메시지가 손실될 일이 거의 없다.

### Idempotence(멱등성) producer
* Producer는 Record 단위로 메시지를 발행하여 Acks(Acknowledgements)를 받는 작업을 atomic하게 수행한다.
* Consumer는 Partition offset으로부터 Record 단위로 데이터를 읽고 변경된 offset을 commit하는 작업을 atomic하게 수행한다.
* 만약 네트워크 장애로 인해 Ack가 유실된다면 Ack가 유실되는 수 만큼 레코드를 중복 적재하게 된다.
* 이러한 문제로 데이터가 중복 적재되는 것을 막기 위해 enable.idempotence옵션을 제공하며, Kafka3.0버전 이후부터는 default 값이 true로 설정된다.
  ```java
  properties.put(ProducerConfig.ENABLE_IDEMPOTENCE_CONFIG, true);
  ```
* Idempotence producer는 Record를 Broker로 전송할 때 PID(Producer unique Id)와 Seq(Sequence number)를 함께 전달한다.
* Broker는 PID와 Seq를 가지고 중복된 레코드가 오면 무시하게 된다.

### Transaction Producer
* Consumer가 또 다른 Producer가 되어 여러 Topic에 메시지를 발행할 수 있다.
  * Producer가 이벤트를 발행하고 Consumer에서 특정 이벤트를 받은 후, Comsumer 쪽에서 또 다른 이벤트를 발행할 수 있다.
* 이러한 상황에서 데이터를 처리하고 offset commit을 하기 전에 장애가 발생하면,오프셋이 커밋되지 않은 채로 애플리케이션이 종료될 수 있다.
* 그러면 다시 동작할 때 이미 전달한 레코드를 중복하여 처리하게 된다.
* 이러한 문제를 해결하기 위해선 Offset commit과 레코드 전달을 하나로 묶어야 한다.
* Transaction Producer를 통해 Consumer가 offset commit하지 않고 Producer가 트랜잭션 내부에서 커밋하도록 한다.
* 이러한 기능을 사용하기 위해선 Auto Commit Config를 false로 설정해야 한다.
  ```java
  properties.put(ConsumerConfig.ENABLE_AUTO_COMMIT_CONFIG, false);
  ```
  
### Idempotence(멱등성) Consumer
#### Unique Key 활용
* 데이터의 unique key를 활용하여 중복 적재를 방지한다.
* 저장소에서는 데이터의 유일성을 보장할 수 있는 기능이 필요하다.

#### Upsert 활용
* 중간 결과값들을 저장소에 저장하고 최종 결과값으로 Update하는 방식이다.
* 중간 단계에서 발생한 데이터 중복이 최종 결과에 영향을 미치지 않도록 하며, 최종적으로 최신 데이터만이 저장되도록 한다.

#### Write-ahead log 활용
* 트랜잭션이 커밋되기전에 WAL에 미리 기록하여 적재 과정을 Atomic하게 보관한다.
* 중복 적재를 검사할 때 로그 파일과 Topic offset들을 확인해야 하기 떄문에 로직이 복잡해질 수 있다.
