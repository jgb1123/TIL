# SQL 처리과정과 I/O
## SQL 파싱과 최적화
### SQL?
* SQL은 Structured Query Language 의 줄임말이다.
* SQL은 깁노적으로 구조적(structured)이고, 집합적(set-based)이고, 선언적(declarative)인 질의 언어이다.
* 원하는 결과 집합을 구조적, 집합적으로 선언하지만 그 결과 집합을 만드는 과정은 절차적일 수 밖에 없다.
* 결국 프로시저가 필요한데, 그 프로시저를 만들어내는 DBMS 내부 엔진이 바로 SQL 옵티마이저이다.
* DBMS 내에서 프로시저를 작성하고 컴파일해 실행 가능한 상태로 만드는 모든 과정을 SQL 최적화라고 한다.


### SQL 최적화 과정
1) SQL 파싱
   * 사용자로부터 SQL을 전달받으면, 가장 먼저 SQL 파서가 파싱을 진행한다.
   * 파싱 트리 생성 : SQL문을 이루는 개별 구성 요소를 분석해 파싱 트리를 생성한다.
   * Syntax 체크 : 문법적 오류가 없는지 확인한다.
   * Semantic 체크 : 의미상 오류가 없는지 확인한다.

2) SQL 최적화
   * SQL최적화는 옵티마이저가 진행한다.
   * 옵티마이저가 미리 수집한 시스템 및 오브젝트 통계정보를 바탕으로 실행 경로를 생성해 비교한 후 가장 효울적인 하나를 선택한다.
3) 로우 소스 생성
   * SQL 옵티마이저가 선택한 실행 경로를 실행가능한 코드나 프로시저 형태로 포맷팅한다.
   * 로우 소스 생성기(Row-Source Generator)가 그 역할을 맡는다.

### SQL 옵티마이저
* SQL옵티마이저는 사용자가 원하는 작업을 가장 효율적으로 수행할 수 있는 최적의 데이터 액세스 경로를 선택해주는 DBMS의 핵심 엔진이다.
1) 사용자로부터 전달받은 쿼리를 수행하는 데 후보군이 될만한 실행계획들을 찾아낸다.
2) 데이터 딕셔너리에 미리 수집해 둔 오브젝트 통계 및 시스템 통계정보를 이용해 각 실행계획의 예상 비용을 산정한다.
3) 최저 비용을 나타내는 실행 계획을 선택한다.

### 실행계획과 비용
* DBMS에 SQL 실행경로 미리 보기 기능이 있는데, 실행 계획(Execution Plan)이다.
* 이를 통해 자신이 작성한 SQL이 테이블을 스캔하는지, 인덱스를 스캔하는지, 인덱스를 스캔한다면 어떤 인덱스인지를 확인할 수 있으며, 예상과 다른 방식으로 처리되면 실행경로를 변경할 수 있다.
* SQL 실행계획에 표시되는 Cost도 예상치이며, 실행경로를 선택하기 위해 옵티마이저가 여러 통계 정보를 활용해 계산해낸 값이다.

### 옵티마이저 힌트
* 옵티마이저가 보통 좋은 선택을 하지만, 그 선택이 최선은 아니다.
* 옵티마이저 힌트를 이용해 데이터 액세스 경로를 바꿀 수 있다.
* 중요한 시스템이라면, 옵티마이저의 판단에 맡기긴 어렵지만, 힌트를 쓸거면 빈틈없이 써야 한다.
```oraclesqlplus
/*+ INDEX(T1 TEST_PK) */
```
#### 주의사항
* `--+ INDEX(A TEST_PK)`방식은 가급적 쓰지 말아야 한다.
* 인자를 나열할 땐 `,`를 사용할 수 있지만 힌트와 힌트 사이에 사용하면 안된다.
* 테이블을 지정할 때 스키마 명까지 명시하면 안된다.
* FROM절 옆에 ALIAS를 지정했으면 힌트에도 반드시 ALIAS를 사용해야 한다.

## SQL 공유 및 재사용
### 소프트 파싱 vs 하드 파싱
* SQL 파싱, 최적화, 로우소스 생성 과정을 거쳐 생성한 내부 프로시저를 반복해서 재사용할 수 있도록 캐싱해두는 메모리 공간을 **라이브러리 캐시**라고 한다.
  * 라이브러리 캐시는 SGA의 구성 요소이다.
    > SGA(System Global Area)
    >* 서버 프로세스와 백그라운드 프로세스가 공통으로 액세스하는 데이터와 제어 구조를 캐싱하는 메모리 공간
* 사용자가 SQL문을 전달하면 DBMS는 SQL을 파싱한 후 해당 SQL이 라이브러리 캐시에 존재하는지부터 확인한다.
* SQL을 캐시에서 찾아 바로 실행단계로 넘어가는것을 **소프트파싱** 이라고 한다.
* 찾는데 실패 해 최적화 및 로우 소스 생성 단계까지 모두 거치는것을 **하드파싱** 이라고 한다.
  * 하드파싱은 CPU를 많이 소비하는 몇 안되는 작업 중 하나이다.
  * 쿼리를 수행할 때, 후보군이 될만한 많은 실행경로를 도출하고, 딕셔너리와 통계정보를 읽어 각각의 효율성을 판단하는 과정은 결코 가볍지 않다.

### 바인드 변수의 중요성
#### 이름이 없는 SQL 문제
* 사용자 정의 함수/프로시저, 트리거, 패키지 등은 생성할 때 부터 이름을 갖기 때문에, 컴파일한 상태로 딕셔너리에 영구적으로 보관된다. 
* SQL은 이름이 따로 없고, 전체 텍스트가 이름 역할을 한다.
#### 공유 가능 SQL
* 바인드 변수를 사용하지 않으면, DBMS에서 발생하는 부하는 대개 I/O가 원인인데, I/O가 거의 발생하지 않음에도 CPU사용률은 급격히 올라갈 수 있다. (SQL 하드파싱 문제)
* 따라서 바인드 변수를 사용해 파라미터 Driven 방식으로 SQL을 작성하면 이러한 하드파싱 문제를 방지할 수 있다.

## 데이터 저장 구조 및 I/O 메커니즘
* I/O 튜닝이 곧 SQL 튜닝이라 해도 과언이 아니다.
* 즉 SQL튜닝 원래를 제대로 이해하려면 I/O에 대한 이해가 필수적이다.

### SQL이 느린 이유
* I/O는 잠 이라고 설명할 수 있다.
* I/O를 처리하는 동안 프로세스는 잠을 자기 때문이다.
* 프로세스가 잠을 자는 이유는 여러가지가 있지만, I/O가 가장 대표적이고 많은 비중을 차지한다.
* interrupt 없이 일하는 프로세스도 디스크에서 데이터를 읽어야 할 땐 CPU를 OS에 반환하고 waiting 상태에서 I/O가 완료되기를 기다린다.
  * CPU를 반환한 채 대기 큐에서 잠을 자는 것이다.
* 열심히 일해야 할 프로세스가 잠을 자게 되므로, I/O가 많으면 성능이 느릴 수 밖에 없다.
* SQL이 느린 이유는 여기에 있으며, 즉 디스크 I/O 때문이다.

### 데이터베이스 저장 구조
* 데이터를 저장하려면 **테이블 스페이스**를 생성해야 한다.
  * 테이플 스페이스는 세그먼트를 담는 콘테이너로, 여러 개의 데이터파일로 구성된다.
* 테이플스페이스를 생성했으면, **세그먼트**를 생성한다.
  * 세그먼트는 테이블, 인덱스 처럼 데이터 저장공간이 필요한 오브젝트이다.
    * 따라서 테이블, 인덱스를 생성할 때 데이터를 어떤 테이블 스페이스에 저장할지를 지정한다.
  * 파티션 구조가 아니라면 테이블도 하나의 세그먼트, 인덱스도 하나의 세그먼트이다.
  * 파티션구조라면 각 파티션이 하나의 세그먼트가 된다.
  * LOB컬럼은 그 자체가 하나의 세그먼트를 구성하므로 자신이 속한 테이블과 다른 공간에 값을 저장한다. (작은 LOB는 설정에 따라 in-row 저장 가능)
* 세그먼트는 여러 **익스텐트**로 구성된다.
  * 익스텐트는 공간을 확장하는 단위이다.
  * 테이블이나 인덱스에 데이터를 입력하다 공간이 부족해지면 오브젝트가 속한 테이블 스페이스로부터 익스텐트를 추가로 할당받는다.
  * 익스텐트는 연속된 블록들의 집합이기도 하다.
* 익스텐트 단위로 공간을 확장하지만, 레코드를 실제로 저장하는 공간은 **데이터 블록**이다.
  * 한 블록은 하나의 테이블이 독점한다.
  * 즉, 한 블록에 저장된 레코드는 같은 테이블 레코드이다.
  * 추가로, 한 익스텐트도 하나의 테이블이 독점한다.
  * 즉, 한 익스텐트에 담긴 블록은 모두 같은 테이블 블록이다. (MS-SQL은 한 익스텐트를 여러 오브젝트가 사용할 수도 있음)
* 세그먼트 공간이 부족해지면 테이블 스페이스로부터 익스텐트를 추가로 할당받지만, 세그먼트에 할당된 모든 익스텐트가 같은 데이터파일에 위치하지 않을 수도 있다.
  * 서로 다른 데이터 파일에 위치할 가능성이 더 높다.
  * 하나의 테이블스페이스를 여러 데이터파일로 구 성하면 파일 경합을 줄이기 위해 DBMS가 가능한 여러 데이터로 분산해 저장하기 때문이다.

#### DBA(Data Block Address)
* 데이터 블록은 디스크 상에서 몇 번 데이터 파일의 몇 번째 블록인지를 나타내는 고유 주소값(DBA)을 갖는다.
* 데이터를 읽으려면 DBA부터 확인해야 한다.
* 인덱스를 이용해 테이블 레코드를 읽을 때에는 인덱스 ROWID를 이용한다.
* ROWID 는 DBA + 로우 번호(블록 내 순번)으로 구성되므로, 이를 통해 읽어야 할 테이블 레코드가 저장된 DBA를 알 수 있다.
* 테이블 스캔 시에는 테이블 세그먼트 헤더에 저장된 익스텐트 맵을 이용한다.
* 익스텐트 맵을 통해 각 익스텐트의 첫 번쨰 블록 DBA를 알 수 있다.
* 즉, 익스텐트는 연속된 블록의 집합이므로 테이블을 스캔할 때에는 첫 번쨰 블록 뒤에서 연속해서 저장된 블록을 읽으면 된다.

### 블록 단위 I/O
* 데이터베이스에서 데이터를 읽고 쓰는 단위는 블록이다.
* 데이터 I/O 단위가 블록이므로, 특정 레코드를 하나 읽고 싶어도 해당 블록을 통째로 읽는다.
* 오라클은 기본적으로 8KB 크기의 블록을 사용한다. (1Byte를 읽기 위해 8KB를 읽는다)

### 시퀀셜 액세스 vs 랜덤 액세스
#### 시퀀셜 액세스
* 시퀀셜 액세스는 논리적또는 물리적으로 연결된 순서에 따라 차례대로 블록을 읽는 방식이다.
* 인덱스 리프 블록은 앞뒤를 가리키는 주소값을 통해 논리적으로 연결되어 있다.
  * 이 주소값에 따라 앞 또는 뒤로 순차적으로 스캔하는 방식이 시퀀셜 액세스이다.
* 테이블 블록 간에는 논리적인 연결고리를 갖고있지 않다.
  * 세그먼트에 할당된 익스텐트 목록을 세그먼트 헤더에 맵으로 관리한다.
  * 이 익스텐트 맵은 첫 번째 블록 주소 값을 갖는다.
  * 따라서 읽어야 할 익스텐트 목록을 익스텐트 맵에서 얻고, 각 익스텐트의 첫 번째 블록 뒤에 연속해서 저장된 블록을 순서대로 읽으면, 바로 Full Table Scan이다.

#### 랜덤 액세스
* 랜덤 액세스는 논리적, 물리적 순서를 따르지 않고 레코드 하나를 읽기 위해 한 블록씩 접근하는 방식이다.

### 논리적 I/O vs 물리적 I/O
#### DB 버퍼캐시
* SGA 구성요소로 라이브러리 캐시가 있는데, 데이터를 캐싱하는 DB 버퍼캐시도 SGA의 중요한 구성요소중 하나이다.
* 라이브러리 캐시가 SQL과 실행계획, DB 저장형 함수/프로시저 등을 캐싱하는 코드캐시라고 한다면, DB버퍼캐시는 데이터 캐시라고 할 수 있다.
* 디스크에서 읽은 데이터 블록을 캐싱해 같은 블록에 대한 반복적인 I/O Call을 줄이는데 목적이 있다.
* 서버 프로세스와 데이터 파일 사이에 버퍼 캐시가 있으므로, 데이터 블록을 읽을 때에는 항상 버퍼캐시부터 탐색한다.
  * 버퍼캐시는 공유메모리 영역이므로, 같은 블록을 읽는 다른 프로세스도 득을 본다.


#### 논리적 I/O vs 물리적 I/O
* 논리적 블록 I/O는 SQL을 처리하는 과정에 발생한 총 블록 I/O를 말한다.
* 물리적 블록 I/O는 디스크에서 발생한 총 블록 I/O를 말한다.
  * 읽어야 할 블록을 버퍼캐시에서 찾지 못할 때만 디스크를 액세스 하므로, 논리적블록 I/O의 일부가 물리적 I/O이다.
* 메모리 I/O는 전기적신호이지만, 디스크 I/O는 액세스 암을 통해 물리적 작용이 일어나므로 메모리 I/O에 비해 상당히 느리다. (보통 10000배 느림)
* Direct Path Read방식으로 읽는 경우를 제외하면, 모든 블록은 DB 버퍼캐시를 경유해서 읽는다.
* 즉, 일반적으로 논리적 I/O횟수는 DB버퍼캐시에서 블록을 읽은 횟수와 일치한다.
* DB버퍼캐시에서 블록을 찾지 못해 디스크에서 읽은 블록 I/O가 물리적 I/O이다.


#### 버퍼캐시 히트율
* 버퍼캐시 효율을 측정하는데 가장 많이 사용해 온 지표는 버퍼캐시 히트율(BCHR)이다.
 ```
BCHR = (캐시에서 찾은 블록 수 / 총 읽은 블록 수 ) * 100
     = ((논리적 I/O - 물리적 I/O) / 논리적 I/O) * 100
     = (1 - (물리적 I/O) / (논리적 I/O)) * 100
 ```
* 애플리케이션 특성에 따라 다르지만, 온라인 트랜잭션을 주로 처리하는 애플리케이션이라면 시스템 레벨에서 평균 99% 히트율을 달성해야 한다.
  * 핵심 트랜잭션이 시스템 전체 부하의 대부분을 차지하므로, 열심히 튜닝하면 99%는 달성하기 어려운 수치가 아니다.
* 논리적 I/O는 일정하므로, 물리적 I/O 는 버퍼캐시 히트율에 의해 결정된다.
* 결국 SQL 성능을 높이기 위해 할 수 있는 일은 논리적 I/O를 줄이는 일이다.
* 따라서 SQL을 튜닝해서 읽는 총 블록 수를 줄이면 된다.
* 논리적 I/O를 줄임으로써 물리적 I/O를 줄이는 것이 SQL 튜닝이다.
* 버퍼캐시 히트율이 SQL 성능을 좌우하지만, 높다고해서 효율적인 SQL을 의미하진 않는다.
  * 같은 블록을 반복해서 읽는 비효율적인 일이었을 수 있다.

### Single Block I/O vs Multiblock I/O
* 한 번에 한 블록씩 요청해서 메모리에 적재하는 방식을 Single Block I/O라고 한다.
* 한번에 여러 블록을 요청해서 메모리에 적재하는 방식을 Bultiblock I/O라고 한다.
* 인덱스를 이용할 때에는 기본적으로 인덱스와 테이블 블록 모두 Single Block I/O방식을 사용한다.
  * 인덱스 루트 블록을 읽을 때
  * 인덱스 루트 블록에서 얻은 주소 정보로 브랜치 블록을 읽을 때
  * 인덱스 브랜치 블록에서 얻은 주소 정보로 리프 블록을 읽을 때
  * 인덱스 리프 블록에서 얻은 주소 정보로 테이블 블록을 읽을 때
* 많은 데이터 블록을 읽을 때는 Multiblock I/O 방식이 효율적이다.
* Multiblock I/O는 캐시에서 찾지 못한 특정 블록을 읽으려고 디스크 상에 그 블록과 인접함 블록들을 한꺼번에 읽어 캐시에 미리 적재하는 기능이다.
* DBMS의 블록사이즈가 얼마건 간에 OS단에서는 일반적으로 1MB 단위로 I/O를 수행한다. (OS마다 다르다)
  * 오라클 I/O단위가 8KB이므로, db_file_multiblock_read_count를 128로 설정하면 최대가 된다. (8KB * 128)


### Table Full Scan vs Index Range Scan
* Table Full Scan은 테이블에 속한 블록 전체를 읽어 사용자가 원하는 데이터를 찾는 방식이다.
  * Table Full Scan은 시퀀셜 액세스와 Multiblock I/O 방식으로 디스크 블록을 읽는다.
* Index Range Scan은 인덱스에서 일정량을 스캔하면서 얻은 ROWID로 테이블 레코드를 찾아가는 방식이다.
  * Index Range Scan은 랜덤 액세스와 Single Block I/O방식으로 디스크 블록을 읽는다.
* Table Full Scan은 스토리지 스캔 성능이 좋아지는 만큼 성능도 좋아진다.
* Index Range Scan은 스토리지 성능이 많이 좋아져도 성능이 조금만 좋아진다.
* Table Full Scan을 피해야 한다는 인식과 달리 인덱스가 SQL성능을 떨어뜨리는 경우도 많다.
  * 한번에 많은 데이터를 처리하는 집계용 SQL과 배치프로그램이 그렇고, 이 경우엔 오히려 Full Scan으로 유도하면 성능이 좋아진다.
  * 물론 큰 테이블에서 소량의 데이터를 검색할 때에는 반드시 인덱스를 이용해야 한다.
* 인덱스는 굉장히 중요하긴 하지만, 항상 옳은 것은 아니므로 모든 성능문제를 인덱스로 해결하려고 하면 안된다.

### 캐시 탐색 메커니즘
* Direct Path I/O를 제외한 모든 블록 I/O는 메모리 버퍼캐시를 경유한다.
  * 인덱스 루트 블록을 읽을 때
  * 인덱스 루트 블록에서 얻은 주소 정보로 브랜치 블록을 읽을 때
  * 인덱스 브랜치 블록에서 얻은 주소 정보로 리프 블록을 읽을 때
  * 인덱스 리프 블록에서 얻은 주소 정보로 테이블 블록을 읽을 때
  * 테이블 블록을 Full Scan 할 때
* 버퍼 캐시에서 블록을 찾을 때 해시 알고리즘으로 버퍼 헤더를 찾고, 거기서 얻은 포인터로 버퍼 블록을 액세스하는 방식을 사용한다.
  * 같은 입력값은 항상 동일한 해시 체인에 연결됨
  * 다른 입력값이 동일한 해시 체인에 연결될 수 있음
  * 해시 체인 내에서는 정렬이 보장되지 않음
* 버퍼캐시에 캐싱된 버퍼블록은 모두 공유자원이기 때문에, 동시에 두 개 이상 프로세스가 접근하려고 할 때 정합성에 문제가 생길 수 있다.
* 따라서 한 프로세스씩 순차적으로 접근하도록 구현해야 하며, 이를 위한 직렬화 메커니즘이 필요한데, 이를 지원하는 메커니즘이 래치이다.
> 캐시버퍼 체인 래치
> * 대량의 데이터를 읽을 때 모든 블록에 대해 해시 체인을 탐색한다.
> * DBA를 해시 함수에 입력하고 반환된 값으로 스캔해야 할 해시 체인을 찾는다.
> * 해시체인에는 키를 획득한 프로세스만이 체인으로 진입할 수 있다.
* 버퍼캐시에는 캐시버퍼 체인 래치, 캐시버퍼 LRU 체인 래치 등이 작동한다.
* 캐시I/O도 생각보다 빠르지 않을 수 있는데, 이 래치에 의한 경합이 생길 수 있기 때문이다.
* 캐시버퍼 체인 뿐 아니라 버퍼블록 자체에도 버퍼 Lock이라는 직렬화 메커니즘이 존재한다.
* 따라서 이러한 직렬화 메커니즘에 의한 캐시 경합을 줄이려면 SQL튜닝을 통해 논리적 I/O 자체를 줄여야 한다.

> 버퍼 Lock
> * 래치를 해제한 상태로 버퍼 블록 데이터를 읽고 쓰는 경우 같은 블록에 접근할 수 있다.
> * 같은 블록에 접근해 데이터를 읽고 쓴다면 데이터 정합성에 문제가 생길 수 있다.
> * 이를 방지하기 위해 오라클은 버퍼 Lock을 사용한다.
> * 캐시버퍼 체인 래치를 해제하기 전 버퍼 헤더에 Lock을 설정함으로써 버퍼블록 자체에 대한 직렬화 문제를 해결한다.

# 인덱스 기본
## 인덱스 구조 및 탐색
### 미리보는 인덱스 튜닝
#### 데이터를 찾는 두 가지 방법
* 데이터베이스 테이블에서 데이터를 찾는 방법은 두가지 이다.
  * 테이블 전체를 스캔한다.
  * 인덱스를 이용한다.
#### 인덱스 튜닝의 두 가지 핵심 요소
* 인덱스는 큰 테이블에서 소량 데이터를 검색할 때 사용한다.
* 온라인 트랜잭션 처리 시스템에서는 소량 데이터를 주로 검색하므로, 인덱스 튜닝이 무엇보다 중요하다.
* 인덱스 튜닝 방법은 여러가지가 있지만, 크게 두 가지로 나뉜다.
  * 인덱스 스캔 과정에서 발생하는 비효율을 줄이는 것으로, 인덱스 스캔 효울화 튜닝이다.
  * 테이블 액세스 횟수를 줄이는 것으로, 랜덤 액세스 최소화 튜닝이다. (인덱스 스캔 후 레코드를 액세스할 때 랜덤 I/O 방식 사용)
* 인덱스 스캔 효울화 튜닝과 랜덤 액세스 최소화 튜닝 모두 중요하지만, 랜덤 액세스 최소화 튜닝이 성능에 미치는 영향이 더 크다.
* 즉, SQL 튜닝은 랜덤 I/O가 가장 중요하다.

#### SQL 튜닝은 랜덤 I/O와의 전쟁
* 성능을 위해 DBMS가 제공하는 많은 기능은 느린 랜덤 I/O를 극복하기 위해 개발되었다.
  * IOT, 클러스터, 파티션, 테이블 Prefetch, Batch I/O 등
* 조인 메서드 중 가장 일방적으로 사용하는 NL조인이 대량 데이터 조인할 때 느린 이유도 랜덤 I/O 때문이다.
  * 그래서 소트머지 조인과 해시 조인이 개발되었다.

### 인덱스 구조
* 인덱스는 대용량 테이블에서 필요한 데이터만 빠르게 효율적으로 액세스하기 위해 사용하는 오브젝트이다.
* 데이터베이스에서 인덱스 없이 데이터를 검색하려면 테이블을 처음부터 끝까지 읽어야 하지만, 인덱스를 이용해 일부만 읽고 멈출 수 있다.
  * 즉 범위스캔이 가능하며, 범위스캔이 가능한 이유는 인덱스가 정렬되어있기 때문이다.
* DBMS는 일반적으로 B-Tree 인덱스를 사용한다.
  * 루트와 브랜치 블록에는 키 값을 갖지 않는 특별한 레코드가 하나 있는데, 첫 번째 레코드이며, LMC(Leftmost Child)라고 한다.
  * LMC는 자식 노드 중 가장 왼쪽 끝에 위치한 블록을 가리킨다.
  * LMC가 기리키는 주소로 찾아간 블록에는, 키값을 가진 첫 번째 레코ㅓ드보다 작거나 같은 레코드가 저장되어있다.
  * 리프블록에 저장된 레코드는 키값 순으로 정렬되어있을 뿐만 아니라, 테이블 레코드를 가리키는 주소값인 ROWID를 갖는다.
* 인덱스 키값이 같으면 ROW_ID 순으로 정렬된다.
* 인덱스를 스캔하는 이유는 검색조건에 맞는 소량의 데이터를 빨리 찾아 ROWID를 얻기 위함이다.
  * ROWID = 데이터 블록 주소 + 로우번호
  * 데이터 블록 주소 = 데이터 파일 번호 + 블록 번호
  * 블록 번호 : 데이터파일 내에서 부여한 상대적 순번
  * 로우 번호 : 블록 내 순번
* 인덱스 탐색 과적은 수직적 탐색과 수평적 탐색으로 나눌 수 있다.
  * 수직적 탐색 : 인덱스 스캔 시작지점을 찾는 과정
  * 수평적 탐색 : 데이터를 찾는 과정

### 인덱스 수직적 탐색 
* 정렬된 인덱스 레코드 중, 조건을 만족하는 첫 번째 레코드를 찾는 과정으로, 인덱스 스캔 시작 지점을 찾는 과정이다.
* 인덱스 수직적 탐색은 루트블록에서부터 시작한다.
* 루트를 포함해 브랜치 블록에 저장된 각 인덱스 레코드는 하위 블록에 대한 주소값을 갖는다.
* 루트에서 시작해 리프블록까지 수직적으로 탐색이 가능한 이유다.
* 찾고자 하는 값보다 크거나 같은 값을 만나면 직전 레코드가 가리키는 하위 블록으로 이동한다.
* 수직적 탐색은 레코드를 찾는 과정이 아니라, **조건을 만족하는 첫 번째 레코드를 찾는 과정**이다.

### 인덱스 수평적 탐색
* 수직적 탐색을 통해 스캔 시작점을 찾은 후, 찾고자 하는 데이터가 더 안나타날 때까지 인덱스 리프 블록을 수평적으로 스캔한다.
* 즉, 인덱스에서 본격적으로 데이터를 찾는 과정이다.
* 인덱스 리프 블록끼리는 서로 앞뒤 블록에 대한 주소값을 갖고 있는 양방향 연결 리스트 구조이다.
  * 좌에서 우, 우에서 좌로 수평적 탐색이 가능하다.
* 인덱스를 수평적 탐색하는 이유는 조건절을 만족하는 데이터를 모두 찾고, ROWID를 얻기 위해서이다.
  * 일반적으로는 인덱스를 스캔하고 테이블도 액세스 하는데, 이 때 ROWID가 필요하다.

### 결합 인덱스 구조와 탐색
* 두 개 이상 컬럼을 결합해 인덱스를 만들 수 있다.
* 인덱스를 `[고객명 + 성별]`로 하던, `[성별 + 고객명]`으로 하던 읽는 인덱스 블록 개수는 똑같다.
* 인덱스 선두 컬럼을 모두 `=` 조건으로 검색할 때에는 어느 컬럼을 앞쪽에 두든 블록 I/O 개수가 같으므로 성능도 같다.
  * 선택도가 낮은 컬럼을 앞쪽에 둬야 성능에 유리하다는 말은 잘못되었다.
  * `=`조건이 아니라 다른 조건들이 섞여있으면 컬럼 순서의 영향이 있다.

> Balanced
> * B-Tree에서의 B는 Balanced의 약자로, 어떤 값으로 탐색하더라도 인덱스 루트에서 리프까지 도달하기 위해 읽는 블록 수는 같다.

## 인덱스 기본 사용법
### 인덱스를 사용한다는 것
* 인덱스 컬럼을 가공하지 않아야 인덱스를 정상적으로 사용할 수 있다.
* 인덱스를 정상적으로 사용한다는 표현은 리프블록에서 스캔 시작점을 찾아 거기서부터 스캔하다가 멈추는걸 의미하며, 리프 블록 일부만 스캔하는 Index Range Scan을 의미한다.
* 인덱스 컬럼을 가공해도 인덱스를 사용할 순 있지만, 스캔 시작점을 찾을 수 없고 멈출 수 없어 리프블록 전체를 스캔해야 하므로, Index Full Scan방식으로 동작하게 된다.

### 인덱스를 Range Scan 할 수 없는 이유
* 인덱스 컬럼을 가공했을 때 인덱스를 정상적으로 사용할 수 없는 이유는, 인덱스 스캔 시작점을 찾을 수 없기 때문이다.
  * 일정 범위를 스캔하려면 시작 지점과 끝 지점이 있어야 한다.
* 인덱스에는 가공되지 않은 생년월일 값이 저장되어 있는데, 가공된 값을 기준으로 검색을 하려면 스캔 시작점과 끝지점을 찾을 수 없다.
  * `where substr(생년월일, 5, 2) = '05'`
* 값이 NULL이면 0으로 치환한 값 기준으로 100보다 작은 레코드를 찾아달라고 하면 인덱스 스캔 시작지점을 알 수가 없다.
  * `where nvl(주문수량, 0) < 100`
* 대한으로 시작하는 값은 특정 구간에 몰려있겠지만, 대한을 포함하는 값은 전체 구간에 흩어져 있어 Range Scan이 불가능하다.
  * `where 업체명 like '%대한%'`
* 전화번호가 01012341234 이거나 고객명이 홍길동인 한 시작점을 찾을 수 없기 떄문에 Range Scan이 불가능하다. 
  * `where (전화번호 = :tel_no OR 고객명 = :cust_nm'`

> OR Expansion
> * 아래 쿼리와 같이 변경하면 고객명, 전화번호 인덱스에 대해 Index Range Scan이 가능하다.
> * OR조건식을 SQL옵티마이저가 위와 같은 형태로 변환할 수 있는데, 이것을 OR Expansion이라고 한다.
> * ```
>   select *
>   from 고객
>   where 고객명 = :cust_nm
>   union all
>   select *
>   from 고객
>   where 전화번호 = :tel_no
>   and (고객명 <> :cust_nm or 고객명 is null)
>   ```
> * `use_concat` 힌트를 이용해 OR Expansion을 유도할 수 있다.

* IN 조건 또한 한 지점을 찾을 수 없다.
  * `where 전화번호 in (:tel_no1, :tel_no2)`
  * IN 조건 또한 UNION ALL 방식으로 작성하면 인덱스 스캔 시작점을 찾을 수 있다.
  * 그래서 IN 조건절에 대해서는 SQL 옵티마이저가 IN-List Iterator 방식을 사용한다.
  > IN-List Iterator
  > * List 개수만큼 Index Range Scan 반복한다.

### 더 중요한 인덱스 사용 조건
* 인덱스를 정상적으로 사용하는 데 있어 중요한 선행조건이 있다.
* 인덱스 Range Scan을 하기 위해선 인덱스 선두 컬럼이 조건절에 있어야 한다. 또한 가공하지 않은 상태여야 한다.
* 인덱스를 잘 탄다고 해도, 스캔 범위를 줄이는데 전혀 역할을 하지 못할 수 있다.

### 인덱스를 이용한 소트 연산 생략
* 테이블과 달리 인덱스는 정렬되어 있다.
* PK 인덱스를 스캔하면서 출력한 집합은 어차피 PK 순으로 정렬되기 때문에 ORDER BY가 있어도 연산을 따로 수행하지 않는다.
* 또한 DESCENDING 정렬에도 인덱스를 활용할 수 있다.
  * 인덱스 리프 블록은 양방향 연결 리스트 구조이기 때문이다.
* 따라서 ORDER BY 절에서 DESC정렬을 요구해도, ORDER BY 연산을 하지 않고, `INDEX (RAGE SCAN DESCENDING)`과 같이 인덱스를 사용한다.

### ORDER BY 절에서 컬럼 가공
* 인덱스 컬럼을 가공하면 인덱스를 정상적으로 사용할 수 없다는 말은 대개 조건절에 사용한 컬럼을 의미한다.
* 그런데 조건절이 아닌 ORDER BY 또는 SELECT-LIST에서 컬럼을 가공함으로 인해 인덱스를 정상적으로 사용할 수 없는 경우도 있다.
* 아래 예시는 주문_PK 인덱스는 `[주문일자 + 주문번호]` 순으로 구성되어 있는 경우에도 ORDER BY 연산을 하게 된다. 
```oracle
SELECT *
FROM (
    SELECT TO_CHAR(A.주문번호, 'FM000000') AS 주문번호, A.업체번호, A.주문금액
    FROM 주문 A
    WHERE A.주문일자 = :dt
        AND A.주문번호 > NVL(:next_ord_no, 0)
    ORDER BY 주문번호   -- TO_CHAR의 FM000000옵션이 사용된 주문번호이기 때문, A.주문번호로 해야 함
    )
WHERE ROWNUM <= 30
```

### SELECT-LIST 에서 컬럼 가공
* 인덱스가 `[장비번호 + 변경일자 + 변경순번]` 순으로 구성되어 있으면서 정렬연산을 생략하는 예시
  ```oracle
  SELECT MIN(변경순번)
  FROM 상태태변경이력
  WHERE 장비번호 = 'C'
    AND 변경일자 = '20180316'
  ```
  ```oracle
  SELECT MAX(변경순번)
  FROM 상태태변경이력
  WHERE 장비번호 = 'C'
    AND 변경일자 = '20180316'
  ```
* 정렬연산을 생략할 수 없는 예시
  ```oracle
  SELECT NVL(MAX(TO_NUMBER(변경순번)), 0)
  FROM 상태변경이력
  WHERE 장비번호 = 'C'
    AND 변경일자 = '20180316'
  ```
  * 인덱스에는 문자열 기준으로 정렬되어 있는데, 이를 숫자값으로 바꾼 값 기준으로 최종 변경순번을 요구했기 때문이다.
  ```oracle
  SELECT NVL(TO_NUMBER(MAX(변경순번)), 0)
  FROM 상태변경이력
  WHERE 장비번호 = 'C'
    AND 변경일자 = '20180316'
  ```
  * 위와같이 바꾸면 정렬연산 없이 변경순번을 쉽게 찾을 수 있다.

### 자동 형변환
* 생년월일이 선두 컬럼인 인덱스가 있을 때, 아래 SQL은 생년월일 컬럼을 가공하지 않았는데도 테이블 전체 스캔을 선택한다.
```oracle
SELECT * FROM 고객
WHERE 생년월일 = 19821225
```
* 옵티마이저가 SQL을 아래와 같이 변환했고 결국 인덱스 컬럼이 가공되었기 때문이다.
```oracle
SELECT * FROM 고객
WHERE TO_NUMBER(생년월일) = 19821225
```
* 각 조건절에 양쪽 값의 데이터 타입이 다르면, 타입체크를 엄격하게 하여 컴파일 시점에 에러를 내는 DBMS가 있고, 자동으로 형변환 처리해주는 DBMS가 있다.
  * 오라클은 자동으로 형변환처리 해준다.
* 오라클에서는 숫자형과 문자형이 만나면 숫자형이 이기므로, 숫자형 컬럼 기준으로 문자형 컬럼을 변환한다.
  * 연산자가 LIKE일때에는 문자열 비교 연산자이므로, 문자형 기준으로 숫자형 컬럼이 변환된다.
* 날짜형과 문자형('01-JAN-2018')이 만나면 날짜형이 이긴다.
* 따라서 아래와 같이 타입을 정확히 지정해주는 습관이 좋다.
```oracle
SELECT * FROM 고객
WHERE 가입일자 = TO_DATE('01-JAN-2018, DD_MON_YYYY')
```
* LIKE 조건을 옵션 처리 목적으로 사용하는 경우가 종종 있다.
  * 아래 예시에서 계좌번호가 숫자형이면 인덱스를 사용할 수 없다.
  ```oracle
  SELECT * FROM 거래
  WHERE 계좌번호 LIKE :acnt_no || '%'
  AND 거래일자 between :trd_dt1 and :trd_dt2
  ```
* 자동 형변환으로 인해 쿼리의 결과가 의도와는 다르게 나오는 경우도 있다.
* 따라서 자동 형변환의 기능에 의존하면 안된다.
* **인덱스 컬럼 기준으로 반대 편 컬럼 또는 값들을 정확하게 형변환 해줘야 한다.**

## 인덱스 확장기능 사용법
### Index Range Scan
* Index Range Scan은 B-Tree 인덱스의 가장 일반적이고 정상적인 형태의 액세스 방식이다.
* 인덱스 루트에서 리프블록까지 수직적으로 탐색 후 필요한 범위만 스캔한다.

### Index Full Scan
* Index Full Scan은 수직적 탐색 없이 인덱스 리프 블록을 처음부터 끝까지 수평적으로 탐색하는 방식이다.
* 보통 데이터 검색을 위한 최적의 인덱스가 없을 경우 차선으로 선택된다.

#### Index Full Scan의 효용성
* 인덱스 선두컬럼이 조건절에 없으면 옵티마이저는 Table Full Scan을 고려한다.
* 하지만 대용량 테이블이어서 Table Full Scan에 따른 부담이 큰 경우 옵티마이저는 인덱스 활용을 다시 고려하지 않을 수 있다.
* 데이터 저장공간은 (컬럼길이 * 레코드 수)에 의해 결정되므로 인덱스가 차지하는 면적은 테이블보다 훨씬 적다.
* 인덱스 스캔 단계에서는 대부분 레코드를 필터링하고 아주 일부만 테이블을 액세스 하는 상황이라면 면적이 큰 테이블보다 인덱스를 스캔하는쪽이 유리하다.
* 옵티마이저가 이럴경우엔 Index Full Scan 방식을 선택한다.

#### 인덱스를 이용한 소트 연산 생략
* 인덱스를 Full Scan하면 결과 집함이 인덱스 컬럼 순으로 정렬된다.
* 따라서 옵티마이저가 전략적으로 선택한 경우, Sort Order By 연산을 생략할 목적으로 사용할 수도 있다.
* 하지만 대부분이 조건을 만족하는 상황에서 Index Full Scan을 선택하면 거의 모든 레코드에 대해 테이블 액세스가 발생하므로, Table Full Scan보다 오히려 불리하다.

### Index Unique Scan
* Index Unique Scan은 수직적 탐색만으로 데이터를 찾는 스캔 방식으로, Unique 인덱스를 `=` 조건으로 탐색하는 경우 작동한다.
* Unique Index가 존재하는 컬럼은 중복 값이 입력되지 않게 DBMS가 데이터 정합성을 관리해준다.
* 그러므로 해당 인덱스 키 컬럼을 모두 `=` 조건으로 검색할 때는 데이터를 한 건 찾는 순간 더이상 탐색할 필요가 없다.

### Index Skip Scan
* 인덱스 선두 컬럼을 조건절에 사용하지 않으면 옵티마이저는 기본적으로 Table Full Scan을 선택한다.
* 하지만 Table Full Scan 보다 I/O를 줄일 수 있거나 정렬된 결과를 쉽게 얻을 수 있다면 Index Full Scan을 사용하기도 한다.
* 오라클은 인덱스 선두컬럼이 조건절에 없어도 인덱스를 활용하는 새로운 스캔 방식을 9i버전에서 선보였는데, 이게 Index Skip Scan이다.
* Index Skip Scan은 조건절에 빠진 인덱스 선두 컬럼의 Distinct Value 개수가 적고 후행 컬럼의 Distinct Value 개수가 많을 때 유용하다.
* `index_ss`, `no_index_ss` 힌트를 사용해 이 스캔 방식을 유도하거나 방지할 수 있다.

#### Index Skip Scan이 작동하기 위한 조건
* Index Skip Scan은 Distinct Value 개수가 적은 선두 컬럼이 조건절에 없고, 후행 컬럼의 Distinct Value 개수가 많을 때 효과적이다.
* 하지만 인덱스 선두 컬럼이 없을 때만 Index Skip Scan이 작동하는건 아니다.
* 선두컬럼에 대한 조건절은 있고, 중간컬럼에 대한 조건절이 없는 경우에도 Skip Scan을 사용할 수 있다.
  ```oracle
  -- 일별업종별거래_PK : 업종유형코드 + 업종코드 + 기준일자
  SELECT /*+ INDEX_SS(A 일별업종별거래_PK) */
        기준일자, 업종코드, 체결건수, 체결수량, 거래대금
  FROM 일별업종별거래 A
  WHERE 업종유형코드 = '01'
      AND 기준일자 BETWEEN '20080501' AND '20080531'
  ```
  * 위 예시에서 Index Range Scan을 사용하면 업종코드유형이 '01'인 인덱스 구간을 모두 스캔해야 한다.
  * Index Skip Scan을 사용하면 업종코드유형이 '01'인 구간에서 기준일자 조건의 레코드를 포함할 가능성이 있는 리프 블록만 골라서 액세스할 수 있다.
* 선두컬럼이 부등호, BETWEEN, LIKE같은 범위 검색 조건일 때도 Index Skip Scan을 사용할 수 있다.
  ```oracle
  -- 일별업종별거래_X01 : 기준일자 + 업종유형코드
  SELECT /*+ INDEX_SS(A 일별업종별거래_X01) */
        기준일자, 업종코드, 체결건수, 체결수량, 거래대금
  FROM 일별업종별거래 A
  WHERE 기준일자 BETWEEN '20080501' AND '20080531'
    AND 업종유형코드 = '01'
  ```
  * 위 예시에서 Index Range Scan을 사용한다면, 기준일자 BETWEEN 조건을 만족하는 인덱스 구간을 모두 스캔해야 한다.
  * 조건을 만족하는 인덱스 구간에서 업종유형코드 = '01'인 레코드를 포함할 가능성이 있는 리프블록만 골라서 액세스할 수 있다.
* Index Range Scan이 불가능하거나 효율적이지 못한 상황에서 Index Skip Scan이 종종 빛을 발할 수 있다.
* **인덱스는 기본적으로 최적의 Index Range Scan을 목표로 설계해야 하며, 수행횟수가 적은 SQL을 위해 인덱스를 추가하는것이 비효율적일 때 이러한 스캔 방식을 차선책으로 활용하는 전략이 바람직하다.**

### Index Fast Full Scan
* Index Fast Full Scan은 Index Full Scan보다 빠르다
* 논리적인 인덱스 트리구조를 무시하고 인덱스 세그먼트 전체를 Multiblock I/O 방식으로 스캔하기 때문에 Index Full Scan보다 빠르다.
* `index_ffs`, `no_index_ffs` 힌트로 이 스캔방식을 유도하거나 방지할 수 있다.
* Index Fast Full Scan은 루트와 브랜치블록은 읽지만 필요없으므로 무시한다.
* Index Fast Full Scan은 Multiblock I/O 방식을 사용하므로, 디스크로부터 대량의 인덱스 블록을 읽어야 할 때 큰 효과를 발위한다.
* 연결리스트 구조를 무시한 채 데이터를 읽기 때문에, 결과 집합이 인덱스 키 순서대로 정렬되지 않는다.
* 쿼리에 사용한 컬럼이 모두 인덱스에 포함되어 있을 때만 사용할 수 있다.
* Index Range Scan 또는 Index Full Scan과 달리 인덱스가 파티션 되어있지 않더라도 병렬쿼리가 가능하다.
* 병렬 쿼리 시에는 Direct Path I/O 방식을 사용하기 때문에 I/O 속도가 더 빨라진다.

### Index Range Scan Descending
* Index Range Scan과 기본적으로 동일한 스캔 방식으로, 인덱스를 뒤에서 앞쪽으로 스캔하기 떄문에 내림차순으로 정렬된 결과집합을 얻는다.
* `index_desc` 힌트를 이용해 유도할 수 있다.
* MAX 값을 구하고자 할 때에 해당 컬럼에 인덱스가 있으면 인덱스를 뒤에서부터 한 건만 읽고 멈추는 실행계획이 자동으로 수립된다.

# 인덱스 튜닝
## 테이블 액세스 최소화
### 테이블 랜덤 액세스
#### 인덱스 ROWID는 물리적? 논리적?
* 인덱스 ROWID는 물리적 주소보다 논리적 주소에 가깝다.
* 물리적으로 직접 연결되지 않고 테이블 레코드를 찾아가기 위한 논리적 주소 정보를 담고있기 때문이다.
* 메모리상 데이터를 찾아가는데 있어 포인터의 비용은 0에 가깝고, 물리적으로 직접 연결된 구조와 다름없다.
* 인덱스 ROWID는 포인터와는 다르다. 테이블 레코드를 찾아가기위한 위치 정보만을 담고 있으며, 테이블 레코드와 물리적으로 연결된 구조가 아니다.
* 오라클은 테이블 블록이 수시로 버퍼캐시에서 밀려났다 다시 캐싱되기를 반복한다.
* 그때마다 다른 공간에 캐싱되기 때문에 인덱스에서 포인터로 직접 연결할 수 없는 구조이다.
* 메모리 주소 정보(포인터)가 아닌 디스크 주소 정보(Data Block Address)를 이용해 해시 알고리즘으로 버퍼 블록을 찾아간다.

#### I/O 메커니즘
* DBA는 디스크 상에서 블록을 찾기 위한 주소 정보이다.
* I/O성능을 높으려면 매번 디스크에서 블록을 읽을 수 없기 때문에 버퍼캐시를 활용해야 한다.
* 그래서 블록을 읽을 때 디스크로 가기 전 버퍼캐시부터 찾아본다.
  * 읽고자 하는 DBA를 해시함수에 입력해 해시 체인을 찾고 거기서 버퍼 헤더를 찾는다.
* 캐시에 적재할 때와 읽을 때 같은 해시 함수를 사용하므로, 버퍼 헤더는 항상 같은 해시 체인에 연결된다.
* 하지만 실제 데이터가 담긴 버퍼 블록은 매번 다른 위치에 캐싱되는데, 그 메모리 주소값을 버퍼 헤더가 갖고 있다.
* 따라서 해싱 알고리즘으로 버퍼 헤더를 찾고 거기서 얻은 포인터로 버퍼블록을 찾아간다.
* 인덱스로 테이블 블록을 액세스할 때에는 리프블록에서 읽은 ROWID를 분해해 DBA정보를 얻고, 테이블을 Full Scan할 때는 익스텐트 맵을 통해 읽은 블록들의 DBA정보를 얻는다.
* 즉, ROWID는 포인터가 아니고 테이블 레코드를 찾아가기 위한 논리적인 주소정보이다.
  * ROWID가 가리키는 테이블 블록을 버퍼캐시에서 먼저 찾은 후, 못 찾을 때만 디스크에서 블록을 읽는다. (버퍼캐시 적재 후 읽음)
  * 모든 데이터가 캐싱되어 있어도 테이블 레코드를 찾기 위해 DBA 해싱과 래치 획득 과정을 반복해야 한다.
  * 캐시버퍼 체인 래치와 버퍼 Lock에 대한 경합까지 발생한다.
* ROWID를 이용한 테이블 액세스는 생각보다 고비용 구조이다. 

* `TABLE ACCESS BY INDEX ROWID` 오퍼레이션을 볼 때마다 이러한 복잡한 처리과정을 떠올려야 한다.

### 인덱스 클러스터링 팩터
* 클러스터링 팩터(CF)는 군집성 계수로, 특정 컬럼을 기준으로 같은 값을 갖는 데이터가 서로 모여있는 정도를 의미한다.
* CF가 좋은 컬럼에 생성한 인덱스는 검색효율이 매우 좋다.
* 테이블 액세스량에 비해 블록 I/O가 적게 발생하기 때문이다.
* 인덱스 ROWID로 테이블을 액세스 할 때, 오라클은 래치 획득과 해시체인 스캔 과정을 거쳐 찾아간 테이블 블록에 대한 포인터를 해제하지 않고 유지한다. (버퍼 Pinning)
* 이 상태에서 다음 인덱스 레코드를 읽었는데, 직전과 같은 테이블 블록을 가리킨 경우, 래치 획득과 해시 체인 스캔 과정을 생략하고 테이블 블록을 읽을 수 있다.
  * 논리적 I/O 과정을 생략할 수 있는 것이다.

### 인덱스 손익 분기점
* 인덱스 ROWID를 이용한 테이블 액세스는 생각보다 고비용 구조이다.
* 읽어야할 데이터가 일정량을 넘는 순간, 테이블 전체를 스캔하는것보다 오히려 느려진다.
  * Table Full Scan은 시퀀셜 액세스인 반면, 인덱스 ROWID를 이용한 테이블 액세스는 랜덤 액세스 방식이다.
  * Table Full Scan은 Multiblock I/O인 반면, 인덱스 ROWID를 이용한 테이블 액세스는 Single Block I/O 방식이다.
* Index Range Scan에 의한 테이블 액세스가 Table Full Scan보다 느려지는 지점을 인덱스 손익분기점이라고 부른다.
* 인덱스 CF가 나쁘면 같은 테이블 블록을 여러번 반복 액세스 하면서 논리적 I/O 횟수가 늘어나고, 물리적 I/O 횟수도 늘어나게 된다.
* 인덱스 손익분기점은 보통 5~20%의 낮은 수준에서 결정된다.
  * CF가 나쁘면 손익분기점은 5%미만에서 결정되며, CF가 아주 좋을 땐 90%수준까지도 올라간다.
  * 이 수치는 많아봐야 100만건 이내 테이블에나 적용되는 수치이고, 1000만건 수준의 큰 테이블에선 손익분기점이 더 낮아진다.
    * 버퍼캐시에 할당하는 메모리 크기가 점점 커지는 추세이지만, 요즘 기준으로 보통 수백만개 블록을 캐싱하는 수준이다.
    * 따라서 특정 테이블을 인덱스로 100만건 이상 액세스 한다면 캐시 히트율은 극히 낮을수밖에 없다.
    * 1000만건 정도의 테이블이면 CF도 안좋을 가능성이 높고, 손익분기점 자체가 의미가 없어지므로, 만건만 넘게 읽어도 Table Full Scan방식이 빠를 수 있다.
* 따라서 테이블스캔이 항상 나쁜것은 아니고, 인덱스 스캔이 항상 좋은것도 아니라고 말할 수 있다.

#### 온라인 프로그램 튜닝 vs 배치 프로그램 튜닝
* 온라인 프로그램은 보통 소량 데이터를 읽고 갱신하므로 인덱스를 효과적으로 활용하는것이 무엇보다 중요하다.
  * 조인도 대부분 NL 방식을 사용한다.
  * NL조인은 인덱스를 이용하는 조인 방식으로, 인덱스를 이용해 소트 연산을 생략함으로써 부분범위 처리 방식으로 구현할 수 있다면 온라인 환경에서 대량의 데이터를 조회할 때에도 아주 빠른 응답 속도를 낼 수 있다.
* 배치 프로그램은 항상 전체 범위 처리 기준으로 튜닝해야 한다.
  * 대량의 데이터를 빠르게 처리하려면 인덱스와 NL조인보다 Full Scan과 해시조인이 유리하다.
  * 배치 프로그램에선 인덱스보다 Full Scan이 효과적이지만, 초대용량 테이블을 Full Scan하면 상당히 오래 기다려야 하고 시스템에 주는 부담도 커진다.
  * 따라서 파티션 활용 전략이 매우 중요한 튜닝 요소이며, 병렬처리까지 할 수 있으면 더더욱 좋다.
  * 파티셔닝하는 이유는 결국 Full Scan을 빠르게 처리하기 위해서이다.

### 인덱스 컬럼 추가
* 실 운영 환경에서는 인덱스 구성을 변경하기는 절대 쉽지 않다.
* 그렇다고 인덱스를 새로 만들게 되면, 인덱스를 계속 추가하게 되어 테이블마다 인덱스가 수십개씩 달려 있는 상황이 되어버릴 수 있다.
  * 인덱스 관리 비용이 증가하는것은 물론이며, DML 부하에 따른 트랜잭션 성능 저하가 생길 수 있다.
* 따라서 기존 인덱스에 특정 컬럼만 추가하는것으로 큰 효과를 얻을 수 있다.

### 인덱스만 읽고 처리
* 비효율이 없어도 인덱스 스캔 과정에서 얻은 데이터가 많다면 테이블 랜덤 액세스가 많이 발생하므로 성능은 느릴수밖에 없다.
```oracle
SELECT 부서번호, SUM(수량)
FROM 판매집계
WHERE 부서번호 LIKE '12%'
GROUP BY 부서번호;
```
* 이런 상황에서 고려해볼 수 있는 건 쿼리에 사용된 컬럼을 모두 인덱스에 추가해 테이블 액세스가 아예 발생하지 않도록 할 수 있다.
* 이렇게 인덱스만 읽어서 처리하는 쿼리를 **Covered 쿼리**라고 부르며, 그 쿼리에 사용한 인덱스를 **Covered 인덱스**라고 부른다.
* 하지만 이러한 방법은 추가해야 할 컬럼이 많아 실제로는 적용하기 곤란한 경우가 많다.

#### include 인덱스
* Oracle엔 없고 SQL Server 2005 버전에 추가된 기능이다.
* 인덱스 키 외에 미리 지정한 컬럼을 리프 레벨에 함께 저장하는 기능이다.
* `create index emp_x01 on emp (deptno) incloude (sal)`
  * 해당 인덱스를 이용하면, 수직적 탐색에는 DEPTNO만 사용하고, 수평적 탐색에는 SAL 컬럼도 필터 조건으로 사용할 수 있다.
  * 즉, SAL 컬럼은 테이블 랜덤 액세스 횟수를 줄이는 용도로 사용된다. 
  * 하지만 소트 연산은 생략할 수 없다.

### 인덱스 구조 테이블
* 랜덤액세스가 아예 발생하지 않도록 테이블을 인덱스 구조로 생성할 수 있다.
* 오라클은 IOT(Index-Organized Table)이라고 부른다.
  * MS-SQL Server는 클러스터형(Clustered) 인덱스라고 부른다.
* 테이블을 찾아가기위한 ROWID를 갖는 일반 인덱스와는 달리 IOT는 그 자리에 테이블 데이터를 갖는다.
  * 테이블 블록에 있어야할 데이터를 리프블록에 모두 저장하고 있다.
  * 즉, IOT에서는 인덱스 리프블록이 곧 데이터 블록이다.
* 아래와 같이 테이블을 인덱스 구조로 만들 수 있다.
  ```oracle
  create table index_org_t(t number, b varchar(10), constraint index_org_t_pk primary key (a))
  organization index ;
  ```
* 일반 테이블은 힙구조 테이블이라 부르며, 테이블 생성 시 보통 생략하지만 직접 명시할 수도 있다.
  ```oracle
  create table heap_org_t(a number, b varchar(10), constraint heap_org_t_pk primary key (a))
  organization heap ;
  ```
* 일반 힙구조 테이블에 데이터를 입력할 때에는 랜덤 방식을 사용하지만(Freelist로부터 할당받은 블록에 순서 없이 데이터 입력), IOT는 인덱스 구조 테이블이므로 정렬 상태를 유지하며 데이터를 입력한다.
* IOT는 인위적으로 CF를 좋게 만드는 방법 중 하나이다.

### 클러스터 테이블
* 클러스터 테이블에는 인덱스 클러스터와 해시 클러스터 두 가지가 있다.
#### 인덱스 클러스터 테이블
* 인덱스 클러스 테이블은 클러스터 키 값이 같은 레코드를 한 블록에 모아서 저장하는 구조이다.
* 한 블록에 모두 담을 수 없을 때는 새로운 블록을 할당해서 클러스터 체인으로 연결한다.
* 또한 여러 테이블 레코드를 한 블록에 저장할 수 있는데, 이를 다중 테이블 클러스터라고 부른다.
  * 일반 테이블은 하나의 데이터 블록을 여러 테이블이 공유할 수 없다.
* 아래와 같이 인덱스 클러스터형 테이블을 만들 수 있다.

  ```oracle
  create cluster c_dept# ( deptno number(2) ) index ;
  ```
* 클러스터에 테이블을 담기 전 아래와 같이 클러스터 인덱스를 반드시 정의해야 한다.

  ```oracle
  create index c_dept#idx on cluster c_dept#;
  ```
  * 클러스터 인덱스는 데이터 검색용도로 사용할 뿐만 아니라 데이터가 저장된 위치를 찾을 때에도 사용한다.

* 클러스터 인덱스도 일반 B-Tree 인덱스 구조를 사용하지만, 테이블 레코드를 일일이 가리키지 않고 해당 키 값을 저장하는 첫 번째 테이블 블록을 가리킨다.
  * 일반 테이블에 생성한 인덱스 레코드는 테이블 레코드와 1:1대응관계를 갖지만 클러스터 인덱스는 테이블 레코드와 1:N 관계를 갖는다.
* 그러므로 클러스터 인덱스의 키 값은 항상 Unique하다.
* 클러스터 인덱스를 스캔하면서 값을 찾을 때에는 랜덤액세스가 값 하나당 한번씩 밖에 발생하지 않는다. (클러스터 체인을 스캔하면서 발생하는 랜덤 액세스는 제외)
* **클러스터에 도달해 시퀀셜 방식으로 스캔하기 때문에 넓은 범위를 읽더라도 비효율이 없다.**

#### 해시 클러스터 테이블
* 해시 클러스터는 인덱스를 사용하지 않고 해시 알고리즘을 사용해 클러스터를 찾아간다.
* 아래와 같이 해시 클러스터를 생성할 수 있다.
  ```oracle
  create cluster c_dept# ( deptno number(2) ) hashkeys 4 ;
  ```
* 그리고 아래와 같이 클러스터 테이블을 생성한다.
  ```oracle
  create table dept (
    deptno number(2) not null
  , dname varchar2(14) not null
  , loc varchar2(13))
  cluster c_dept#( deptno );
  ```

## 부분범위 처리 활용
* 부분범위 처리를 활용하면 인덱스로 액세스할 대상 레코드가 아무리 많아도 아주 빠른 응답속도를 낼 수 있다.

### 부분범위 처리
* 아무리 많은건을 요청을 해도, DBMS가 데이터를 모두 읽고 한번에 전송하지 않고 먼저 읽는 데이터부터 일정량을 전송하고 멈춘다.
* 데이터를 전송하고 나면 서버 프로세스는 CPU를 OS에 반환하고 대기 큐에서 잠을 잔다.
* 다음 Fetch Call을 받으면 대기 큐에서 나와 그다음 데이터부터 일정량(Array Size) 읽어서 전송하기 다시 잠을 잔다.
* 이렇게 전체 쿼리 결과집합을 쉼없이 전송하지 않고 사용자로부터 Fetch Call이 있을 때마다 일정량씩 나눠서 전송하는 것을 부분범위 처리라고 한다.
* JAVA에서 Array Size 기본 값은 10이고, Statement 객체 setFetchSize 메소드를 통해 설정을 변경할 수 있다.

> 잘못된 상식
> * 쿼리 수행 시 결과 집합을 버퍼캐시에 모두 적재하고 사용자에게 전송한다고 알고있는 사람이 있는데, 잘못되었다.

#### 정렬 조건이 있을 때 부분 범위 처리
* DB서버는 모든 데이터를 다 읽어 정렬을 마치고 나서야 클라이언트에게 데이터 전송을 시작할 수 있다.
* Sort Area와 Temp 테이블스페이스까지 이용해 데이터 정렬을 마치고 나면 그때부터 일정량씩 나눠 클라이언트에게 데이터를 전송한다.
* 정렬기준 컬럼이 선두인 인덱스가 있으면 부분범위처리가 가능하다.
* 인덱스는 항상 정렬된 상태를 유지하므로, 전체 데이터를 정렬하지 않고도 정렬된 상태의 결과집합을 바로 전송할 수 있다.

#### Array Size 조정을 통한 Fetch Call 최소화
* 대량의 데이터를 파일로 내려받는다면 어차피 데이터를 모두 전송해야하기 때문에 가급적 Array Size를 크게 해야 한다.
  * Fetch Call 횟수를 그만큼 줄일 수 있다.
* 하지만 앞 쪽 일부 데이터만 Fetch하다 멈추는 프로그램이라면 Array Size를 작게 설정하는 것이 좋다.
  * 불필요하게 많은 데이터를 전송하고 버리는 비효율을 줄일 수 있다.

#### 쿼리 툴에서 부분 범위 처리
* 토드나 오렌지같은 쿼리 툴에서도 쿼리의 결과가 아무리 많아도 일찍 끝나는 것을 볼 수 있다.
* 그 이유는 전체 데이터 중 먼저 읽은 일정 수의 데이터만 출력하도록 해놓았기 때문이다. 즉, Array Size가 기본적으로 설정되어있기 때문이다.
* 모든 DBMS는 부분범위 처리 방식으로 결과집합을 전송한다. 
* 이 특징을 이용해 중간에 멈췄다 사용자의 추가 요청이 있을 때마다 데이터를 가져오도록 구현하고 안하고는 클라이언트 프로그램을 개발하는 개발자의 몫이다.

### 부분범위 처리 구현
* 부분범위 처리와 관련된 부분을 개발자가 일일이 구현할 순 없기 때문에, 개발 프레임워크에 미리 구현되어있는 기능을 활용한다.

### OLTP 환경에서 부분범위 처리에 의한 성능개선 원리
* OLTP 시스템도 수천 수만건을 조회하는 경우도 있다.
* 인덱스를 이용해 수천 수만건을 조회하려고 하면 성능을 내기 어려울 수 있다.
  * 많은 랜덤액세스가 발생하기 때문이다.
* OLTP성 업무에서 쿼리 결과 집함이 아주 많을 때 사용자가 모든 데이터를 일일이 확인하진 않고, 특정한 정렬 순으로 상위 일부 데이터만 확인한다.
* 따라서 인덱스와 부분범위 처리 원리를 잘 활용하면 OLTP 환경에서 좋은 성능개선 효과를 얻을 수 있다.

#### 멈출 수 있어야 의미있는 부분범위 처리
* 클라이언트와 DB 서버 사이에 WAS, AP 서버 등이 존재하는 n-Tier 아키텍처에서는 클라이언트가 특정 DB 커넥션을 독점할 수 없다.
* 단위작업을 마치면 DB 커넥션을 커넥션 풀에 반환해야 하므로, 그 전에 SQL조회 결과를 클라이언트에 모두 전송하고 커서를 닫아야 한다.
* 그렇다고 해서 부분범위 처리는 n-Tier 환경에서 의미없는 개념은 아니다.

> 배치 I/O
> * 디스크 랜덤 I/O 성능을 DBMS업체들이 계속 노력을 하는데, 오라클에서는  배치 I/O 기능으로 눈에 띄는 개선을 했다.
> * 배치 I/O는 읽는 블록마다 건건이 I/O Call을 발생시키는 비효율을 줄이기 위해 고안한 기능이다.
> * 인덱스를 이용해 테이블을 액세스하다 버퍼캐시에서 블록을 찾지 못하면 디스크 블록을 바로 읽는데, 이 기능이 작동하면 테이블 블록에 대한 디스크 I/O Call을 미뤘다가 읽을 블록이 일정량 쌓이면 한꺼번에 처리한다.
>   * 11g에서는 NL 조인 Inner 쪽 테이블 액세스할 때에만 이 기능이 작동했지만, 12c부터는 인덱스 ROWID로 테이블 액세스하는 어떤 부분에서든 이 기능이 작동할 수 있다.
> * 배치 I/O 기능이 작동하면 인덱스를 이용해 출력하는 데이터 정렬 순서가 매번 다를 수 있다.
> * 따라서 배치 I/O 기능이 작동하면 데이터 정렬순서를 보장할 수 없기 때문에 소트연산을 생략할 수 없을 수 있다.
> * 배치 I/O를 통해 얻을 수 있는 이점이 많음에도 이 기능을 비활성화하는 이유가 이러한 이유이다.

## 인덱스 스캔 효율화
### 인덱스 탐색
* 인덱스가 어떻게 설정되어있고, 어떤식으로 조건을 걸 때 인덱스를 어떤식으로 스캔할지 알아야 한다.
* p174 ~ p179 참고

### 인덱스 스캔 효율성
* 인덱스가 어떻게 설정되어있고, 어떤식으로 조건을 걸 때 인덱스를 어떤식으로 스캔할지 알아야 한다.
* 인덱스 선행 컬럼이 조건에 없는 경우 스캔 범위가 늘어난다는 것을 알아야 한다.
* p180 ~ p184 참고

#### 인덱스 스캔 효율성 측정
* 조건절 데이터를 일일이 조회해보는 방법도 있지만, SQL 트레이스를 통해 쉽게 알 수 있다.
* 예를들면 인덱스를 스캔하고 얻은 레코드가 적은데데, 그 과정에 읽은 블록수가 많으면 비효율적이라고 볼 수 있다.

### 액세스 조건과 필터 조건
* 인덱스를 스캔하는 단계에 처리하는 조건절은 액세스 조건과 필터 조건으로 나뉜다.
* 인덱스 액세스 조건은 인덱스 스캔 범위를 결정하는 조건절이다.
  * 인덱스 수직적 탐색을 통해 스캔 시작점을 결정하는데 영향을 미친다.
  * 인덱스 리프 블록을 스캔하다가 어디서 멈출지를 결정하는데 영향을 미친다.
* 인덱스 필터 조건은 테이블로 액세스할지를 결정하는 조건절이다.
* 인덱스를 이용하던 테이블을 Full Scan하던 테이블 액세스 단계에서 처리되는 조건절은 모두 필터조건이다.
* 테이블 필터 조건은 쿼리 수행 다음 단계로 전달하거나 최종 결과 집함에 포함할지를 결정한다.

### 비교 연산자 종류와 컬럼 순서에 따른 군집성
* 선행 컬럼이 모두 `=` 조건인 상태에서 첫 번째 나타나는 범위검색 조건까지만 만족하는 인덱스 레코드는 모두 연속해서 모여있다.
* 그 이하 조건까지 만족하는 레코드는 비교연산자 종류에 상관없이 흩어지게 된다.

#### 범위검색 조건 맨 처음과 마지막 구간에서의 액세스 조건
```oracle
where C1 between 1 and 3
    and C2 = 'A'
    and C3 = '나'
    and C4 = 'a'
```
* C1, C2, C3, C4로 인덱스가 구성되어있을 때 위 조건일 경우엔 액세스 조건은 C1, 필터조건은 C2, C3, C4이다.
* 하지만 `C1 = 1`, `C1 = 3`인 구간에서 C2, C3, C4도 인덱스 스캔량을 줄이는데 어느정도 역할을 할 수 있다.
  * `C1 = 3` 구간에서 `C2 = 'A'`, `C3 = '나'`, `C4 = 'a'`보다 큰 값을 만나는순간 멈추기 때문이다.
* 첫 범위검색 이후 조건절 컬럼은 스캔범위를 줄이는데 큰 역할을 못함에도, 아래와 같이 몇 가지 케이스를 제외하면 인덱스 컬럼에 대한 조건절은 모두 액세스 조건에 표시된다. 
  * 좌변 컬럼을 가공한 조건절
  * 왼쪽 `%` 또는 양쪽 `%` 기호를 사용한 `like` 조건절
  * 같은 컬럼에 대한 조건절이 두 개 이상일 때 인덱스 액세스 조건으로 선택되지 못한 조건절
  * OR Expansion 또는 INLIST ITERATOR로 선택되지 못한 OR 또는 IN 조건절
* 복잡하게 생각할 것 없이, 첫 번째 나타나는 범위검색 조건까지가 인덱스 액세스 조건이고, 나머지는 필터조건이라고 이해하는게 좋다.

### 인덱스 선행 컬럼이 등치(`=`) 조건이 아닐 때 생기는 비효율
* 인덱스 스캔 효율성은 인덱스 컬럼을 조건절에 모두 `=` 조건으로 사용할 때 가장 좋다.
* 인덱스 컬럼 중 일부가 조건절에 없거나 등치 조건이 아니더라도, 그것이 뒤 쪽 컬럼일 때에는 비효율이 전혀 없다.
* 인덱스 선행 컬럼이 모두 `=` 조건일 때 필요한 범위만 스캔하고 멈출 수 있는 것은 조건을 만족하는 레코드가 모두 한 곳에 모여있기 때문이다.

### BETWEEN을 IN-List로 전환
* 범위검색 컬럼이 맨 뒤로 가도록 인데스를 변경하면 좋겠지만, 운영 시스템에서 인덱스 구성을 바꾸기는 쉽지 않다.
* 이럴 때에는 BETWEEN 조건을 IN-List로 바꿔주면 큰 효과를 얻는 경우가 있다.
* IN-LIST 개수 만큼 UNION ALL 브랜치가 생성되고 각 브랜치마다 모든 컬럼을 `=` 조건으로 검색하기 때문에, BETWEEN을 사용할 때와 같은 비효율이 사라진다.
  * 옵티마이저가 IN-List Iterator 방식 사용하게 된다.
* IN-List 항목 개수가 늘어날 수 있는 경우 BETWEEN을 IN-List로 전환하는 방식은 사용하기 곤란할 수 있다.
  * 이럴 경우엔 NL의 조인문이나 서브쿼리로 구현하면 된다. (IN-List 값들을 코드 테이블로 관리하고 있는 경우만)
* 또한 Index Skip Scan 방식으로 유도해도 비슷한 효과를 얻을 수 있다.

#### BETWEEN 조건을 IN-List로 전환할 때 주의사항
* IN-List의 개수가 많지 않아야 한다.
* IN-List의 개수가 많으면 수직적 탐색이 많이 발생한다.
* 이런 경우엔 리프블록을 많이 스캔하는 비효율보다 IN-List 개수만큼 브랜치 블록을 반복 탐색하는 비효율이 더 클 수 있다.
  * 루트에서 브랜치 블록까지 Depth가 더 깊을 경우 특히 그렇다.
* BETWEEN 조건 때문에 인덱스를 비효율적으로 스캔하더라도, 블록 I/O 측면에서는 보통 소량에 그치는 경우가 많다.
* 인덱스 리프 블록에는 테이블 블록과 달리 많은 레코드가 담기기 때문이다.
* 또한 IN-List 개수가 많으면 수직적 탐색 과정에서 많은 블록을 읽게 된다.
* **데이터 분포나 수직적 탐색 비용을 따져보지도 않고 BETWEEN을 IN-List로 변환하면 안된다.**

#### Index Skip Scan 활용
* BETWEEN 조건을 IN-List 조건으로 변환하면 도움이 되는 상황에서 굳이 조건절을 바꾸지 않고도 효과를 낼 방법이 있는데, Index Skip Scan을 활용하면 된다.
* 선두 컬럼이 BETWEEN이어서 나머지 검색 조건을 만족하는 데이터들이 서로 멀리 떨어져 있을 때 Index Skip Scan이 효율적이다.

### IN 조건은 `=`인가?
* IN 조건은 `=`이 아니다.
* 인덱스를 어떻게 구성하냐에 따라 성능도 달라질 수 있다.
* IN 조건에 `=`이 되려면 IN-List Interator 방식으로 풀려야만 한다.
* 그렇지 않으면 IN 조건은 필터 조건이다.
  * p204~p205 참고
* 또한 IN 조건을 액세스 조건으로 만들기 위해 `=`조건으로 만들어 IN-List Iterator 방식으로 푸는게 항상 효과적이지도 않다.

#### NUM_INDEX_KEYS
* IN-List를 액세스조건 또는 필터조건으로 유도하는 방법이 있다.
* 인덱스가 `[고객번호 + 상품ID]` 순으로 구성된 상황에서 고객번호만 인덱스 액세스 조건으로 사용하려면 아래와 같이 힌트를 사용하면 된다.
```oracle
select /*+ num_index_keys(a 고객별가입상품_X1 1)*/ 
    *
from 고객별가입상품 a
where 고객번호 = :cust_no
and 상품ID in ('T01', 'T05', 'T52')
```
* 3번째 인자 1은 인덱스의 첫 번째 컬럼까지만 액세스 조건으로 사용하라는 의미이다.
* 또한 힌트 사용 없이, 인덱스 컬럼을 일부로 가공하는 방법도 있다.
```oracle
select *
from 고객별가입상품 a
where 고객번호 = :cust_no
and RTRIM(상품ID) in ('T01', 'T05', 'T52')

select *
from 고객별가입상품 a
where 고객번호 = :cust_no
  and 상품ID || ''  in ('T01', 'T05', 'T52')
```

### BETWEEN과 LIKE 스캔 범위 비교
* LIKE와 BETWEEN은 둘 다 범위검색 조건으로, 범위검색 조건을 사용할 때의 비효율 원리가 똑같이 적용된다.
* LIKE보다는 BETWEEN을 사용하는게 좋다.
* 물론 사용하기엔 BETWEEN이 불편하지만, BETWEEN을 사용하면 적어도 손해는 안본다.

### 범위검색 조건을 남용할 때 생기는 비효율
* 사용자 입력에 따라 조건절이 다양하게 바뀌는 경우, SQL을 간편하게 작성하려고 조건절을 모두 LIKE로 개발해버리는 경우가 있다.
* 이럴 경우 액세스 조건이 필터 조건으로 변해버릴 수도 있다.
* 코딩을 쉽게 하기 위해 인덱스 컬럼에 범위검색조건을 남용하면 인덱스 스캔 비효율이 생긴다.
* 대량의 테이블을 넓은 범위로 검색할 경우 더더욱 영향이 심해진다.
* 데이터 분포에 따라 인덱스 컬럼에 대한 비교 연산자를 신중하게 선택해야 한다.

### 다양한 옵션 조건 처리 방식의 장단점 비교
#### OR 조건 활용
```oracle
select * from 거래
where (:cust_id is null or 고객ID = :cust_id)
    and 거래일자 between :dt1 and :dt2
```
* 인덱스가 `[고객ID + 거래일자]`로 구성되어있는 경우, 옵션 조건 컬럼을 선두에 두어도 인덱스를 사용할 수 없다.
* `[거래일자 + 고객ID]`로 구성한 인덱스는 사용할 순 있지만, 고객 ID를 필터조건으로 사용하는 문제가 있다.
* 인덱스에 포함되지 않은 컬럼에 대한 옵션 조건은 어차피 테이블에서 필터링할 수 밖에 없으므로 그럴 때에는 위와같은 방식을 사용해도 무방하다.
* 따라서 OR 조건을 활용한 옵션 조건 처리는 아래를 고려해야 한다.
  * 인덱스 액세스 조건으로 사용 불가
  * 인덱스 필터 조건으로 사용 불가
  * 테이블 필터 조건으로만 사용 가능
  * 인덱스 구성 컬럼 중 하나 이상이 Not Null 컬럼이면 18c부터 인덱스 필터조건으로 사용 가능
* 이런 특성을 항상 고려할 수 있는게 아니라면, OR 조건을 이용한 옵션 조건 처리는 가급적 사용하지 않아야 한다.
  * 이방식의 유일한 장점은 옵션 조건 컬럼이 NULL 허용 컬럼이어도 결과 짛밥을 보장한다는 것이다.

#### LIKE/BETWEEN 조건 활용
* 필수조건의 변별력이 좋은 경우엔 이런 패턴을 사용하는것도 나쁘지 않다.
  ```oracle
  -- 인덱스 : 등록일시 + 상품분류코드
  select * from 상품
  where 등록일시 >= trunc(sysdate)             -- 필수 조건
      and 상품분류코드 like :prd_cls_cd || '%'  -- 옵션 조건
  ```
  * 또한 필수 조건이 `=` 이면 옵션조건까지도 인덱스 액세스 조건이므로 최적의 성능을 낼 수 있다.

* 아래 예시와 같이, 필수조건의 변별력이 좋지 않을 때에는 문제가 된다.
  ```oracle
  -- 인덱스 : 상품대분류코드 + 상품코드
  select * from 상품
  where 상품대분류코드 = :prd_lcls_cd    -- 필수 조건
      and 상품코드 like :prd_cd || '%' -- 옵션조건
  ```
  * 상품 대분류 코드만으로 조회할 때에는 Table Full Scan이 유리하지만, 옵티마이저가 상품코드까지 입력할 때를 기준으로 Index Range Scan을 선택한다.
* 따라서 LIKE/BETWEEN 조건을 활용한 옵션 조건 처리는 아래를 고려해야 한다.
  * 인덱스 선두 컬럼인지
    * 인덱스 선두컬럼인 경우, 모든 데이터를 스캔하는 일이 생김
  * NULL 허용 컬럼인지
    * NULL 허용컬럼인데 실제 NULL 값이 입력되어 있다면 그 데이터는 결과집합에서 누락된다. (BETWEEN도 같음)
  * 숫자형 컬럼인지
    * 자동형변환이 일어나 필터조건으로 사용될 수 있다.
  * 가변 길이 컬럼인지
    * 가변컬럼이면 길이가 다른 값이 같이 조회되어버릴 수 있다.

#### UNION ALL 활용
```oracle
-- 인덱스 : 고객ID + 거래일자
select * from 거래
where :cust_id is null
    and 거래일자 between :dt1 and :dt2
union all
select * from 거래
where :cust_id is not null
    and 거래일자 between :dt1 and :dt2
```
* 위 쿼리는 `:cust_id` 변수에 값에 입력을 하던 안하던 인덱스를 최적으로 활용한다.
* 이 방식은 옵션 조건 컬럼도 인덱스 액세스 조건으로 사용한다.
* 또한 고객ID가 NULL 허용 컬럼이어도 사용하는데 문제가 없다.
* 유일한 단점은 SQL 코딩량이 길어진다는 점이다.

#### NVL/DECODE 함수 활용
```oracle
select * from 거래
where 고객ID = nvl(:cust_id, 고객ID)
and 거래일자 between :dt1 and :dt2

select * from 거래
where 고객ID = decode(:cust_id, null, 고객ID, :cust_id)
  and 거래일자 between :dt1 and :dt2
```
* 고객 ID컬럼을 함수 인자로 사용했는데도 인덱스를 사용할 수 있는 것은 OR Expansion 쿼리변환이 일어났기 때문이다. (UNION ALL 방식으로 변환)
  * 이 기능이 작동하지 않으면 액세스조건으로 사용 불가능하다.
  * 고객ID = 고객ID 형태가 되기 때문에 인덱스에서 이 조건을 만족하는 한 시작점을 찾을 수 없다.
* 이 방식의 큰 장점은 옵션 조건 컬럼을 인덱스 액세스 조건으로 사용할 수 있다는 점이다.
* UNION ALL 보다 단순하면서도 같은 성능을 낼 수 있다.
* 옵션 조건 처리용 NVL/DECODE 함수를 여러개 사용하면 그중 변별력이 가장 좋은 컬럼 기준으로 한 번만 OR Expansion이 일어난다.
  * OR Expansion 기준으로 선택되지 않으면 인덱스 구성컬럼이어도 모두 필터 조건으로 처리된다.
* 즉, NVL/DECODE 함수의 장점에도 불구하고 모든 옵션 조건을 이 방식으로 처리할 수 없다.

> Dynamic SQL
> * Dynamic SQL을 이용해 조건절을 동적으로 구성할 수 있는 환경에서는 위와같은 내용에 대해 공감을 하지 못할 수 있다.
> * Dynamic SQL을 이용해 `=` 연산자를 활용할 경우 변별력있는 컬럼을 액세스 조건으로 사용할 수 있게 인덱스만 잘 구성해주면 된다.
> * 하지만 힌트로 액세스 경로를 고정하려고 할 때 위와같은 튜닝 기법을 적절히 활용해야 한다.
> * Dynamic SQL에 힌트를 명시하면 동적으로 구성된 조건절과 서로 상충하여 성능 문제를 야기할 수 있다.
> * Dynamic SQL을 허용하지 않는 시스템도 있다.(금융권)

### 함수호출부하 해소를 위한 인덱스 구성
#### PL/SQL 함수의 성능적 특성
* PL/SQL 사용자 정의 함수는 생각보다 매우 느리다.
* PL/SQL 사용자 정의 함수가 느린 이유가 있다.
  * 가상머신상에서 실행되는 인터프리터 언어
  * 호출 시마다 컨텍스트 스위칭 발생
  * 내장 SQL에 대한 Recursive Call 발생
* 오라클은 오라클 서버가 아닌 Oracle Form, Oracle Reports 같은 제품에서도 수행될 수 있도록 PL/SQL을 설계하였다.
* 따라서 PL/SQL로 작성한 함수와 프로시저를 컴파일하면 JAVA 언어처럼 바이트코드를 생성해 데이터 딕셔너리에 저장하며, 이를 해석할 수 있는 PL/SQL 엔진만 있으면 어디서든 실행할 수 있다.
  * 바이트 코드를 런타임 시 해석하면서 실행한다.
* 결국 PL/SQL도 인터프리터 언이이기 때문에 Native 코드로 완전 컴파일된 내장 함수에 비해 많이 느리다.
* PL/SQL 함수는 실행 시 SQL 실행엔진과 PL/SQL 가상머신 사이에 컨텍스트 스위칭이 일어난다.
* 따라서 PL/SQL 함수를 작은단위로 모듈화, 공용화하면 안된다.
* PL/SQL 사용자 정의 함수의 성능을 떨어뜨리는 가장 결정적인 요소는 Recursive Call이다.
  ```oracle
  select 회우너번호, 회원명, GET_ADDR(우편번호) as 기본주소
  from 회원
  where 생월일 like '01%'
  ```
  * 조건을 만족하는 회원이 100만명이면 GET_ADDR도 100만번 실행하고, SQL이 내장되어있다면 SQL도 100만번 실행하게 된다.


#### 효과적인 인덱스 구성을 통한 함수 호출 최소화
```oracle
select /*+ full(a) */ 회원번호, 회원명, 생년, 생월일
from 회원 a
where 생년 = '1994' 
    and 암호화된전화번호 = encryption( :phone_no )
```
* 위와 같이 다른 조건절이 있으면, encryption 함수는 그 조건절을 만족하는 만큼만 수행된다.

1. `create index 회원_X01 on 회원(생년);`
   * encryption 함수는 생년 조건을 만족하는 건수만큼 수행된다.
2. `create index 회원_X02 on 회원(생년, 생월일);`
   * encryption 함수는 생년 조건을 만족하는 건수만큼 수행된다.
3. `create index 회원_X03 on 회원(생년, 암호화된전화번호);`
   * encryption 함수는 단 한번만 수행된다.
   * 생년과 함께 암호화된전화번호도 인덱스 액세스 조건으로 사용되기 때문이다.

## 인덱스 설계
* OLTP 시스템에서 인덱스 설계는 매우 중요하다.

### 인덱스 설계가 어려운 이유
* SQL 각각에 최적화된 인덱스를 계속 생성할 수 있다면 SQL튜닝과 인덱스 설계는 매우 쉬운일이 된다.
* 하지만 이렇게 인덱스를 생성하다보면 테이블마다 인덱스가 수십 개씩 달리게 되며, 관리비용 뿐만 아니라 시스템 부하를 증가시키게 된다.
* 인덱스가 많을 경우 아래와같은 문제가 생긴다.
  * DML 성능 저하 (TPS 저하)
  * 데이터베이스 사이즈 증가 (디스크 공간 낭비)
  * 데이터베이스 관리 및 운영 비용 상승
* 인덱스가 여러개 있는 경우, 신규 데이터를 입력할 때마다 여러 인덱스에도 데이터를 입력해야 한다.
  * 테이블과 달리 정렬상태를 유지하기 때문에, 수직적 탐색을 통해 입력할 블록을 찾아야 한다.
  * 찾은 블록에 여유 공간이 없다면 인덱스 분할(Index split)도 발생한다.
* 데이터를 지울 때에도 여러 인덱스에서 일일이 찾아 지워줘야 한다.
* 따라서 개별 쿼리성능 뿐만 아니라, 그 개수를 최소화해야 하므로 인덱스 설계는 어렵다.

> 개발단계에서의 인덱스 설계
> * 운영환경이 되는 순간 인덱스를 변경하는건 쉽지 않다.
> * 그나마 신규 인덱스 추가는 변경 영향도가 작지만, 그럴수록 시스템 수준 TPS는 나빠질 수 밖에 없다.
> * 따라서 개발단계에서 인덱스를 정교하게 설계해야 한다.

### 가장 중요한 두 가지 선택 기준
1. 조건절에 항상 사용하거나 자주 사용하는 컬럼을 선정해야 한다.
2. 이렇게 선정한 컬럼 중 `=`조건으로 자주 조회하는 컬럼을 앞 쪽에 두어야 한다.
* 위 두 선택 기준은 인덱스 스캔 효율성이 판단 기준이다.

### 스캔 효율성 이외의 판단 기준
* 가장 중요한 두 가지 선택 기준만으로 설계를 하다보면, 오히려 인덱스 개수만 더 늘어나는 현상이 발생할 수 있다.
* 즉, 인덱스 스캔 효율성 외 고려해야 할 판단 기준들이 있다.
  * 수행 빈도
  * 업무상 중요도
  * 클러스터링 팩터
  * 데이터량
  * DML 부하
  * 저장 공간
  * 인덱스 관리 비용 등
* 이러한 다양한 판단 기준에 대한 해석이 서로 다르기 때문에, 설계자의 성향이나 스타일에 따라 결과물도 많이 달라진다.

#### 수행빈도
* 여러 판단 기준 중 가장 중요한 하나를 꼽자면 바로 수행빈도이다.
  * 자주 수행하지 않는 SQL이면 인덱스 스캔 과정에 약간의 비효율은 있어도 큰 문제가 안될 수 있기 때문이다.
* 수행빈도가 가장 높은 SQL에는 최적의 인덱스를 구성해줘야 한다.
* NL 조인할 때 어느 쪽에서 자주 액세스되는지도 중요한 판단 기준이 된다.
* NL 조인할 때, Outer쪽에서 액세스하는 인덱스는 스캔과정에 비효율이 있더라도 큰 문제가 아닐 수 있다.
* 하지만 NL 조인에서 Inner 쪽 인덱스 스캔 과정에 비효율이 있다면, 이것은 성능에 큰 문제를 야기할 수 있다.
* 수행빈도가 매우 높은 SQL이라면, 테스트 과정에 성능이 좋게 나오더라도 인덱스를 최적으로 구성해줘야 한다.
* NL조인 Inner 쪽 인덱스는 `=` 조건 컬럼을 선두에 두는것이 중요하고, 될 수 있으면 테이블 액세스 없이 인덱스에서 필터링을 마치도록 구성해야 한다.

#### 데이터량
* 데이터량이 적다면 굳이 인덱스를 많이 만들 필요가 없으며, Full Scan으로도 충분히 빠르다
  * 또한 인덱스를 많이 만들어도 저장 공간이나 트랜잭션 부하 측면에서 그다지 문제될 것이 없다.
  * 즉, 테이블이 작으면 심각하게 고민할 필요가 없다.
* 하지만 초대용량 테이블은 인덱스 설계가 매우 중요하다.
  * 초당 DML 발생량은 TPS에 직접적인 영향을 준다.
  * 인덱스를 하나라도 줄였을 때 그것이 시스템에 미치는 영향은 적지 않다.

### 공식을 초월한 전략적 설계
* SQL 튜닝 전문가라면 가장 핵심적인 액세스 경로 한 두개를 전략적으로 선택해 최적 인덱스를 설계하고, 나머지 액세스 경로는 약간의 비효율이 있더라도 목표 성능을 만족하는 수준으로 인덱스를 구성할 줄 알아야 한다.
* 왜 그렇게 설계를 했는지 근거가 무엇인지 답할 수 있어야 하며, 업무 상황을 이해하고 나름의 판단 기준을 가지고 결정을 내려야 한다.

#### 가계약 테이블 예시
* 이 예시는 인덱스 설계 시, 일자/일시 조건을 선두에 두고 자주 사용하는 필터 조건을 뒤쪽에 추가한다.
  * 일자 조회 구간이 길지 않으면 인덱스 스캔 비효율이 성능에 미치는 영향이 크지 않다.
  * 인덱스 스캔 효율보다 테이블 액세스가 더 큰 부하 요소이다.
    * 가계약은 보통 최근 데이터만 조회하므로, 인덱스 스캔량이 그리 많지 않다.
* 또한, 추가로 가장 많이 사용하는 패턴에 최적의 스캔 효율을 제공하는 인덱스를 추가한다.   

### 소트 연산을 생략하기 위한 컬럼 추가
* 조건절에 사용하지 않는 컬럼이더라도 소트 연산을 생략할 목적으로 인덱스 구성에 포함시킴으로써 성능 개선을 할 수 있다.
```oracle
select 계약ID, 청약일자, 입력자ID, 계약상태코드, 보험시작일자, 보험종료일자
from 계약
where 취급지점ID = :trt_brch_id
  and 청약일자 between :sbcp_dt1 and :sbcp_dt2
  and 입력일자 >= trunc(sysdate - 3)
  and 계약상태코드 in (:crt_stat_cd1, :crt_stat_cd2, :crt_stat_cd3)
order by 청약일자, 입력자ID
```
* 위 예시에서 성능을 고려하지 않고 소트연산을 생략하려고 하면 `[청약일자 + 입력자ID]`로 인덱스를 구성하면 된다.
* `=`조건절 컬럼은 ORDER BY 절에 없더라도 인덱스 구성에 포함할 수 있다.
  * `[청약일자 + 취급지점ID + 입력자ID]` 순으로 구성해도 소트연산을 생략할 수 있다. (앞뒤 중간 어디에 두어도 상관 없음)  
* `=`이 아닌 조건절들은 반드시 ORDER BY 컬럼보다 뒤쪽에 두어야 소트연산을 생략할 수 있다.
* I/O를 최소화하면서 소트 연산을 생략하려면 아래와 같은 공식으로 인덱스를 구성하면 된다.
  * `=` 연산자로 사용한 조건절 컬럼 선정
  * ORDER BY 절에 기술한 컬럼 추가
  * `=` 연산자가 아닌 조건절 컬럼은 데이터 분포를 고려해 추가 여부 결정
* 따라서 인덱스를 `[취급지점ID + 청약일자 + 입력자ID]` 순으로 구성하면 된다.

#### IN 조건은 `=`이 아니다.
```oracle
-- 인덱스 : 거주지역 + 혈액형 + 연령
select 고객번호, 고객명, 거주지역, 혈액형, 연령
from 고객
where 거주지역 = '서울'
  and 혈액형 in ('A', 'O')
order by 연령
```
* IN 조건이 `=`이 되려면 IN-List Iterator 방식으로 풀려야 한다.
* 그러면 IN 조건이 `=`이 되었지만 UNION ALL 위아래 두 집합을 묶어 연령 순으로 정렬하는 문제가 남게 된다.
* 따라서 소트연산을 생략하려면 IN 조건절이 IN-List Iterator 방식으로 풀려선 안된다.
* 즉, 위 예시는 `[거주지역 + 연령 + 혈액형]`이 되어야 한다.

### 결합 인덱스 선택도
* 인덱스 생성 여부를 결정할 때, 선택도가 충분히 낮은지가 중요한 판단 기준이다.
* 선택도가 높은 인덱스는 생성해봐야 효용가치가 별로 없다.
  * 테이블액세스가 많이 발생하기 때문이다.

#### 컬럼 순서 결정 시 선택도
* 인덱스 생성 여부를 결정할 때 선택도가 매우 중요하지만, 결합인덱스의 컬럼간 순서를 결정할 때 각 컬럼의 선택도보다는 필수조건의 여부나 연산자의 형태가 더 중요하다
  * 모두 `=` 조건이라면 선택도가 낮은 컬럼이 앞에 오든 뒤에오든 효율에 영향이 없다. 

### 중복 인덱스 제거
* 아래 3개의 인덱스는 완전 중복이므로, 위 2개의 인덱스는 모두 지워도 된다.
  * `[계약ID + 청약일자]`
  * `[계약ID + 청약일자 + 보험개시일자]`
  * `[계약ID + 청약일자 + 보험개시일자 + 보험종료일자]`
  

* 아래 4개의 인덱스는 얼핏 보기엔            중복은 아니다.
  * `[계약ID + 청약일자]`
  * `[계약ID + 보험개시일자]`
  * `[계약ID + 보험종료일자]`
  * `[계약ID + 데이터생성일시]`
* 하지만 계약ID의 카디널리스트가 매우 낮으면 사실상 중복이다. 
  * `[계약ID + 청약일자 + 보험개시일자 + 보험종료일자 + 데이터생성일시]` 와 같이 하나만 만들어도 충분하다.

#### 중복인덱스 실습예시
* p245 ~ p249 참고

### 인덱스 설계도 작성
* 인덱스 설계 시 시스템 전체 효율을 고려해야 한다.
* 따라서 인덱스 설계에도 설계도면이 필요하다.
* p249 참고

# 조인 튜닝
## NL 조인
### 기본 메커니즘
* NL조인은 말그대로 중첩 루프문과 같은 수행구조를 사용한다.
* 일반적으로 NL 조인은 Outer와 Inner 양쪽 테이블 모두 인덱스를 이용한다.
* Outer쪽 테이블은 사이즈가 크지 않으면 인덱스를 이용하지 않을 수 있다.
  * Table Full Scan 하더라도 한번에 그치기 때문이다.
* 반면에, Inner쪽 테이블은 인덱스를 사용해야 한다.
  * Inner 루프에서 인덱스를 이용하지 않으면 Outer 루프에서 읽은 건수만큼 Table Full Scan을 반복하기 때문이다.
* 따라서 NL조인은 인덱스를 이용한 조인 방식이라고 할 수 있다.
* 소트 머지 조인과 해시 조인도 가공해둔 데이터를 이용한다는 점만 다를 뿐 기본적인 프로세싱은 다르지 않다.

### NL 조인 실행계획 제어
* NL조인은 각 테이블을 액세스할 때 인덱스를 이용한다.
* NL조인을 제어할 때에는 `use_nl` 힌트를 사용하면 된다.
  * `ordered` 힌트는 FROM절에 기술한 순서대로 조인하라고 지시할 때 사용한다.

* 아래는 A, B, C, D순으로 조인하되 B, C와조인할때는 NL 방식으로 조인하고, D와 조인할 때는 해시 방식으로 조인하라는 뜻이다.
  ```oracle
  select /*+ ordered use_nl(B) use_nl(C) use_hash(D) */
  from A, B, C, D
  where ...
  ```
  
* `ordered` 대신 `leading` 힌트를 사용할 수도 있다.
  ```oracle
  select /*+ leading(C, A, D, B) use_nl(A) use_nl(D) use_hash(B) */
  from A, B, C, D
  where ...
  ```

* 네 개의 테이블을 NL방식으로 조인하되 순서는 옵티마이저가 정하도록 맡긴 것이다.
  ```oracle
  select /*+ ordered use_nl(A, B, C, D) */
  from A, B, C, D
  where ...
  ```

### Nl 조인 수행 과정 분석
```oracle
select /*+ ordered use_nl(c) index(e) index(c) */
       e.사원번호, e.사원명, e.입사일자
     , c.고객번호, c.고객명, c.전화번호, c.최종주문금액
from 사원 e, 고객 c
where c.관리사원번호 = e.사원번호
  and e.입사일자 >= '19960101'
  and e.부서코드  = 'Z123'
  and c.최종주문금액 >= 20000
```
* 인덱스 구성
  * `[사원_PK : 사원번호]`
  * `[사원_X1 : 입사일자]`
  * `[고객_PK : 고객번호]`
  * `[고객_X1 : 관리사원번호]`
  * `[고객_X2 : 최중주문금액]`
* 사용되는 인덱스는 사원_X1, 고객_X1이다.
* p263 그림 참고


### NL 조인 튜닝 포인트
* 위 예시의 NL조인 튜닝포인트들이 있다.
* 튜닝포인트 1
  * 사원_X1 인덱스를 읽고 나서 테이블을 액세스 하는 부분이다.
  * 사원테이블이 아주 많은 양의 랜덤액세스가 발생할 경우 테이블에서 부서코드에 의해 필터링되는 비율이 높을 수 있다.
  * 인덱스에서 부서코드 컬럼을 추가하는 방안을 고려해야 한다.
* 튜닝포인트 2
  * 고객_X1 인덱스 탐색 부분이다.
  * 조인 액세스 횟수는 Outer 테이블을 읽고 필터링한 결과 건수에 의해 결정된다.
  * 예시로, 부서코드까지 만족하는 레코드가 10만건이고, 고객_X1의 인덱스 Depth가 3이라면 수직적 탐색 과정에서만 30만개 블록을 읽어야 하며, 수평적 스캔 과정에 추가적인 블록 I/O가 생긴다.
* 튜닝포인트 3
  * 고객_X1 인덱스를 읽고 고객 테이블을 액세스하는 부분이다.
  * 최종주문금액 조건에 의해 필터링되는 비율이 높다면 고객_X1 인덱스에 최종주문금액 컬럼을 추가하는 방안을 고려해야 한다.
* 튜닝포인트 4
  * 맨 처음 액세스하는 사원_X1 인덱스에서 얻은 결과 건수에 의해 전체 일량이 좌우된다.
  * 따라서, 사원_X1 인덱스를 스캔하면서 추출한 레코드가 많으면, 사원 테이블로 랜덤액세스하는 횟수, 고객_X1 인덱스를 탐색하는 횟수, 고객테이블로 랜덤액세스하는 횟수가 많아질 수 밖에 없다.

#### 올바른 조인 메소드 선택
* OLTP 시스템에서 튜닝할 때는 일차적으로 NL조인부터 고려하는것이 올바른 순서이다.
* 성능이 느리다면 위 튜닝포인트에 따라 각 단계의 수행 일량을 분석해서 과도한 랜덤액세스가 발생하는 지점을 우선 파악해야 한다.
  * 조인순서를 변경해 랜덤액세스 발생량을 줄일 수 있는지, 더 효과적인 다른 인덱스가 있는지 등을 검토해야 하며, 인덱스 추가 및 구성 변경도 고려해본다.
* 여러 방안을 검토 후, NL 조인으로 결코 좋은 성능을 내기 어렵다고 판단되면, 소트머지 조인이나 해시조인을 검토한다.

### NL 조인 특징 요약
1. 랜덤 액세스 위주의 조인 방식이다.
   * 대량 데이터 처리 시 매우 치명적인 한계가 있다.
2. 조인을 한 레코드씩 순차적으로 진행한다.
   * 아무리 큰 테이블을 조인하더라도 매우 빠른 응답속도를 낼 수 있다.
   * 부분범위처리가 가능한 상황에서 더더욱 그렇다.
3. 다른 조인방식과 비교할 때 인덱스 구성 전략이 특히 중요하다.
   * 인덱스에 따라 조인 효율이 크게 달라진다.
* 따라서 NL 조인은 소량 데이터를 주로 처리하거나, 부분범위 처리가 가능한 시스템에 적합한 조인방식이다. (주로 OLTP)

### NL 조인 튜닝 실습
```oracle
select /*+ ordered use_nl(c) index(e) index(c) */
       e.사원번호, e.사원명, e.입사일자
     , c.고객번호, c.고객명, c.전화번호, c.최종주문금액
from 사원 e, 고객 c
where c.관리사원번호 = e.사원번호
  and e.입사일자 >= '19960101'
  and e.부서코드 = 'Z123'
  and c.최종주문금액 >= 20000

```
#### 인덱스 튜닝 관점
* 위 예시에서, 사원 테이블에 액세스한 횟수가 많고, 테이블에서 필터링한 결과가 적을 경우 인덱스에 테이블 필터 조건인 `부서코드`를 추가하는 것을 고려할 필요가 있다.
* 하지만 인덱스 스캔 단계에서의 일량도 확인해야 한다.
* `입사일자 >= '19960101'` 조건에 해당하는 레코드가 아주 많다면, 그만큼 인덱스 블록을 스캔하면서 `부서코드 = 'Z123'` 조건을 필터링했을 것이다.
* 따라서 이럴 경우엔, 인덱스를 `부서코드 + 입사일자` 순으로 구성해주면 된다. (물론 쿼리에 미치는 영향도 분석이 선행되어야 함)

#### 조인 튜닝 관점
* 사원테이블을 비효율 없이 읽고 고객테이블과 조인하는 과정에서, 조인횟수는 많지만 필터링까지 마친 결과 집합은 적을 수도 있다.
* 이럴 때는 조인 순서 변경을 고려해볼 수 있다.
* 예를들면 최종주문금액 조건을 만족하는 레코드가 별로 없다면 튜닝에 성공할 가능성이 있다.
* 하지만 그 반대의 결과가 나타날 수도 있다.(최종주문금액 단독으로 조회 시 데이터량이 훨씬 많아질 수도 있음)
* 따라서 조인 순서를 바꿔도 별 소득이 없다면, 소트 머지 조인과 해시 조인을 검토해야 한다.

### NL 조인 확장 메커니즘
* 버전이 올라가면서 오라클은 NL조인 성능을 높이기 위해 테이블 Prefetch, 배치 I/O 기능을 도입했다.
  * 테이블 Prefetch : 인덱스를 이용해 테이블을 액세스하다가 디스크 I/O가 필요해지면 이어서 곧 읽게 될 블록까지 미리 읽어 버퍼캐시에 적재하는 기능
  * 배치 I/O : 디스크 I/O Call을 미뤘다가 읽을 블록이 일정량 쌓이면 한꺼번에 처리하는 기능
* 튜닝하는 과정에서 이 기능을 표현한 실행계획들을 자주 볼 수 있다.

  1) 정통적인 실행계획
  ```
  5 NESTED LOOPS
  3   TABLE ACCESS BY INDEX ROWID OF 사원
  5     INDEX RANGE SCAN OF 사원_X1
  5   TABLE ACCESS BY INDEX ROWID OF 고객
  8     INDEX RANGE SCAN OF 고객_X1
  ```

  2) 테이블 Prefetch 실행계획
  ```
  5  TABLE ACCESS BY INDEX ROWID OF 고객
  12   NESTED LOOPS
  3      TABLE ACCESS BY INDEX ROWID OF 사원
  3        INDEX RANGE SCAN OF 사원_X1
  8      INDEX RANGE SCAN OF 고객_X1
  ```
  
  * Inner 쪽 테이블에 대한 디스크 I/O 과정에 테이블 Prefetch 기능이 작동할 수 있음을 표시하기 위함이다.
  * `nlj_prefetch`, `no_nlj_prefetch` 힌트를 이용해 이 실행계획을 제어할 수 있다.

  3) 배치 I/O 실행계획
  ```
  5  NESTED LOOPS
  12   NESTED LOOPS
  3      TABLE ACCESS BY INDEX ROWID OF 사원
  3        INDEX RANGE SCAN OF 사원_X1
  8      INDEX RANGE SCAN OF 고객_X1
  5    TABLE ACCESS BY INDEX ROWID OF 고객
  ```
  * Inner 쪽 테이블에 대한 디스크 I/O 과정에 배치 I/O 기능이 작동할 수 있음을 표시하기 위함이다.
  * `nlj_batching`, `no_nlj_batching` 힌트를 이용해 이 실행계획을 제어할 수 있다. 

* Inner 쪽 테이블 블록을 모두 버퍼캐시에서 읽는다면 어떤 방식으로 수행하든 성능에 차이는 없고, 데이터 출력 순서도 100% 같다.
* 하지만 일부를 디스크에서 읽게 되면 성능에 차이가 나타날 수 있고, 배치 I/O 실행계획이 나타날 때는 결과집함의 정렬 순서도 다를 수 있어 주의가 필요하다.
  * 12c에 도입된 일반 배치 I/O가 데이터 정렬 순서를 보장하지 않는다.
  * 11g부터 NL 조인 Inner 쪽 테이블에 작동하는 배치 I/O 기능도 결과집합의 정렬 순서를 보장하지 않는다.
* 11g부터 NL 조인 결과집합이 항상 일정한 순서로 출력되기를 원하면 배치 I/O 기능이 작동하지 못하도록 `no_nlj_batching` 힌트를 추가하거나, 맨 바깥쪽 ORDER BY 절에 정렬 기준을 명시해야 한다.
  * 11g에서 바깥쪽 메인 쿼리에 ORDER BY를 추가했어도 안쪽 ORDER BY를 함부로 제거하면 안된다.
  * Top N 쿼리를 구현하기 위한 것 때문이다.

## 소트 머지 조인
* 조인 컬럼에 인덱스가 없을 때, 대량 데이터 조인어어서 인덱스가 효과적이지 않을 때 옵티마이저는 NL 조인 대신 소트 머지 조인이나 해시 조인을 선택한다.
* 해시 조인의 등장으로 소트 머지 조인의 쓰임새가 예전만 못하지만, 해시 조인을 사용할 수 없는 상황에서 대량 데이터를 조인하고자 할 땐 여전히 유용하다.

### SGA vs PGA
* 공유 메모리 영역인 SGA에 캐시된 데이터는 여러 프로세스가 공유할 수 있지만, 동시에 액세스할 수는 없다.
* 동시에 액세스하려는 프로세스 간 액세스를 직렬화하기 위한 Lock 메커니즘으로 래치가 존재한다.
* 테이블 블록과 인덱스 블록을 캐싱하는 DB 버퍼 캐시는 SGA의 가장 핵심적인 구성요소이며, 여기서 블록을 읽으려면 버퍼 Lock도 얻어야 한다.
* 오라클 서버 프로세스는 SGA에 공유된 데이터를 읽고 쓰면서, 동시에 자신만의 고유 메모리 영억을 갖는다.
* 각 오라클 서버 프로세스에 할당된 메모리 영역을 PGA(Precess/Program/Private Global Area)라고 부르며, 프로세스에 종속적인 고유 데이터를 저장하는 용도로 사용한다.
  * PGA 공간이 작아 데이터를 모두 저장할 수 없을 때는 Temp 테이블스페이스를 이용한다.
* **PGA는 다른 프로세스와 공유하지 않는 독립적인 메모리 공간이므로, 래치 메커니즘이 불필요하다.**
* **따라서 같은 양의 데이터를 읽더라도 SGA 버퍼캐시에서 읽을때보다 훨씬 빠르다.**

### 기본 메커니즘
* 소트 머지 조인은 이름이 의미하는것처럼 두 단계로 진행한다.
  * 소트 단계 : 양쪽 집합을 조인 컬럼 기준으로 정렬한다.
  * 머지 단계 : 정렬한 양쪽 집합을 서로 머지한다.
* 소트 머지 조인은 `use_merge` 힌트로 유도할 수 있다.

```oracle
select /*+ ordered use_merge(c) */
       e.사원번호, e.사원명, e.입사일자
     , c.고객번호, e.고객명, e.전화번호, e.최종주문금액
from 사원 e, 고객 c
where c.관리사원번호 = e.사원번호
  and e.입사일자 >= '19960101'
  and e.부서코드 = 'Z123'
  and c.최종주문금액 >= 20000
```

1) 조건에 해당하는 사원 데이터를 읽어 조인컬럼인 `사원번호` 순으로 정렬한다.
   * 정렬한 결과집합은 PGA영역에 할당된 Sort Area에 저장한다.
   * PGA에 담을 수 없도록 크면 Temp 테이블스페이스에 저장한다.

2) 조건에 해당하는 고객데이터를 읽어 조인컬럼인 `관리사원번호` 순으로 정렬한다.
   * 정렬한 결과집합은 PGA영역에 할당된 Sort Area에 저장한다.
   * PGA에 담을 수 없도록 크면 Temp 테이블스페이스에 저장한다.
3) PGA에 저장한 사원 데이터를 스캔하면서, PGA에 저장된 고객데이터와 조인한다.

* 이 때 사원데이터를 기준으로 고객 데이터를 매번 Full Scan하지 않는다.
* 고객 데이터가 정렬되어 있으므로, 조인 대상 레코드가 시작되는 지점을 쉽게 찾을 수 있고, 조인에 실패하는 레코드를 만나는 순간 바로 멈출 수 있다.
* 따라서 Sort Area에 저장한 데이터 자체가 인덱스 역할을 하므로, 소트 머지 조인은 조인 컬럼에 인덱스가 없어도 사용할 수 있는 조인 방식이다.
* 조인 컬럼에 인덱스가 있어도 NL 조인은 대량 데이터를 조인할 때 불리하므로, 소트 머지 조인을 사용할 수 있다.

### 소트 머지 조인이 빠른 이유
1) NL 조인
   * NL 조인의 치명적인 단점은, 대량 데이터를 조인할 때 성능이 매우 느리다는 것이다.
     * 소트 머지 조인과 해시 조인이 개발된 이유이다.
   * NL 조인은 인덱스를 이용하는 조인 방식으로, 조인 과정에서 액세스하는 모든 블록을 랜덤 액세스 방식으로 건건이 DB 버퍼캐시를 경유해 읽는다.
   * 즉, 인덱스든 테이블이든 읽는 모든 블록에 래치획득 및 캐시버퍼 체인 스캔 과정을 거친다.
   * 버퍼캐시에서 찾지 못한 블록은 건건이 디스크에서 읽어 들인다.
   * 따라서 인덱스를 이용하기 때문에 인덱스 손익분기점 한계를 그대로 드러낸다.
   * 이것이 대량 데이터 조인에 NL조인이 불리한 이유이다.
2. 소트 머지 조인
   * 소트 머지 조인은 양쪽 테이블로부터 조인 대상 집합을 읽어 PGA에 저장한 후 조인한다.
   * PGA는 프로세스만을 위한 독립적인 메모리 공간이므로, 데이터를 읽을 때 래치 획득 과정이 없다.
   * 소트 머지 조인이 대량 데이터 조인에 유리한 이유이다.

* 양쪽 집합에 대한 소트 연산을 추가로 수행하므로 NL조인보다 느릴 수 있다고 생각하겠지만, 이것이 오히려 소트머지 조인을 대량 데이터 조인에 유리하게 만든 핵심 요인이다.
* 하지만 소트머지 조인도 양쪽 테이블로부터 조인 대상 집합을 읽을 때는 DB 버퍼캐시를 경유한다.
  * 이 때 인덱스를 이용하기도 하고, 이 과정에서 생기는 버퍼캐시 탐색 비용과 랜덤액세스 부하는 소트머지 조인도 피할 순 없다.

### 소트 머지 조인의 주용도
* 랜덤 액세스 위주의 NL 조인이 대량 데이터 처리에 한계를 보일 때 소트 머지 조인이 해결책으로 많이 사용되던 시절이 있었다.
* 하지만 해시 조인의 등장으로 이제 소트 머지 조인의 쓰임새는 예전만 못한데, 대부분 해시조인이 더 빠르기 때문이다.
* 하지만 해시조인은 조건이 등치(`=`) 조건이 아닐 때 사용할 수 없다는 단점이 있다.
* 따라서 아래와 같은 상황에서 많이 사용된다.
  * 조인 조건식이 등치(`=`) 조건이 아닌 대량 데이터 조인
  * 조인 조건식이 아예 없는 조인(Cross Join, 카테시안 곱)

### 소트 머지 조인 제어하기
* 소트 머지 조인의 실행 계획 예시이다.
```
0    SELECT STATEMENT Optimizer=ALL_ROWS
1  0   MERGE JOIN
2  1     SORT (JOIN)
3  2       TABLE ACCESS (BY INDEX ROWID) OF '사원' (TABLE)
4  3         INDEX (RAGE SCAN) OF '사원_X1' (INDEX)
5  4     SORT (JOIN)
6  5       TABLE ACCESS (BY INDEX ROWID) OF '고객' (TABLE)
7  6         INDEX (RAGE SCAN) OF '고 객_X1' (INDEX)
```
* 소트 머지 조인 실행계획을 제어할 때에는 `use_merge` 힌트를 사용하면 된다.

### 소트머지 조인 특징 요약
* 소트 머지 조인은 조인을 위해 실시간으로 인덱스를 생성하는 것과 비슷하다.
  * 양쪽 집합을 정렬한 다음 NL 조인과 같은 방식으로 진행하지만, PGA영역에 저장한 데이터를 이용하기 때문에 빠르다.
  * 따라서 소트 부하만 감수한다면 건건이 버퍼캐시를 경유하는 NL 조인보다 빠르다.
* NL 조인은 조인 컬럼에 대한 인덱스 유무에 크게 영향을 받지만, 소트 머지 조인은 영향을 받지 않는다.
  * 양쪽 집합을 개별적으로 읽고 나서 조인을 시작한다.
  * 즉, 조인 컬럼에 인덱스가 없는 상황에서 두 테이블을 읽어 조인 대상 집합을 줄일 수 있을 때 유리하다.
* 스캔 위주의 액세스 방식을 사용한다는 점도 중요한 특징이다.
  * 하지만 모든 처리가 스캔 방식으로 이루어지진 않는다
  * 양쪽 소스 집합으로부터 조인 대상 레코드를 찾는 데 인덱스를 이용할 수 있고, 그때는 랜덤 액세스가 일어난다. (해시 조인도 마찬가지임)

## 해시조인
* NL 조인은 인덱스를 이용한 조인 방식으로, 인덱스 구성에 따른 성능 차이가 심하다.
* 또한 인덱스를 아무리 완벽하게 구성해도 랜덤 I/O 때문에 대량 데이터 처리에 불리하고, 버퍼 캐시 히트율에 따라 들쭉날쭉한 성능을 보인다.
* 소트 머지 조인과 해시 조인은 조인 과정에 인덱스를 이용하지 않기 때문에 대량 데이터 조인할 때 NL 조인보다 훨씬 빠르고 일정한 성능을 보인다.
* 소트 머지 조인은 항상 양쪽 테이블을 정렬하는 부담이 있는데, 해시 조인은 그런 부담이 없지만, 모든 조인을 해시 조인으로 처리할 순 없다.

### 기본 메커니즘
* 해시 조인은 아래와 같이 두 단계로 진행된다.
  * Build 단계 : 작은 쪽 테이블(Build Input)을 읽어 해시 테이블(해시 맵)을 생성한다.
  * Probe 단계 : 큰 쪽 테이블(Probe Input)을 읽어 해시 테이블을 탐색하면서 조인한다.
* 해시 조인은 `use_hash` 힌트로 유도할 수 있다.

```oracle
select /*+ ordered use_hash(c) */
       e.사원번호, e.사원명, e.입사일자
     , c.고객번호, c.고객명, c.전화번호, c.최종주문금액
from 사원 e, 고객 c
where c.고객사원번호 = e.사원번호
  and e.입사일자 >= '19960101'
  and e.부서코드 = 'Z123'
  and c.최종주문금액 >= 20000
```
1) Build 단계
   * 조건에 해당하는 사원 데이터를 읽어 해시 테이블을 생성한다.
   * 이 때, 조인 컬럼인 사원번호를 해시 테이블 키 값으로 사용한다.
   * 사원번호를 해시 함수에 입력 반환된 값으로 해시 체인을 찾고, 그 해시 체인에 데이터를 연결한다.
   * 해시 테이블은 PGA 영역에 할당된 Hash Area에 저장하며, 해시 테이블이 너무 커서 PGA에 담을 수 없다면 Temp 테이블스페이스에 저장한다.
2) Probe 단계
   * 조건에 해당하는 고객 데이터를 하나씩 읽어 앞에 생성한 해시 테이블을 탐색한다.
   * 관리 사원번호를 해시 함수에 입력해 반환된 값으로 해시 체인을 찾고, 그 해시 체인을 스캔해서 값이 같은 사원번호를 찾는다.
   * 찾으면 조인에 성공한 것이고, 못찾으면 실패한 것이다.

> 해시 테이블에 담기는 정보
> * 해시 테이블에는 조인 키값만 저장하지 않는다.
> * 조인 키값뿐만 아니라 SQL에 사용한 모든 컬럼을 저장한다.

* Build 단계에서 사용한 해시 함수를 Probe 단계에서도 사용하기 때문에, 같은 사원번호를 입력하면 같은 해시 값을 반환한다.
* 즉 해시 함수가 반환하 값에 해당하는 해시 체인만 스캔하면 된다.

### 해시 조인이 빠른 이유
* Hash Area에 생성한 해시 테이블을 이용한다는 점만 다르고, 조인 프로세싱 자체는 NL 조인과 같다.
* 그럼에도 해시 조인이 인덱스 기반의 NL 조인보다 빠른 이유는, 해시 테이블을 PGA영역에 할당하기 때문이다.
* NL 조인은 Outer 테이블 레코드마다 Inner 쪽 테이블 레코드를 읽기 위해 래치 획득 및 캐시버퍼 체인 스캔 과정을 반복하지만, 해시 조인은 그러한 과정 없이 PGA에서 빠르게 데이터를 탐색하고 조인할 수 있다.
* 해시 조인도 Build Input, Probe Input 각 테이블을 읽을 때는 DB 버퍼캐시를 경유하며, 이 때 인덱스를 이용하기도 한다.
* 따라서 이 과정에서 생기는 버퍼캐시 탐색 비용과 랜덤 액세스 부하는 해시 조인도 피할 수 없다.

#### 해시 조인 vs 소트 머지 조인
* 해시 조인과 소트 머지 조인 둘 다 오퍼레이션을 PGA에서 처리한다는 공통점이 있다.
* 하지만 일반적으로는 해시 조인이 더 빠르다.
* 소트 머지 조인에서 사전 준비 작업은 양쪽 집합을 모두 정렬해 PGA에 담는 작업이다.
* PGA는 큰 메모리 공간이 아니기 때문에 두 집합중 하나가 크다면 Temp 테이블스페이스, 즉 디스크에 쓰는 작업을 반드시 수행한다.
* 하지만 해시 조인에서 사전 준비작업은 양쪽 집합 중 한쪽을 읽어 해시 맵을 만드는 작업이다.
* 해시 조인은 둘 중 작은 집합을 해시 맵 Build Input으로 선택하기 때문에, 두 집합이 모두 Hash Area에 담을 수 없는정도로 큰 경우가 아니면 Temp 테이블스페이스, 즉 디스크에 쓰는 작업은 전혀 일어나지 않는다.
* 즉, 해시 조인은 NL조인처럼 조인 과정에서 발생하는 랜덤 액세스 부하도 없고, 소트 머지 조인처럼 양쪽 집합을 미리 정렬하는 부하도 없다.
  * 해시 테이블을 생성하는 비용이 수반되지만, 둘 중 작은 집합을 Build Input으로 선택하기 때문에 보통 부담이 크지 않다.
  * Build Input이 Hash Area 크기를 초과하면 다른 조인을 해야 한다는 의미는 아니이고, Temp 테이블스페이스를 쓰게 되더라도 대량 데이터 조인 시에는 일반적으로 해시 조인이 빠르다. 

### 대용량 Build Input 처리
* 모두 대용량 테이블인 T1, T2 테이블이 있고, 두 테이블 모두 대용량 테이블이여서 인메모리 해시 조인이 불가능한 상황이다.
* 이럴 때 DBMS는 아래 두 단계로 나눠서 진행한다. (분할 정복 방식)
  1) 파티션 단계
     * 조인하는 양쪽 집합의 조인 컬럼에 해시 함수를 적용하고, 반환된 해시 값에 따라 동적으로 파티셔닝한다.
     * 독립적으로 처리할 수 있는 여러 개의 작은 서브 집합으로 분할함으로 파티션 짝(pair)을 생성하는 단계이다.
     * 양쪽 집합을 읽어 디스크 Temp 공간에 저장해야 하므로 인메모리 해시 조인보다 성능이 많이 떨어진다.
  2) 조인 단계
     * 파티션 단계를 완료하면 각 파티션 짝(pair)에 대해 하나씩 조인을 수행한다.
     * 이 때, 각각에 대한 Build Input과 Probe Input은 독립적으로 결정된다.
     * 파티션하기 전 어느 쪽이 작은 테이블이었는지에 상관없이 각 파티션 짝 별로 작은 쪽을 Build Input으로 선택하고 해시 테이블을 생성한다.
     * 해시 테이블을 생성하고 나면 반대쪽 파티션 로우를 하나씩 읽으면서 해시 테이블을 탐색한다.
     * 모든 파티션 짝에 대한 처리를 마칠 때까지 이 과정을 반복한다.

### 해시 조인 실행계획 제어
```
0    SELECT STATEMENT Optimizer=ALL_ROWS
1  0   HASH JOIN
2  1       TABLE ACCESS (BY INDEX ROWID) OF '사원' (TABLE)
3  2         INDEX (RAGE SCAN) OF '사원_X1' (INDEX)
4  3       TABLE ACCESS (BY INDEX ROWID) OF '고객' (TABLE)
5  4         INDEX (RAGE SCAN) OF '고 객_X1' (INDEX)
```
* 위 실행계획에서, 위쪽 데이터로 해시 테이블을 생성한 후 아래쪽 테이블에서 읽은 조인 키값으로 해시 테이블을 탐색하면서 조인한다고 해석하면 된다.
* 해시 조인 실행계획을 제어할 때 `use_hash` 힌트를 사용하면 된다.
  * 이 힌트만 사용하면 Build Input을 옵티마이저가 선택하는데, 일반적으로 둘 중 조건절에 대한 카디널리티가 작은 테이블을 선택한다.
* 오라클은 기본적으로 `leading`이나 `ordered` 과같은 힌트로 지시한 순서에 따라 가장 먼저 읽는 테이블을 Build Input으로 선택한다.
* `swap_join_inputs` 힌트로 Build Input을 명시적으로 선택할 수도 있다.
```oracle
select /*+ leading(e) use_hash(c) swap_join_inputs(c) */
       e.사원번호, e.사원명, e.입사일자
     , c.고객번호, c.고객명, c.전화번호, c.최종주문금액
from 사원 e, 고객 c
where c.고객사원번호 = e.사원번호
  and e.입사일자 >= '19960101'
  and e.부서코드 = 'Z123'
  and c.최종주문금액 >= 20000
```

#### 세 개 이상 테이블 해시 조인
* A, B, C 세 개의 테이블이 있을 때 이 세 개의 테이블을 조인하는 경로가 아래와 같다고 가정한다.
  * 경로1 : A와 B, B,와 C
  * 경로2 : A와 B, A 와 C
* 이 경우를 결국 정리해보면 아래와 같다.
  * A <-> B <-> C
  * B <-> A <-> C
* 결국 T1 <-> T2 <-> T3의 형식이 된다는 것이다.


#### 세 개 이상 테이블 해시조인 유도 예시
```oracle
select /*+ leading(T1, T2, T3) use_hash(T2) use_hash(T3) */
from T1, T2, T3
where T1.key = T2.key
  and T2.key = T3.key
```
* `leading` 힌트 첫 번째 파라미터로 지정한 테이블은 무조건 Build Input으로 선택된다.
* 따라서 위와 같이 힌트를 지정했을 때에는 아래와 같은 실행계획이 나올 수 있다.
```
0     SELECT STATEMENT Optimizer=ALL_ROWS
 1  0    HASH JOIN
 2  1      HASH JOIN
 3  2        TABLE ACCESS (FUL) OF 'T1' (TABLE)
 4  2        TABLE ACCESS (FUL) OF 'T2' (TABLE)
 5  1      TABLE ACCESS (FULL) OF 'T3' (TABLE)
```
```
 0     SELECT STATEMENT Optimizer=ALL_ROWS
 1  0    HASH JOIN
 2  1      TABLE ACCESS (FULL) OF 'T3' (TABLE)
 3  1      HASH JOIN
 4  3        TABLE ACCESS (FUL) OF 'T1' (TABLE)
 5  3        TABLE ACCESS (FUL) OF 'T2' (TABLE)
```
* 이 때, T2를 Build Input으로 선택하고 싶다면 아래와 같이 `swap_join_inputs`힌트를 사용하면 된다.
  ```
  select /*+ leading(T1, T2, T3) swap_join_inputs(T2) */
  ```

```
0     SELECT STATEMENT Optimizer=ALL_ROWS
 1  0    HASH JOIN
 2  1      HASH JOIN
 3  2        TABLE ACCESS (FUL) OF 'T2' (TABLE)
 4  2        TABLE ACCESS (FUL) OF 'T1' (TABLE)
 5  1      TABLE ACCESS (FULL) OF 'T3' (TABLE)
```
```
 0     SELECT STATEMENT Optimizer=ALL_ROWS
 1  0    HASH JOIN
 2  1      TABLE ACCESS (FULL) OF 'T3' (TABLE)
 3  1      HASH JOIN
 4  3        TABLE ACCESS (FUL) OF 'T2' (TABLE)
 5  3        TABLE ACCESS (FUL) OF 'T1' (TABLE)
```
* 여기서 패턴 1를 패턴 2로 바꾸고 싶다면, T3를 Build Input으로 선택하려는 것이므로 아래와 같이 힌트를 사용하면 된다.
  ```
  select /*+ leading(T1, T2, T3) swap_join_inputs(T3) */
  select /*+ leading(T1, T2, T3) swap_join_inputs(T2) swap_join_inputs(T3) */
  ```

* 패턴 2를 패턴 1로 바꾸고 싶을 때에는, `no_swap_join_inputs` 힌트를 사용하면 된다.
  * T3를 Probe Input으로 선택해주는 방식이다.
  ```oracle
  select /*+ leading(T1, T2, T3) no_swap_join_inputs(T3) */
  ```
  * 이 힌트는 오라클 10.1.0.3 버전부터 제공한다.

### 조인 메소드 선택 기준
* 여해시 조인이 워낙 빠르다 보니 내부 수행원리를 모르는 개발자들은 웬만하면 해시조인으로 처리하려는 유혹에 빠지기 쉽다.
  * 인덱스 설계에 공들이지 않아도 되니 편하기까지 하기 때문이다.
* 이러한 사고는 굉장히 위험한데, 특히 수행빈도가 매우 높은 쿼리에 대해선 더더욱 그렇다.
* 일반적인 조인 메소드 선택 기준은 아래와 같다.
  * 소량 데이터 조인 시 : NL 조인
  * 대량 데이터 조인 시 : 해시 조인
  * 대량 데이터 조인인데 해시조인으로 처리할 수 없을 때 (조인 조건식이 등치(`=`) 조건이 아닐 때) : 소트 머지 조인
* 여기서 대량의 기준은, NL 조인 기준으로 최적화 했는데도 랜덤 액세스가 많아 만족할만한 성능을 낼 수 없는 경우를 의미한다.
* 수행 빈도가 매우 높은 쿼리에 대해서는 아래와 같은 기준을 적용할 수 있다.
  * NL 조인과 해시 조인 성능이 같은 경우 : NL 조인
  * 해시 조인이 약간 더 빠른경우 : NL 조인
  * NL 조인보다 해시 조인이 매우 빠른 경우 : 해시 조인

#### NL조인을 가장 먼저 고려해야 하는 이유
* NL 조인 위주로 처리하려면 인덱스를 세밀하게 설계해야 하는데에도 NL 조인을 선택해야 하는 이유가 있다.
* NL 조인에 사용하는 인덱스는 영구적으로 유지하면서 다양한 쿼리를 위해 공유 및 재사용하는 자료구조이다.
* 하지만 해시 테이블은 단 하나의 쿼리를 위해 생성하고 조인이 끝나면 소멸하는 자료 구조이다.
* 같은 쿼리를 100개 프로세스가 동시에 수행하면 해시 테이블도 100개가 만들어진다.
* 따라서 수행시간이 짧으면서 수행빈도가 매우 높은 쿼리를 해시 조인으로 처리하면 CPU와 메모리 사용률이 크게 증가하며, 또한 해시 맵을 만드는 과정에 여러 래치 경합도 발생한다.
* 따라서 해시 조인은 아래 세 가지 조건을 만족하는 SQL문에 사용해야 한다.
  * 수행빈도가 낮다.
  * 쿼리 수행 시간이 오래 걸린다.
  * 대량 데이터를 조인한다.
* 위 세 가지 조건은 배치프로그램, DW, OLAP성 쿼리의 특징이기도 하다.
* 결과적으로, OLTP 환경에서 해시 조인을 사용할 순 있지만 3가지 조건에 해당하는지 점검해봐야 한다.
  * 예시로, OLTP환경에서 NL 조인으로 0.1초 걸리는 쿼리를 0.01초로 단축할 목적으로 해시조인을 쓰는건 자제해야 한다.
  * 만약 이렇게 단축해달라는 요청이 온 경우엔 더더욱 자제해야 한다. 이 경우엔 수행빈도가 아주 높은 쿼리임이 분명하다.


## 서브 쿼리 조인
* 실무에서는 복잡한 서브쿼리와 조인하는 형태를 많이 볼 수 있다.
* 따라서 옵티마이저가 서브쿼리 조인을 어떻게 처리하는지 이해하고, 원하는 방식으로 실행계획을 제어할 수 있어야 튜닝도 가능하다.
* 특히 옵티마이저는 서브쿼리에 대해 다양한 형태로 쿼리변환을 시도하기 때문에, 서브쿼리 조인을 이해하는 출발점을 쿼리 변환에서 찾아야 한다.

### 서브쿼리 변환이 필요한 이유
* 최근 옵티마이저는 비용을 평가하고 실행계획을 생성하기에 앞서, 사용자로부터 전달받은 SQL을 최적화에 유리한 형태로 변환하는 작업인 쿼리 변환부터 진행한다.
  * 하나의 결과집합을 얻기 위해 SQL을 여러 가지 다른 형태로 표현할 수 있고, 어떤 것을 선택하느냐에 따라 성능도 다를 수 있기 때문이다.
  * 애초에 사용자가 최적화에 유리한 형태로 SQL을 작성하면 좋지만, 그렇지 않기 때문에 옵티마이저가 대신 해주는 것이고, 이로인해 옵티마이저 엔진은 점점 무거워지며 최적화에 소요되는 시간도 늘고 있다.
* 쿼리 변환은, 옵티마이저가 SQL을 분석해 의미적으로 동일하면서도 더 나은 성능이 기대되는 형태로 재작성하는것을 의미한다.
  * SQL 성능과 관련해 새로 개발되는 핵심 기능 대부분이 쿼리 변환 영역에 속한다.
* 서브쿼리는 하나의 SQL문 안에 괄호로 묶은 별도의 쿼리 블록을 말한다.
  * 쿼리에 내장된 또 다른 쿼리이다. 
* 서브 쿼리를 DBMS마다 조금씩 다르게 분류하는데, 오라클은 아래 세 가지로 분류한다.
  1) **인라인 뷰(Inline View)** : FROM 절에 사용한 서브쿼리를 말한다.
  2) **중첩된 서브쿼리(Nexted Subquery)** : 결과 집합을 한정하기 위해 WHERE 절에 사용한 서브쿼리를 말한다. 서브쿼리가 메인쿼리를 참조하는 형태를 **상관관계(Correlated) 있는 서브쿼리** 라고 부른다.
  3) **스칼라 서브쿼리(Scalar Subquery)** : 한 레코드당 정확히 하나의 값을 반환하는 서브쿼리다. 주로 SELECT-LIST에서 사용하지만, 몇가지 예외사항을 제외하면 컬럼이 올 수 있는 대부분에 위치할 수 있다.
* 서브쿼리를 참조하는 메인쿼리도 하나의 쿼리 블록이며, 옵티마이저는 쿼리 블록 단위로 최적화를 수행한다.
* **서브쿼리별로 최적화한 쿼리가 전체적으로도 최적화되었다고 말할 순 없기 때문에, 전체적으로 바라보는 관점에서 쿼리를 이해하려면 서브쿼리를 풀어내야만 한다.**

### 서브쿼리와 조인
#### 필터 오퍼레이션
* `no_unnest` 힌트는 서비쿼리를 풀어내지 말고 그대로 수행하라고 옵티마이저에 지시하는 힌트이다.

```oracle
select c.고객번호, c.고객명
from 고객 c
where c.가입일시 >= trunc(add_months(sysdate, -1), 'mm')
  and exists (
        select /*+ no_unnest */ 'x'
        from 거래
        where 고객번호 = c.고객번호
          and 거래일시 >= trunc(sysdate, 'mm'))
```
```
0     SELECT STATEMENT Optimizer=ALL_ROWS (Cost=289 Card=1 Bytes=39)
1  0    FILTER
2  1      TABLE ACCESS (BY INDEX ROWID) OF '고객' (TABLE) (Cost=4 Card=190 ...)
3  2        INDEX (RANGE SCAN) OF '고객_X01' (INDEX) (Cost=2 Card=190)
4  1      INDEX (RANGE SCAN) OF '거래_X01' (INDEX) (Cost=3 Card=4K Bytes=92K)
```
* 위와같은 실행계획에서는 `FILTER`를 `NESTED LOOPS`로 치환하고 처리 루틴을 해석하면 된다.
* NL 조인과 같이 부분범위 처리도 가능하다.
* NL 조인과의 차이점은 아래와 같다.
  * 필터는 메인쿼리의 한 로우가 서브쿼리의 한 로우와 조인에 성공하는 순간 진행을 멈추고, 메인쿼리의 다음 로우를 계속 처리한다.
  * 필터는 캐싱기능을 갖는다.
    * 서브쿼리 입력값이 따른 반환값(true 또는 false)을 캐싱하는 기능이다. (캐시에서 true/false 여부를 확인할 수 있으면 성능을 높이는데 큰 도움이 됨)
    * 이 기능이 작동하기 때문에 서브쿼리를 수행하기 전 항상 캐시부터 확인한다.
  * 필터는 메인쿼리에 종속되므로 조인 순서가 고정된다. (항상 메인쿼리가 드라이빙 집합)

#### 서브쿼리 Unnesting
* nest는 중첩을 의미하며, un- 을 붙인 unnest는 중첩된 상태를 풀어내라는 뜻이 된다.
* 즉, 서브쿼리 Unnesting은 메인과 서브쿼리 간의 계층 구조를 풀어 서로 같은 레벨로 만들어 준다는 의미에서, 서브쿼리 Flattening 이라고 부르기도 한다.
* 서브쿼리를 그대로 두면 필터 방식을 사용할 수 밖에 없지만, Unnesting 하고 나면 일반 조인문처럼 다양한 최적화 기법을 사용할 수 있다.
* Unnesting된 서브쿼리는 NL 세미조인 외에도 다양한 방식으로 실행될 수 있다.
* 필터 방식은 항상 메인쿼리가 드라이빙 집합이지만, Unnesting된 서브쿼리는 쿼리 집합보다 먼저 처리될 수 있다.
```oracle
select /*+ leading(거래@subq) use_nl(c) */ c.고객번호, c.고객명
from 고객 c
where c.가입일시 >= trunc(add_months(sysdate, -1), 'mm')
  and exists (
        select /*+ qb_name(subq) unnest */
        from 거래
        where 고객번호 = c.고객번호
          and 거래일시 >= trunc(sysdate, 'mm')) 
```
```
0    SELECT STATEMENT Optimizer=ALL_ROWS (Cost=253K Card=190 Bytes=11K)
1  0   NESTED LOOPS
2  1    NESTED LOOPS (Cost=253K Card=190 Bytes=11K)
3  2      SORT (UNIQUE) (Cost=2K Card=427K Bytes=9M)
4  3        TABLE ACCESS (BY INDEX ROWID) OF '거래' (TABLE) (Cost=2K ...)
5  4          INDEX (RANGE SCAN) OF '거래_X02' (INDEX) (Cost=988 Card 427K)
6  2      INDEX (RANGE SCAN) OF '고객_X01' (INDEX) (Cost=1 Card=190)
7  1    TABLE ACCESS (BY INDEX ROWID) OF '고객' (TABLE) (Cost=3 Card=1 ...)
```
* 서브쿼리를 풀어서 조인하면 메인쿼리 결과집합이 서브쿼리 쪽 집합수준으로 확장될 수 있으므로 서브쿼리 집합에 대한 Sort Unique 오퍼레이션이 수행되었다.
* 즉, 서브쿼리 집합에서 고객번호 중복을 제거하기 위해 쿼리를 아래와 같이 변환한 것이다.
```oracle
select /* no_merge(t) leading(t) ues_nl(c) */ c.고객번호, c.고객명
from (select distinct 고객번호
      from 거래
      where 거래일시 >= trunc(sysdate, 'mm')) t, 고객 c
where c.가입일시 >= trunc(add_months(sysdate, -1), 'mm')
  and c.고객번호 = t.고객번호
```

* 서브쿼리를 Unnesting 해서 메인쿼리와 같은 레벨로 만들면, 다양한 조인 메소드를 선택할 수 있고, 조인 순서도 마음껏 정할 수 있다.
* 옵티마이저는 많은 조인 테크닉을 가지기 때문에 조인 형태로 변환했을 때 필터 오퍼레이션보다 더 좋은 실행경로를 찾을 가능성이 높아진다.

> 서브쿼리에서 ROWNUM
> * 서브쿼리를 Unnesting 하면 필터 오퍼레이션보다 더 좋은 실행경로를 찾을 가능성이 커진다.
> * 하지만 서브쿼리에 rownum을 사용하면 이 중요한 옵티마이징 기능을 사용하지 못하게 막는 부작용이 있다.
>   * 즉, 옵티마이저에게 이 서브쿼리 블록을 손대지 말라고 선언한것과 같다.
> * 따라서 서브쿼리 Unnesting을 방지하려는 목적이 아니라면 서브쿼리에 함부로 쓰지 않는게 좋다.

#### 서브쿼리 Pushing
* Unnesting 되지 않은 서브쿼리는 항상 필터 방식으로 처리되며, 보통 실행계획 맨 마지막 단계에서 처리된다.
* 하지만 서브쿼리 필터링을 먼저 처리함으로써 조인 단계로 넘어가는 로우 수를 크게 줄일 수 있다면 성능은 그만큼 향상된다.
* Pushing 서브쿼리는 서브쿼리 필터링을 가능한 한 앞 단계에서 처리하도록 강제하는 기능이다.
  * `push_subq`, `no_push_subq` 힌트로 제어할 수 있다.
* 이 기능은 Unnesting 되지 않은 서브쿼리에만 작동한다.
  * 서브쿼리가 Unnesting 되면 필터가 아닌 다양한 조인 방식으로 실행되기 때문이다.
  * 따라서 `push_subq` 힌트는 항상 `no_unnest` 힌트와 같이 기술하는 것이 좋다.

### 뷰(View)와 조인
* 최적화 단위가 쿼리블록이므로, 옵티마이저가 뷰 쿼리를 변환하지 않으면 뷰 쿼리 블록을 독립적으로 최적화한다.

```oracle
select c.고객번호, c.고객명, t.평균거래, t.최소거래, t.최대거래
from 고객 c
    ,(select 고객번호, avg(거래금액) 평균거래
           , min(거래금액) 최소거래, max(거래금액) 최대거래 
      from 거래
      where 거래일시 >= trunc(sysdate, 'mm')                -- 당월 발생한 거래
      group by 고객번호) t
where c.가입일시 >= trunc(add_months(sysdate, -1), 'mm')    -- 전월 이후 가입 고객
  and t.고객번호 = c.고객번호
```
* 위 쿼리에서 당월 거래 전체를 읽어 고객번호 수준으로 Group By 하는 실행계획이 수립될 수 있다.
* 문제는 고객 테이블에서 전월 이후 가입한 고객을 필터링하는 조건이 인라인 뷰 바깥에 있다는 점이다.
* 즉 이 조건이 있는데에도 인라인 뷰 안에서는 당월에 거래한 모든 고객의 거래 데이터를 읽어야 한다.
* `merge`힌트를 이용해 뷰를 메인 쿼리와 머징하도록 할 수 있다. (뷰 머징 방지엔 `no_merge` 힌트)
```oracle
select c.고객번호, c.고객명, t.평균거래, t.최소거래, t.최대거래
from 고객 c
    ,(select /*+ merge */ 고객번호, avg(거래금액) 평균거래
           , min(거래금액) 최소거래, max(거래금액) 최대거래 
      from 거래
      where 거래일시 >= trunc(sysdate, 'mm')                -- 당월 발생한 거래
      group by 고객번호) t
where c.가입일시 >= trunc(add_months(sysdate, -1), 'mm')    -- 전월 이후 가입 고객
  and t.고객번호 = c.고객번호
```
* 위와같이 변경한 경우, 실행계획에서 고객 테이블을 먼저 읽는다.
* 인덱스를 이용해 전월 이후 가입한 고객만 읽고, 거래 테이블과 조인할 때는 해당 고객들에 대한 당월 거래만 읽는다.
* 단점은 조인에 성공한 전체 집합을 Group By 하고서야 데이터를 출력할 수 있다는 점이다.
* 부분범위 처리가 불가능하다는 의미이고, 만약 전월 이후 가입한 고객이 매우 많고 당월거래도 매우 많다면 부분범위 처리 불가능한 상황에서 NL 조인은 좋은 선택이 아니다.
* 그런 상황에서는 보통 해시 조인이 빠르며, 뷰 머징한 거래 테이블을 고객과 해시조인한 후 Group By 할 수 있다. 

#### 조인 조건 Pushdown
* 오라클 11g 이후로 조인 조건 Pushdown이라는 쿼리 변환 기능이 작동한다.
* 메인 쿼리를 실행하면서 조인 조건절 값을 건건이 뷰 안으로 밀어 넣는 작업이다.
* 실행계획에서 `VIEW PUSHED PREDICATE` 오퍼레이션을 통해 이 기능의 작동 여부를 알 수 있다.

```oracle
select c.고객번호, c.고객명, t.평균거래, t.최소거래, t.최대거래
from 고객 c
    ,(select /*+ no_merge push_pred */ 
             고객번호, avg(거래금액) 평균거래
           , min(거래금액) 최소거래, max(거래금액) 최대거래 
      from 거래
      where 거래일시 >= trunc(sysdate, 'mm')                -- 당월 발생한 거래
      group by 고객번호) t
where c.가입일시 >= trunc(add_months(sysdate, -1), 'mm')    -- 전월 이후 가입 고객
  and t.고객번호 = c.고객번호
```
```
0    SELECT STATEMENT Optimizer=ALL_ROWS (Cost=4 Card=1 Bytes=61)
1  0   NESTED LOOPS (Cost=4 Card=1 Bytes=61)
2  1     TABLE ACCESS (BY INDEX ROWID BATCHED) OF '고객' (TABLE) (Cost=2 ...)
3  2       INDEX (RANGE SCAN) OF '고객_X01' (INDEX) (Cost=1 Card=1)
4  1     VIEW PUSHED PREDICATE (Cost=2 Card=1 Bytes=7)
5  4       SORT (GROUP BY) (Cost=2 Card=1 Bytes=7)
6  5         TABLE ACCESS (BY INDEX ROWID BATCHED) OF '거래' (TABLE) (Cost=2 ...)
7  6           INDEX (RANGE SCAN) OF '거래_X02' (INDEX) (Cost=1 Card=5)    
```
* 이 방식을 사용하면 전월 이후 가입한 고객을 대상으로 건건이 당월 거래 데이터만 읽어서 조인하고 Group By를 수행한다.
  * 중간에 멈출수도 있으며, 부분범위 처리가 가능하다.
* 이 기능을 제어하는 힌트는 `push_pred`이며, 옵티마이저가 뷰를 머징하면 힌트가 작동하지 않기 때문에 `no_merge` 힌트를 함께 사용하는게 좋다.

> Lateral 인라인 뷰, Cross/Outer Apply 조인
> * 인라인 뷰 안에서 메인쿼리 테이블 컬럼을 참조하면 ORA-00904(invalid identifier) 에러가 발생한다.
> * 오라클 12c부터 인라인 뷰를 Lateral로 선언하면 인라인 뷰 안에서 메인쿼리 테이블의 컬럼을 참조할 수 있다.
> * Outer 조인이 필요하면 12c의 Outer Apply 조인 구문을 사용할 수도 있다.
> * 12c에서는 Cross Apply 조인 구문도 지원하며,구문만 다를 뿐 Lateral 인라인 뷰와 동일하다.
> * 하지만 이러한 기능들은 유용해 보이지만 사용해야 할 이유를 찾기 힘들다
>   * **최근 오라클 버전에선 조인 조건 Pushdown 기능이 잘 동작**한다.
>   * 오히려 개발자들이 이러한 구문을 남용함으로 얽히고 설킨 복잡한 쿼리만 생겨날 수 있다.
> * 즉, **기본적으로 Lateral 인라인 뷰, Cross/Outer Apply 조인을 쓸 이유는 없다.**
> * 튜닝 과정에 알 수 없는 이유로 조인 조건 Pushdown 기능이 잘 작동하지 않을 때만 활용하면 된다.

### 스칼라 서브쿼리 조인
#### 스칼라 서브쿼리의 특징
* SELECT-LIST에 사용한 함수가 있다면, 함수 안에 있는 SELECT 쿼리를 메인쿼리 건수만큼 재귀적으로 반복 실행한다.
* 하지만 스칼라 서브쿼리는 건수만큼 해당 테이블을 반복해서 읽으며, 레코드마다 정확히 하나의 값만 반환한다.
  * 함수처럼 재귀적으로 실행하는 구조는 아니며, Outer 조인문처럼 하나의 문장으로 이해한다.
  * 조인과 차이가 있다면 스칼라 서브쿼리는 처리 과정에서 캐싱 작용이 일어난다는 것이다.

#### 스칼라 서브쿼리 캐싱 효과
* 스칼라 서브쿼리로 조인하면 오라클은 조인 횟수를 최소화하려고 입력 값과 출력 값을 내부 캐시에 저장해 둔다.
* 조인할 때마다 일단 캐시에서 입력값을 찾아보고 찾으면 저장된 출력값을 반환한다.
* 캐시에서 찾지 못할 때만 조인을 수행하며, 결과는 버리지 않고 캐시에 저장해둔다.
* 스칼라 서브쿼리의 입력 값은, 그 안에서 참조하는 메인쿼리의 컬럼 값이다.
* 스칼라 서브 쿼리 캐싱은 필터 서브쿼리 캐싱과 같은 기능이다.
* 이러한 캐싱 메커니즘은 조인성능을 높이는데 큰 도움이 된다.
  * 메인쿼리 집합이 아무리 커도 조인할 데이터를 대부분 캐시에서 찾는다면 조인 수행횟수를 최소화할 수 있다.
* 캐싱은 쿼리 단위로 이루어지며, 쿼리를 시작할 때 PGA 메모리에 공간을 할당하고 쿼리를 수행하면서 공간을 채워나가며 쿼리를 마치는 순간 공간을 반환한다.

```oracle
select empno, ename, sal, hiredate
     , (select GET_DNAME(e.deptno) from dual) dname
from emp e
where sal >= 2000
```
* SELECT-LIST에 사용한 함수는 메인쿼리 결과 건수만큼 반복 수행되는데, 위와 같이 스칼라 서브쿼리를 덧씌우면 호출 횟수를 최소화할 수 있다. (캐싱 효과 때문)
* 함수에 내장된 SELECT 쿼리도 그만큼 덜 수행된다.

#### 스칼라 서브쿼리 캐시 부작용
* 모든 캐시가 다 그렇듯이, 캐시 공간은 늘 부족하다.
* 스칼라 서브쿼리에 사용하는 캐시도 매우 작은 메모리 공간이다.
* 스칼라 서브쿼리 캐싱 효과는, **입력 값의 종류가 소수여서 해시 충돌 가능성이 작을 때 효과가 있다.**
  * 반대의 경우라면 캐시를 매번 확인하는 비용으로 인해 성능이 나빠지고 CPU 사용률만 높게 만들며, 메모리도 더 사용한다.
  ```oracle
  select 거래번호, 고객번호, 영업조직ID, 거래구분코드
       , (select 거래구분명 from 거래구분 where 거래구분코드 = t.거래구분코드) 거래구분명
  from 거래 t
  where 거래일자 >= to_char(add_months(sysdate, -3), 'yyyymmdd') -- 50,000건
  ```
  * 위 예시에서, 거래구분코드가 20개의 값이 존재한다면 효율적인 캐싱이 일어난다.
  ```oracle
  select 거래번호, 고객번호, 영업조직ID, 거래구분코드
       , (select 고객명 from 고객 where 고객번호 = t.고객번호) 거래구분명
  from 거래 t
  where 거래일자 >= to_char(add_months(sysdate, -3), 'yyyymmdd') -- 50,000건
  ```
  * 위 예시에서, 고객이 매우 많다면 메인 쿼리에서 50000개의 거래를 읽는 동안 매번 탐색하지만, 대부분은 찾지 못해 결국 조인을 해야 한다.
    * 불필요한 캐시 탐색으로 인해 일반 조인문보다 느리고 불필요하게 자원만 낭비하게 된다.
* 또한 **메인쿼리 집합이 매우 작은 경우에는 스칼라 서브쿼리 캐싱이 성능에 도움을 주지 못한다.**
  ```oracle
  select 계좌번호, 계좌명, 고객번호, 개설일자, 계좌종류구분코드
       , (select brch_nm(관리지점코드) from dual) 관리지점명
       , (select brch_nm(개설지점코드) from dual) 개설지점명
  from 계좌
  where 고객번호 = :고객번호
  ```
  * 위 예시에서는 고객당 계좌가 많지 않고 보통은 한 개일 것이기 때문에 쓰지도 않을 캐시를 할당해 값을 채웠다가 바로 버리는 결과가 생긴다.

#### 두 개 이상의 값 반환
* 스칼라 서비쿼리에는 치명적인 제약이 있는데, 두 개 이상의 값을 반환할 수 없다는 제약이다.
  * 부분범위처리가 가능하다는 스칼라 서브쿼리의 장점을 이용하고 싶더라도, 아래와 같은 쿼리는 불가능하다.
  ```oracle
  select c.고객번호, c.고객명
       , (select avg(거래금액), min(거래금액), max(거래금액)
          from 거래
          where 거래일시 >= trunc(sysdate, 'mm')
            and 고객번호 = c.고객번호)
  from 고객 c
  where c.가입일시 >= trunc(add_months(sysdate, -1), 'mm')
  ```
  * 이럴 때 튜너들이 전통적으로 많이 사용해 온 방식은 아래와 같다.
  ```oracle
  select 고객번호, 고객명
       , to_number(substr(거래금액, 1, 10)) 평균거래금액
       , to_number(substr(거래금액, 11, 10)) 최소거래금액
       , to_number(substr(거래금액, 21)) 최대거래금액
  from (
      select c.고객번호, c.고객명
           , (select lpad(avg(거래금액), 10) || lpad(min(거래금액), 10) || max(거래금액)
              from 거래
              where 거래일시 >= trunc(sysdate, 'mm')
                and 고객번호 = c.고객번호) 거래금액
      from 고객 c
      where c.가입일시 >= trunc(add_months(sysdate, -1), 'mm')
  )
  ```
  * 위와 같이 구하는 값들을 문자열로 결합 후 바깥쪽 액세스 쿼리에서 substr함수로 분리하는 방식이다.
  * 또한 오브젝트 TYPE을 사용하는 방법도 있지만, TYPE을 미리 선언해두어야 하기 때문에 불편해서 잘 쓰이지 않는다.
* 두 개 이상 값을 반환하고 싶을 때에는 인라인 뷰를 사용하면 편하긴 하다.
  ```oracle
  select c.고객번호, c.고객명, t.평균거래, t.최소거래, t.최대거래
  from 고객 c
      , (select 고객번호, avg(거래금액) 평균거래
              , min(거래금액) 최소거래, max(거래금액) 최대거래
         from 거래
         where 거래일시 >= trunc(sysdate, 'mm')
         group by 고객번호) t
  where c.가입일시 >= trunc(add_months(sysdate, -1), 'mm')
    and t.고객번호(+) = ㅊ.rhrorqjsgh
  ```
  * 하지만 뷰를 사용하면, 뷰가 머징되지 않았을 때 당월 거래 전체를 읽어야 하거나, 뷰가 머징될 때 Group By 때문에 부분범위처리가 안되는 문제가 있다.
  * 따라서 SQL튜너들은 두 개 이상의 값을 반환해야 할 때 스칼라 서브쿼리와 인라인 뷰 사이에서 많은 고민을 했었다.
  * 11g 이후로눈 조언 조건 Pushdown 기능이 잘 작동하므로 인라은 뷰를 마음편히 사용할 수 있게 되었다.

#### 스칼라 서브쿼리 Unnesting
* 스칼라 서브쿼리도 NL 방식으로 조인하므로 캐싱효과가 크지 않으면 랜덤 I/O 부담이 있다.
* 그래서 다른 조인방식을 선택하기 위해 스칼라 서브쿼리를 일반 조인문으로 변환하고 싶을 때가 있다.
* 병렬 쿼리에선 될 수 있으면 스칼라 서브쿼리를 사용하지 않아야 한다.
  * 대량 데이터를 처리하는 병렬 쿼리는 해시 조인으로 처리해야 효과적이기 때문이다.
* 어떤 이유에서건, 사용자가 직접 쿼리를 변환해야 하는 상황에서 길고 복잡한 스칼라 서브쿼리를 만나면 반갑지 않다.
* 오라클 12c부터 스칼라 서브쿼리도 Unnesting이 가능해졌는데, 옵티마이저가 사용자 대신 자동으로 쿼리를 변환해 주는 것이다.
  * 하지만 이 기능 때문에 곤욕을 치르는 경우도 많이 생겼다.
  * `_optimizer_unnest_scalar_sq` 파라미터를 false로 설정함으로써 일단 문제를 해결했었다.
  * 또한 `no_unnest` 힌트를 이용해 부분적으로 해결하기도 했다.
* `_optimizer_unnest_scalar_sq` 파라미터를 true로 설정하면, 스칼라 서브쿼리를 Unnesting 할 지 여부를 옵티마이저가 결정한다.
  * false로 설정하면 옵티마이저가 이 기능을 사용하진 않지만, 사용자가 `unnest`힌트로 유도할 순 있다.

# 소트 튜닝
## 소트 연산에 대한 이해
* SQL 수행 도중, 가공된 데이터 집합이 필요할 때 오라클은 PGA와 Temp 테이블스페이스를 활용한다.
* 소트 머지 조인, 해시 조인, 데이터 소트와 그룹핑 등이 대표적이다.

### 소트 수행 과정
* 소트는 기본적으로 PGA에 할당한 Sort Area에서 이루어진다.
* 메모리 공간인 Sort Area가 다 차면 디스크 Temp 테이블스페이스를 활용한다.
* Sort Area에서 작업을 완료할 수 있는 지에 따라 소트를 두가지 유형으로 나눈다.
  * 메모리 소트(In-Memory Sort) : 전체 데이터의 정렬 작업을 메모리 내에서 완료하는것을 말하며, Internal Sort 라고도 한다.
  * 디스크 소트(To-Disk Sort) : 할당받은 Sort Area 내에서 정렬을 완료하지 못해 디스크 공간까지 사용하는 경우를 말하며, External Sort 라고도 한다.
* 디스크 소트 과정은 아래와 같다.
  * 소트할 대상 집합을 SGA 버퍼캐시를 통해 읽어들이고, 일차적으로 Sort Area에서 정렬을 시도한다.
  * 양이 많을 경우 정렬된 중간 집합을 Temp 테이블스페이스에 임시 세그먼트를 만들어 저장한다.
    * Sort Area가 찰 때마다 Temp 영역에 저장해 둔 중간 단계의 집합을 Sort Run 이라고 부른다.
  * 정렬된 최종 결과집합을 얻으려면 다시 Merge해야 하는데, 각 Sort Run 내에서는 이미 정렬된 상태이므로 Merge과정은 어렵지 않다.
* 소트 연산은 메모리 집약적일 뿐만 아니라, CPU 집약적이기도 하다.
* 처리할 데이터량이 많을 때는 디스크 I/O까지 발생하므로, 쿼리 성능을 좌우하는 매우 중요한 요소이다.
  * 디스크 소트가 발생하는 순간 SQL 수행 성능은 나빠질 수 밖에 없다.
* 또한, **부분범위 처리를 불가능하게 함으로써 OLTP 환경에서 애플리케이션 성능을 저하시키는 주 요인이 되기도 한다.**
* 따라서 디스크 소트가 발생하지 않도록 SQL을 작성해야 하며, 소트가 불가피한 경우 메모리 내에서 수행을 완료할 수 있도록 해야 한다.

### 소트 오퍼레이션
#### Sort Aggregate
* Sort Aggregate는 전체 로우를 대상으로 집계를 수행할 때 나타난다.
* Sort라는 표현을 사용하지만 실제로 데이터를 정렬하진 않고, Sort Area를 사용한다는 의미로 이해하면 된다.
```oracle
select deptno, sum(sal), max(sal), min(sal), avg(sal) from emp 
```
* 데이터를 정렬하지 않고 SUM, MAX, MIN, AVG 값을 구하는 법
  * Sort Area에 SUM, MAX, MIN, COUNT 값을 위한 변수를 각각 하나씩 할당한다.
  * 테이블 첫 레코드에서 읽은 해당 컬럼의 값을 SUM, MAX, MIN 변수에 저장하고, COUNT 변수에는 1을 저장한다.
  * 테이블에서 레코드를 하나씩 읽어내려가며 SUM 변수에는 값을 누적하고, MAX 변수에는 기존보다 큰 값이 나타날 때만 대체하고, MIN 변수도 기존보다 작은 값이 나타날 때만 값을 대체하며, COUNT 변수에는 NULL이 아닌 레코드를 만날때마다 1씩 증가시킨다.
  * 레코드를 다 읽고 나면 SUM, MAX, MIN, COUNT 변수에 각각 해당하는 값들이 저장되어 있으며, 그 값을 그대로 출력하면 되고 AVG는 SUM 값을 COUNT로 나눈 값을 출력한다.

#### Sort Order By
* Sort Order By는 데이터를 정렬할 때 나타난다.
```oracle
select * from emp order by sal desc 
```

#### Sort Group By
* Sort Group By는 소팅 알고리즘을 이용해 그룹별 집계를 수행할 때 나타난다.
```oracle
select deptno, sum(sal), max(sal), min(sal), avg(sal)
from emp
group by deptno
order by deptno
```
* 즉, 위 예시에서 `deptno`별로 작업할 공간을 만들고, 그 공간을 `deptno` 순으로 만든다.

> 그룹핑 결과의 정렬 순서
> * Sort Group By는 정렬이 보장된다고 믿는 경우가 있는데 그렇지 않다.
> * 실제로는 소팅 알고리즘을 사용해 값을 집계한다는 뜻일 뿐이고, 결과의 정렬을 의미하진 않는다.
> * Order By 절이 없으면 오라클 입장에선 반드시 정렬된 순서로 출력할 의무가 없다.
> * 따라서 실행계획에 Sort Group By라고 표시되더라도, 만드시 Order By를 명시해야 한다.

#### Hash Group By
* 오라클 10gR2 버전에서 도입되었으며, Group By 절 뒤에 Order By 절을 명시하지 않으면 대부분 Hash Group By방식으로 처리한다.
```oracle
select deptno, sum(sal), max(sal), min(sal), avg(sal)
from emp
group by deptno
```
* Sort Group By에서는 해당 그룹별 공간을 위해 소트 알고리즘을 사용했다면, Hash Group By는 해싱 알고리즘을 사용한다.

#### Sort Unique
* Unnesting된 서브쿼리가 M쪽 집합이면(혹은 1쪽집합이더라도 조인 컬럼에 Unique 인덱스가 없으면) 메인 쿼리와 조인하기 전 중복 레코드부터 제거해야 하는데, 이럴 때 Sort Unique 오퍼레이션이 나타난다.
  * 만약 PK/Unique 제약 또는 Unique 인덱스를 통해 Unnesting된 서브쿼리의 유일성이 보장된다면, Sort Unique 오퍼레이션은 생략된다.
* Union, Minus, Intersect 같은 집합(Set) 연산자를 사용할 때도 Sort Unique 오퍼레이션이 나타난다.
* Distinct 연산자를 사용해도 Sort Unique 오퍼레이션이 나타난다.
  * 오라클 10gR2부터는 Distinct 연산에도 Hash Unique 방식을 사용하는데, Order By를 생략할 때 그렇다.

#### Sort Join
* Sort Join 오퍼레이션은 소트 머지 조인을 수행할 때 나타난다.

#### Window Sort
* Window Sort는 윈도우 함수(분석함수)를 수행할 때 나타난다.
```oracle
select empno, ename, job, mgr, sal
     , avg(sal) over (partition by deptno)
from emp
```

## 소트가 발생하지 않도록 SQL 작성
* Union, Minus, Distinct 연산자는 중복 레코드를 제거하기 위한 소트 연산을 발생시키기 때문에 곡 필요한 경우에만 사용해야 한다.

### Union vs Union All
* Union을 사용하면 옵티마이저는 상단과 하단 두 집합 간 중복을 제거하려고 소트 작업을 수행한다.
* 하지만 Union All은 중복을 확인하지 않고 두 집합을 단순히 결합하기 때문에 소트 작업을 수행하지 않는다.
* 따라서 될 수 있으면 Union All을 사용해야 한다.
* 하지만 자칫 결과 집합이 달라질 수 있기 때문에 Union All로 무작정 변경하면 안된다.
  * 데이터 모델에 대한 이해와 집합적 사고가 필요하다.
  * 그렇지 않으면 알 수 없는 데이터 중복, 혹시 모를 데이터 중복을 우려해 중복 제거용 연산자를 불필요하게 자주 사용하게 될 수 있다.

### Exists 활용
* 중복 레코드를 제거할 목적으로 Distinct 연산자를 종종 사용하는데, 이 연산자를 사용하면 조건에 해당하는 데이터를 모두 읽어서 중복을 제거해야 한다.
  * 부분범위 처리는 당연히 불가능하고, 모든 데이터를 읽는 과정에 많은 I/O가 발생한다.
```oracle
select DISTINCT p.상품번호, p.상품명, ...
from 상품p, 계약 c
where p.상품유형코드 = :pclscd
  and c.상품번호 = p.상품번호
  and c.계약일자 between :dt1 and :dt2
  and c.계약구분코드 = :ctpcd
```
* Exists를 사용하면 이러한 문제를 해결할 수 있다.
```oracle
select p.상품번호, p.상품명, ...
from 상품 p
where EXISTS(select 'x' from 계약 c
             where c.상품번호 = p.상품번호
               and c.계약일자 between :dt1 and :dt2
               and c.계약구분코드 = :ctpcd)
```
* 위 예시를 보면 Exists 서브쿼리를 통해 데이터 존재여부만 확인하면 되기 때문에 만족하는 데이터를 모두 읽지 않는다.
* 이와 같이 Distinct, Minus 연산자를 사용한 쿼리는 대부분 Exists 서브쿼리로 변환 가능하다.

### 조인 방식 변경
* 조인문일 때는 조인 방식도 잘 선택해줘야 한다.
```oracle
select c.계약번호, c.상품코드, p.상품명, p.상품구분코드, c.계약일시, c.계약금액
from 계약 c, 상품 p
where c.지점ID = :brch_id
  and p.상품코드 = c.상품코드
order by c.계약일시 desc
```
```
0    SELECT STATEMENT Optimizer=ALL_ROWS
1  0   SORT (ORDER BY)
2  1     HASH JOIN
3  2       TABLE ACCESS (FULL) OF '상품' (TABLE)
4  2       TABLE ACCESS (BY INDEX ROWID) OF '계약' (TABLE)
5  4         INDEX (RAGE SCAN) OF '계약_X01' (INDEX)
```
* 위 예시는 `계약_X01` 인덱스가 `지점ID + 계약일시` 순이면 소트 연산을 생략할 수 있지만, 해시 조인이기 때문에 Sort Order By가 나타난 예시이다.
* 즉 이러한 상황에서는, 아래와 같이 NL조인하도록 조인방식을 변경하면 소트 연산을 생략할 수 있으며, `지점ID` 조건을 만족하는 데이터가 많고 부분범위 처리 가능한 상황에서 큰 성능 개선 효과를 얻을 수 있다.
```oracle
select /*+ leading(c) use_nl(p)*/
       c.계약번호, c.상품코드, p.상품명, p.상품구분코드, c.계약일시, c.계약금액
from 계약 c, 상품 p
where c.지점ID = :brch_id
  and p.상품코드 = c.상품코드
order by c.계약일시 desc
```
```
0    SELECT STATEMENT Optimizer=ALL_ROWS
1  0   NESTED LOOPS
2  1     NESTED LOOPS
3  2       TABLE ACCESS (BY INDEX ROWID) OF '계약' (TABLE)
4  3         INDEX (RAGE SCAN DESCENDING) OF '계약_X01' (INDEX)
5  2       INDEX (UNIQUE SCAN) OF '상품_PK' (INDEX (UNIQUE))
6  1     TABLE ACCESS (BY INDEX ROWID) OF '상품' (TABLE)
```

## 인덱스를 이용한 소트 연산 생략
* 인덱스의 정렬된 상태를 이용하면 Order By 또는 Group By 절이 있어도 소트 연산을 생략할 수 있다.
* 여기에 Top N 쿼리 특성을 결합하면 OLTP 시스템에서 대량 데이터를 조회할 때 매우 빠른 응답 속도를 낼 수 있다.

### Sort Order By 생략
```oracle
select 거래일시, 체결건수, 체결수량, 거래대금
from 종목거래
where 종목코드 = 'KR123456'
order by 거래일시
```
* 위 예시에서, 인덱스 선두 컬럼을 `종목코드 + 거래일시` 순으로 구성하면 소트 연산을 생략할 수 있다.
  * 즉, 실행계획에서 Sort Order By 오퍼레이션이 생략된다.
* 또한 소트 연산을 생략함으로써, 전체 레코드를 읽지 않고도 바로 결과집합 출력을 시작할 수 있게 된다.
  * 즉, 부분범위 처리 가능한 상태가 되었다.

> 부분범위 처리 아직도 유효한가?
> * 요즘 DB 애플리케이션은 대부분 3-Tier 환경에서 작동하므로 부분범위 처리는 의미 없다고 생각할 수 있다.
> * 단위 작업을 마치면 DB 커넥션을 바로 커넥션 풀에 반환해야 하므로, 그 전에 쿼리 조회 결과를 클라이언트에게 모두 전송하고 커서를 닫아야만 하기 때문이다.
> * 하지만 부분범위 처리 원리는 이러한 3-Tier 환경에서도 여전히 유효한데, 바로 Top N 쿼리 때문이다.

### Top N 쿼리
* Top N 쿼리는 전체 결과집합 중 상위 N개 레코드만 선택하는 쿼리이다.
* SQL Server나 Sybase는 Top N 쿼리를`TOP 10`과 같이, IBM DB2는 `FETCH FIRST 10 ROWS ONLY`와 같이, 오라클은 인라인 뷰로 한번 감싸고 `where rownum <= 10`과 같이 사용할 수 있다.
```oracle
select * from (
    select 거래일시, 체결건수, 체결수량, 거래대금
    from 종목거래
    where 종목코드 = 'KR123456'
      and 거래일시 >= '20180304'
    order by 거래일시
)
where rownum <= 10
```
* 여기에 소트 연산을 생략할 수 있는 인덱스(위 예시에선, `종목코드 + 거래일시`)를 사용하면, 옵티마이저는 소트 연산을 생략하고 인덱스를 스캔하다 10개의 레코드를 읽는 순간 바로 멈춘다.
```
0    SELECT STATEMENT Optimizer=ALL_ROWS
1  0   COUNT (STOPKEY)
2  1     VIEW
3  2       TABLE ACCESS (BY INDEX ROWID) OF '종목거래' (TABLE)
4  3         INDEX (RANGE SCAN) OF '종목거래_PK' (INDEX (UNIQUE))
```
* 위 실행계획을 보면, Sort Order By 오퍼레이션은 보이지 않고 `COUNT(STOPKEY)`가 보인다.
* 이것은 조건절에 부합하는 레코드가 많아도 그 중 `ROWNUM`으로 지정한 건수만큼 결과 레코드를 얻고 거기서 멈춘다는 뜻이다.
* 이러한 기능을 TOP N Stopkey 알고리즘이라 한다.

#### 페이징 처리
* 이러한 방식을 이용해 3-Tier 환경에서 페이징 처리를 할 수 있다.
```oracle
select *
from (
    select rownum no, a.*
    from
        (
         /* SQL Body */
        ) a
    where rownum <= (:page * 10)
)
where no >= (:page-1)*10 + 1
```
* 3-Tier 환경에서 부분범위 처리를 활용하기 위해 해야할 일은 다음과 같다.
  * 부분범위 처리 가능하도록 SQL을 작성하고, 부분범위 처리가 잘 작동하는지 쿼리 툴에서 테스트한다.
  * 작성한 SQL 문을 페이징 처리용 표준 패턴 SQL Body에 붙여넣는다.
* 부분범위 처리가 가능하도록 SQL을 작성한다는 것은 아래와 같은 것을 의미한다.
  * 인덱스를 사용 가능하도록 조건절을 구사한다.
  * 조인은 NL 조인 위주로 처리한다. (룩업을 위한 작은 테이블은 해시조인 Build Input으로 처리해도 됨)
  * Order By 절이 있어도 소트 연산을 생략할 수 있도록 인덱스를 구성한다.

#### 페이징 처리 ANTI 패턴
```oracle
select *
from (
    select rownum no, a.*
    from
        (
         /* SQL Body */
        ) a
)
where no between (:page-1)*10 + 1 and (:page * 10)
```
* 간혹 간결하게 표현하고 싶어서 `ROWNUM` 조건 없이 위와같은 식으로 페이징 처리를 하곤 한다.
* 하지만 `ROWNUM` 조건이 없어지면 Top N Stopkey 알고리즘이 동작하지 않아, 실행계획 상에서도 `COUNT` 옆에 `(STOPKEY)`가 없을 것이다.
  * 즉 이런 경우에 SQL 툴에서 해당 쿼리를 확인해보면, 첫 번째 페이지는 금방 출력되긴 하지만 하드디스크가 계속 돌아가는 것을 확인할 수 있다. 

### 최솟값/최댓값 구하기
* 최솟값 또는 최댓값을 구하는 SQL 실행계획을 보면, 아래와 같이 Sort Aggregate 오퍼레이션이 나타난다.
* Sort Aggregate를 위해 전체 데이터를 정렬하진 않지만, 전체 데이터를 읽으면서 값을 비교한다.
* 인덱스는 정렬되어 있으므로, 이를 이용하면 전체 데이터를 읽지 않고도 최솟값 최댓값을 쉽게 찾을 수 있다.

#### 인덱스 이용해 최소/최대값을 구하기 위한 조건
* 데이터를 읽지 않고 인덱스를 이용해 최솟값 또는 최댓값을 구하려면 조건절 컬럼과 MIN/MAX 함수인자 컬럼이 모두 인덱스에 포함되어 있어야 한다.
  * 즉 테이블 액세스가 발생하지 않아야 한다.
* 위와같은 조건에 만족한 경우 아래와 같은 실행계획을 볼 수 있다.
* `FIRST ROW`는 조건을 만족하는 레코드 하나를 찾았을 때 바로 멈춘다는 것을 의미하며, 이것을 First Row Stopkey 알고리즘이라고 한다.

* 인덱스 구성에 따른 실행계획의 차이
  1) ```oracle
     -- 인덱스 : DEPTNO + MGR + SAL
     SELECT MAX(SAL) FROM EMP WHERE DEPTNO = 30 AND MGR = 7698;
     ```
     ```
     0    SELECT STATEMENT Optimizer=ALL_ROWS (Cost=1 Card=1 Bytes=8)
     1  0  SORT (AGGREGATE) (Card=1 Bytes=8)
     2  1    FIRST ROW (Cost=1 Card=1 Bytes=8)
     3  2      INDEX(RANGE SCAN (MIN/MAX)) OF 'EMP_X1' (INDEX) (Cost=1 Card=1 ...)
     ```
     * `DEPTNO`, `MGR`을 만족하는 가장 큰 레코드를 하나 읽으면 끝
  2) ```oracle
     -- 인덱스 : DEPTNO + SAL + MGR
     SELECT MAX(SAL) FROM EMP WHERE DEPTNO = 30 AND MGR = 7698;
     ```
     ```
     0    SELECT STATEMENT Optimizer=ALL_ROWS (Cost=1 Card=1 Bytes=8)
     1  0  SORT (AGGREGATE) (Card=1 Bytes=8)
     2  1    FIRST ROW (Cost=1 Card=1 Bytes=8)
     3  2      INDEX(RANGE SCAN (MIN/MAX)) OF 'EMP_X1' (INDEX) (Cost=1 Card=1 ...)
     ```
     * `DEPTNO`를 만족하는 가장큰 `SAL`값부터 내려오면서, `MGR = 7698`을 만족하는 레코드를 찾으면 끝 
  3) ```oracle
     -- 인덱스 : SAL  + DEPTNO + MGR
     SELECT MAX(SAL) FROM EMP WHERE DEPTNO = 30 AND MGR = 7698;
     ```
     ```
     0    SELECT STATEMENT Optimizer=ALL_ROWS (Cost=1 Card=1 Bytes=8)
     1  0  SORT (AGGREGATE) (Card=1 Bytes=8)
     2  1    FIRST ROW (Cost=1 Card=1 Bytes=8)
     3  2      INDEX(FULL SCAN (MIN/MAX)) OF 'EMP_X1' (INDEX) (Cost=1 Card=1 ...)
     ```
     * 인덱스 전체에서 가장 큰 `SAL`값부터 내려오면서, `DEPTNO = 30`, `MGR = 7698` 조건을 만족하는 레코드를 찾으면 끝
     * Index Range Scan은 불가능하다.
  4) ```oracle
     -- 인덱스 : DEPTNO + SAL
     SELECT MAX(SAL) FROM EMP WHERE DEPTNO = 30 AND MGR = 7698;
     ```
     ```
     0    SELECT STATEMENT Optimizer=ALL_ROWS (Cost=1 Card=1 Bytes=8)
     1  0  SORT (AGGREGATE) (Card=1 Bytes=8)
     2  1    TABLE ACCESS (BY INDEX ROWID) OF 'EMP' (TABLE) (Cost=2 Card=1 Bytes=8)
     3  2      INDEX(RANGE SCAN) OF 'EMP_X1' (INDEX) (Cost=1 Card=5)
     ```
     * `DEPTNO = 30` 조건을 만족하는 전체 레코드를 읽어 `MGR = 7698`을 필터링한 후 `MAX(SAL)`값을 구한다.

### Top N 쿼리를 이용해 최솟값/최댓값 구하기  
```oracle
-- 인덱스 : DEPTNO + SAL
SELECT *
FROM (
    SELECT sal
    FROM emp
    WHERE DEPTNO = 30
      AND MGR = 7698
    ORDER BY SAL DESC
)
WHERE ROWNUM <= 1;
```
```
0    SELECT STATEMENT Optimizer=ALL_ROWS (Cost=2 Card=1 Bytes=13)
1  0   COUNT (STOPKEY)
2  1     VIEW (Cost=2 Card=1 Bytes=13)
3  2       TABLE ACCESS (BY INDEX ROWID) OF 'EMP' (TABLE) (Cost=2 Card=1 ...)
4  3         INDEX (RANGE SCAN DESCENDING) OF 'EMP_X1' (INDEX) (Cost=1 Card=5)  
```
* 위 예시는 Top N 쿼리를 이용해 최댓값을 구하는 방법이다.
* `DEPTNO = 30` 조건을 만족하는 가장 오른쪽부터 역순으로 스캔하면서 테이블 액세스를 하다가, `MGR = 7698` 조건을 만족하는 레코드 하나를 찾았을 때 멈추면 된다.
* 인라인 뷰를 사용하기 때문에 더 복잡하긴 하지만, 성능 측면에서는 더 좋다.

### 이력 조회
* 값이 어떻게 변경되어 왔는지 과거 이력을 조회할 필요가 있다면, 이력 테이블을 따로 관리해야 한다.
* 일반적으로는 이 이력 테이블에는 현재 데이터도 저장하는데, 변경 이력을 완벽히 재생할 수 있기 때문이다.
  * 예시로, 특정 컬럼이 특정 값으로 바뀐 날짜를 일고 싶다면 이력 테이블에서 확인해야 한다.


#### 가장 단순한 이력 조회
* 이력 데이터를 조회할 때, First Row Stopkey, Top N Stopkey 알고리즘이 동작할 수 있도록 인덱스 설계 및 SQL을 구현하는 일은 중요하다.
* 아래 예시는 상태코드가 현재 값으로 변경된 날짜를 상태변경이력 테이블을 통해 조회하는 방법이다.
```oracle
-- 상태변경이력 PK인덱스 : 장비번호 + 변경일자 + 변경순번 
SELECT 장비번호, 장비명, 상태코드
     , (SELECT MAX(변경일자)
        FROM 상태변경이력
        WHERE 장비번호 = P.장비번호) 최종변경일자
FROM 장비 P
WHERE 장비구분코드 = 'A001'
```
* 인덱스가 `장비번호 + 변경일자 + 변경순번` 순으로 구성되어 있었기 때문에 First Row Stopkey 알고리즘이 작동할 수 있다.

#### 좀 더 복잡한 이력 조회
* 위 예시에서 최종 변경 순번까지 읽어야 하는 상황이라면, 쿼리가 좀 더 복잡해진다.
```oracle
-- 상태변경이력 PK인덱스 : 장비번호 + 변경일자 + 변경순번
SELECT 장비번호, 장비명, 상태코드
     , SUBSTR(최종이력, 1, 8) 최종변경일자
     , TO_NUMBER(SUBSTR(최종이력 9, 4)) 최종변경순번
FROM (
    SELECT 장비번호, 장비명, 상태코드
         , (SELECT MAX(H.변경일자 || LPAD(H.변경순번, 4))
            FROM 상태변경이력 H
            WHERE 장비번호 = P.장비번호) 최종이력
    FROM 장비 P
    WHERE 장비구분코드 = 'A001'
) 
```
* 위와같이 쿼리를 작성하면, 인덱스 컬럼을 가공했기 때문에 First Row Stopkey 알고리즘이 작동하지 않는다.
* 즉, 장비별 상태변경이력이 많지 않을 경우엔 문제가 안될 순 있지만, 많으면 문제가 된다.
```oracle
-- 상태변경이력 PK인덱스 : 장비번호 + 변경일자 + 변경순번
SELECT 장비번호, 장비명, 상태코드
     , (SELECT MAX(H.변경일자)
        FROM 상태변경이력 H
        WHERE 장비번호 = P.장비번호) 최종변경일자
     , (SELECT MAX(H.변경순번)
        FROM 상태변경이력 H
        WHERE 장비번호 = P.장비번호
          AND 변경일자 = (SELECT MAX(H.변경일자)
                         FROM 상태변경이력 H
                         WHERE 장비번호 = P.장비번호)) 최종변경순번
FROM 장비 P
WHERE 장비구분코드 = 'A001'
```
* 장비별 상태변경이력이 많은 경우, 오히려 위와같이 쿼리를 작성하는게 좋다.
* 상태변경이력을 세 번 조회하는 비효율은 있지만, First Row Stopkey 알고리즘이 잘 작동하므로 성능은 비교적 좋다.
* 이력테이블에서 읽어야 할 컬럼이 더 많다면 쿼리는 점점 더 복잡해진다.
```oracle
-- 상태변경이력 PK인덱스 : 장비번호 + 변경일자 + 변경순번
SELECT 장비번호, 장비명, 상태코드
     , (SELECT MAX(H.변경일자)
        FROM 상태변경이력 H
        WHERE 장비번호 = P.장비번호) 최종변경일자
     , (SELECT MAX(H1.변경순번)
        FROM 상태변경이력 H1
        WHERE 장비번호 = P.장비번호
          AND 변경일자 = (SELECT MAX(H2.변경일자)
                         FROM 상태변경이력 H2
                         WHERE 장비번호 = P.장비번호)) 최종변경순번
     , (SELECT H1.상태코드
        FROM 상태변경이력 H1
        WHERE 장비번호 = P.장비번호
          AND 변경일자 = (SELECT MAX(H2.변경일자)
                         FROM 상태변경이력 H2
                         WHERE 장비번호=P.장비번호)
          AND 변경순번 = (SELECT MAX(H3.변경순번)
                         FROM 상태변경이력 H3
                         WHERE 장비번호 = P.장비번호
                           AND 변경일자 = (SELECT MAX(H4.변경일자)
                                          FROM 상태변경이력 H4
                                          WHERE 장비번호 = P.장비번호))) 최종상태코드
FROM 장비 P
WHERE 장비구분코드 = 'A001'
```


#### INDEX_DESC 힌트 활용
* 단순하게 쿼리를 작성하면서도 성능을 높이기 위해, Top N + 인덱스 힌트를 사용하는 방법이 있다.
```oracle
-- 상태변경이력 PK인덱스 : 장비번호 + 변경일자 + 변경순번
SELECT 장비번호, 장비명
     , SUBSTR(최종이력, 1, 8) 최종변경일자
     , TO_NUMBER(SUBSTR(최종이력 9, 4)) 최종변경순번
     , SUBSTR(최종이력, 13) 최종상태코드
FROM (
    SELECT 장비번호, 장비명
         , (SELECT /*+ INDEX_DESC(X 상태변경이력_PK) */ 
                   변경일자 || LPAD(변경순번, 4) || 상태코드
            FROM 상태변경이력 X
            WHERE 장비번호 = P.장비번호
              AND ROWNUM <= 1) 최종이력
    FROM 장비 P
    WHERE 장비구분코드 = 'A001'
) 
```
* 이 방식은 성능은 좋긴 한데, 인덱스 구성이 완벽해야만 쿼리가 잘 작동한다.
* 다른 대안이 있다면 그 방법을 사용하는게 좋다.

#### 11g/ 12c 신기능 활용
* 바로 이전 예시는 11g 이하 버전에서 실행해보면 ORA-00904 오류가 발생한다.
* 메인쿼리 컬럼을 서브쿼리 내 인라인 뷰에서 참조했기 때문이다.
* 이럴경우 아래와 같이 해결하면 된다.

  ```oracle
  -- 상태변경이력 PK인덱스 : 장비번호 + 변경일자 + 변경순번
  SELECT 장비번호, 장비명
       , SUBSTR(최종이력, 1, 8) 최종변경일자
       , TO_NUMBER(SUBSTR(최종이력 9, 4)) 최종변경순번
       , SUBSTR(최종이력, 13) 최종상태코드
  FROM (
      SELECT 장비번호, 장비명
           , (SELECT 변경일자 || LPAD(X.변경순번, 4) || 상태코드
              FROM (SELECT 장비번호, 변경일자, 변경순번, 상태코드
                    FROM 상태변경이력
                    ORDER BY 변경일자 DESC, 변경순번 DESC)
              WHERE 장비번호 = P.장비번호
                AND ROWNUM <= 1) 최종이력
      FROM 장비 P
      WHERE 장비구분코드 = 'A001'
  ) 
  ```
  * Predicate Pushing이라는 쿼리변환이 작동하여 장비번호 조건이 인라인 뷰 안쪽으로 파고들어가게 된다.
* 12c에서 부터는 파싱 오류 없이 Top N Stopkey 알고리즘이 잘 작동한다.

#### 상황에 따라 달라져야 하는 이력 조회 패턴
* 위 예시들과 달리, 전체 장비의 이력을 대상으로 조회할 때는 인덱스를 활용한 Stopkey 기능 작동여부가 튜닝의 핵심요소는 아니다.
  * 인덱스를 활용한 패턴은 랜덤 I/O 발생량만큼 성능도 비례해서 느려지므로, 대량데이터 조회 시 좋은 솔루션은 되지 못한다.
* 이런 경우엔 아래와 같이 윈도우 함수를 이용하는 것이 효과적이다.
```oracle
SELECT *
FROM 장비 P
   , (SELECT 장비번호, 변경일자, 변경순번, 상태코드
           , ROW_NUMBER() OVER (PARTITION BY 장비번호
                                ORDER BY 변경일자 DESC, 변경순번 DESC) RNUM
      FROM 상태변경이력) H
WHERE H.장비번호 = P.장비번호
  AND H.RNUM = 1;
```
* 위 예시는, Full Scan과 해시 조인을 이용하기 때문에 오랜 과거 이력까지 모두 읽지만, 인덱스를 이용하는 방식보다 빠르다.
* 아래와 같이 KEEP 절을 활용할 수도 있다.
```oracle
SELECT P.장비번호, P.장비명
     , H.변경일자 AS 최종변경일자
     , H.변경순번 AS 최종변경순번
     , H.상태코드 AS 최종상태코드
FROM 장비 P
   , (SELECT 장비번호
           , MAX(변경일자) 변경일자
           , MAX(변경순번) KEEP (DENSE_RANK LAST ORDER BY 변경일자, 변경순번) 변경순번
           , MAX(상태코드) KEEP (DENSE_RANK LAST ORDER BY 변경일자, 변경순번) 상태코드
      FROM 상태변경이력
      GROUP BY 장비번호) H
WHERE H.장비번호 = P.장비번호
```

#### 선분이력
* 선분(라인 세그먼트)이력 모델을 사용하면 아래와 같이 간단한 쿼리료 이력을 쉽게 조회할 수 있다.
```oracle
SELECT P.장비번호, P.장비명
     , H.상태코드, H.유효시작일자, H.유효종료일자, H.변경일자
FROM 장비 P, 상태변경이력 H
WHERE P.장비구분코드 = 'A001'
  AND H.장비번호 = P.장비번호
  AND H.유효종료일자 = '99991231'
```

### Sort Group By 생략
* 인덱스를 이용해 소트 연산을 생략할 수 있는데, 그루핑 연산에도 인덱스를 활용할 수 있다.
* 실행계획에 Sort Gorup By Nosort 가 있다.
```oracle
select region, avg(age), count(*)
from customer
group by region
```
```
0  SELECT STATEMENT Optimizer=ALL_ROWS (Cost=30142 ...)
1  0  SORT GROUP BY NOSORT (Cost=30142 ...)
2  1    TABLE ACCESS (BY INDEX ROWID) OF 'CUSTOMER' (TABLE) (Cost=30142 ...)
3  2      INDEX (FULL SCAN) OF 'CUSTOMER_X01' (INDEX) (Cost=2337 ...)
```

* 위 예시는 `region`이 선두컬럼인 인덱스가 있는 예시이며, 동작 방식은 아래와 같다.
  * 컬럼의 `region` 값별로 구간을 스캔하면서 테이블을 액세스하다가 다음 값을 만나는 순간 운반수단에 적재한다.
  * Array Size 만큼 집계 결과가 쌓였으면 클라이언트에 전송 하고, Fetch Call이 올 때까지 기다린다. (Fetch Call이 안오면 종료)
  * Fetch Call이 오면 다시 다음 값부터 작업을 반복한다.
* 따라서 Nosort 방식으로 Group By를 처리하면 부분범위 처리가 가능해진다.

## Sort Area를 적게 사용하도록 SQL 작성
* 소트 연산이 불가피하다면 메모리 내에서 처리를 할 수 있도록 노력해야 한다.
* Sort Area 크기를 늘리는 방법도 있지만, 일단 Sort Area를 적게 사용할 방법부터 찾는것이 좋다.

### 소트 데이터 줄이기
1) ```oracle
   select lpad(상품번호, 30) || lpad(상품명, 30) || lpad(고객ID, 10)
       || lpad(고객명, 20) || to_char(주문일시, 'yyyymmdd hh24:mi:ss')
   from 주문상품
   where 주문일시 between :start and :end
   order by 상품번호
   ```

2) ```oracle
   select lpad(상품번호, 30) || lpad(상품명, 30) || lpad(고객ID, 10)
       || lpad(고객명, 20) || to_char(주문일시, 'yyyymmdd hh24:mi:ss')
   from (
       select 상품번호, 상품명, 고객ID, 고객명, 주문일시
       from 주문상품
       where 주문일시 between :start and :end
       order by 상품번호
   )
   ```
* 두 예시에서, 위 1번 SQL은 레코드당 107바이트로 가공한 결과집합을 Sort Area에 담는다.
* 하지만 2번 SQL은 가공하지 않은 상태로 정렬을 완료하고 최종 출력 시 가공한다.
* 즉, 2번 SQL이 Sort Area를 적게 사용한다.

1) ```oracle
   select *
   from 예수금원장
   order by 총예수금 desc
   ```

2) ```oracle
   select 계좌번호, 총예수금
   from 예수금원장
   order by 총예수금 desc
   ```
* 위 예시에선 당연히 2번 SQL이 Sort Area를 적게 사용한다.
* 상황에 따라 읽은 데이터량이 크게 달라져 성능도 크게 달라질 수 있다.

### Top N 쿼리의 소트 부하 경감 원리
* 특정 컬럼으로 인덱스가 설계되어 있는 경우엔, 그 컬럼에서 가장 큰 값 10개를 찾는것은 간단하다. (Top N Stopkey)
* 하지만 이러한 인덱스가 없는 상황에서는 아래와 같은 방법이 가장 효과적일 수 있다.
  * 처음 10개의 레코드를 읽어 후보들로 저장해둔다.
  * 그리고 나머지 레코드들을 읽으며, 후보들의 최솟값보다 큰 값이 나타나면 해당 후보를 제거하고 그 값을 넣는다.
  * 반복해서 진행한다.
* 이러한 알고리즘을 Top N 소트 알고리즘이라 한다.

```oracle
select *
from (
    select rownum no, a.*
    from (
        select 거래일시, 체결건수, 체결수량, 거래대금
        from 종목거래
        where 종목코드 = 'KR123456'
          and 거래일시 >= '20180304'
        order by 거래일시
         ) a
    where rownum <= (:page * 10)
     )
where no >= (:page-1)*10 + 1
```
```
    0 STATEMENT
   10   COUNT STOPKEY (cr=690 pr=0 pw=0 time=83318 us)
   10     VIEW (cr=690 pr=0 pw=0 time=83290 us)
   10       SORT ORDER BY STOPKEY (cr=690 pr=0 pw=0 time=83264 us)
49857         TABLE ACCESS FULL 종목거래 (cr=690 pr=0 pw=0 time=299191 us)   
```
* 위 예시는 인덱스로 소트연산을 생략할 수 없어 Table Full Scan 방식으로 처리할 때의 예시이다.
* 실행계획에 Sort Order By 오퍼레이션이 나타나며, 해당 오퍼레이션 옆에 Stopkey가 표시되어 있다.
* 소트연산을 피할 수 없어 Sort Order By 오퍼레이션을 수행하지만, Top N 소트 알고리즘이 작동한다는 의미이다.
* 이 알고리즘이 작동하면 소트연산 횟수와 Sort Area 사용량을 최소화 해준다.
  * page 변수에 1을 입력하면 열 개 원소를 담을 배열 공간만 있으면 된다.
* 이 방식을 이용하면 대상 집합이 아무리 커도 많은 메모리 공간을 사용하지 않으며, 최솟값 최댓값 등의 레코드를 찾을 수 있다.
  * Top N 소트 알고리즘이 소트 연산횟수와 Sort Area 사용량을 줄여주는 원리이다.
* 또한 위 예시에서는 Physical Read(pr)와 Physical Write(pw)가 전혀 발생하지 않은 것을 확인할 수 있다.
  * 위 예시에서 `rownum <= (:page * 10)` 조건을 빼서 Top N 소트 알고리즘이 작동하지 않도록 하면 Physical Read(pr)와 Physical Write(pw)가 발생할 수 있다.
  * 메모리 내에서 정렬하지 못해 디스크를 이용하는 일이 발생하는 것이다.

### 분석함수에서의 Top N 소트
* 윈도우 함수 중, `rank`나 `row_number` 함수는 `max`함수보다 소트 부하가 적다.
* Top N 소트 알고리즘이 작동하기 때문이다.
* 즉, `rank` 대신 `max`를 사용하면 디스크를 사용하는 비중이 증가하는 상황이 생길 수 있다.

# DML 튜닝
## 기본 DML 튜닝
### DML 성능에 영향을 미치는 요소
* 인덱스
* 무결성 제약
* 조건절
* 서브쿼리
* Redo 로깅
* Undo 로깅
* Lock
* 커밋

#### 인덱스
* 테이블에 레코드를 입력하면 인덱스에도 입력해야 한다.
  * 테이블은 Freelist(테이블마다 데이터 입력이 가능한 블록목록을 관리)를 통해 입력할 블록을 할당받지만, 인덱스는 정렬된 자료구조이기 때문에 수직적 탐색을 통해 입력할 블록을 찾아야 한다.
  * 즉 인덱스에 입력하는 과정이 더 복잡하므로 DML 성능에 미치는 영향도 크다.
* DELETE 할 때에도 마찬가지로, 테이블에서 레코드를 하나 삭제하면 인덱스 레코드도 모두 찾아서 삭제해줘야 한다.
* UPDATE 할 때에는 변경된 컬럼을 참조하는 인덱스만 찾아서 변경해주면 된다.
  * 테이블에서 한 건 변경할 때마다 인덱스에는 두개 오퍼레이션이 발생하는데, 인덱스는 정렬된 자료구조이기 때문에 저장위치가 달라지므로 삭제 후 삽입하는 방식으로 처리한다.
* 인덱스 개수가 DML 성능에 미치는 영향이 매우 크기 때문에, 핵심 드탤잭션 테이블에서 인덱스를 하나라도 줄이면 TPS는 그만큼 향상된다.

#### 무결성 제약
* 데이터베이스에 논리적으로 의미 있는 자료만 저장되게 하는 데이터 무결성 규칙은 아래 네가지가 있다.
  * 개체 무결성(Entity Integrity)
  * 참조 무결성(Referential Integrity)
  * 도메인 무결성(Domain Integrity)
  * 사용자 정의 무결성(또는 업무 제약 조건)
* 이러힌 규칙은 DBMS에서 PK, FK, Check, Not Null 같은 제약을 설정하면 완벽하게 데이터 무결성을 지켜낼 수 있다.
* PK, FK 제약은 Check, Not Null 제약보다 성능에 더 큰 영향을 미친다.
  * Check, Not Null은 해당 제약 조건을 준수하는지만 확인해보면 되지만, PK, FK 제약은 실제 데이터를 조회해 봐야 알기 때문이다.

#### 조건절
* 조건절만 포함하는 기본적인 DML문의 경우, SELECT 문과 실행계획이 다르지 않기 때문에 인덱스 튜닝 원리를 그대로 적용할 수 있다.

#### 서브쿼리
* 서브쿼리를 포함하는 DML의 경우, SELECT 문과 실행계획이 다르지 않기 때문에 조인튜닝 원리를 그대로 적용할 수 있다. (특히 서브쿼리 조인)

#### Redo 로깅
* 오라클은 데이터 파일과 컨트롤 파일에 가해지는 모든 변경 사항을 Redo 로그에 기록한다.
* Redo 로그는 트랜잭션 데이터가 유실되었을 때, 트랜잭션을 재현함으로써 유실 이전 상태로 복구하는 데 사용된다.
* 따라서 DML을 수행할 때마다 Redo 로그를 생성해야 하기 때문에 Redo 로깅은 DML 성능에 영향을 미친다.
* INSERT 작업에 대해 Redo 로깅 생략기능을 제공한다.

> Redo 로그의 용도
> 1) Database Recovery (Media Recovery)
>   * 물리적으로 디스크가 깨지는 등의 Media Fail 발생 시 데이터베이스를 복구하기위해 사용한다.
>   * 온라인 Redo 로그를 백업해 둔 Archived Redo 로그를 이용하게 된다.
> 2) Cache Recovery  (Instance Recovery 시 roll forward 단계)
>    * 모든 DBMS가 버퍼캐시를 도입한 이유는 I/O 성능을 높이기 위해서인데, 버퍼캐시는 휘발성이다.
>    * 캐시에 저장된 변경사항이 디스크 상 데이터 블록에 아직 기록되지 않은 상황에서 인스턴스가 비정상적으로 종료되면 그때까지의 작업 내용을 모두 읽게 된다.
>    * 이러한 유실에 대비하기 위해 Redo 로그를 남긴다.
> 3) Fast Commit
>    * 변경된 메모리 버퍼블록을 디스크 상의 데이터 블록에 반영하는 작업은 랜덤 액세스 방식으로 이루어지기 때문에 매우 느리다.
>    * 반면 로그는 Append 방식으로 기록하므로 상대적으로 빠르다.
>    * 따라서 트랜잭션에 의한 변경을 우선 로그파일에 기록 후, 변경된 메모리 버퍼블록과 데이터파일 블록간 동기화는 적절한 수단(DBWR, Checkpoint)을 이용해 나중에 배치방식으로 일괄 수행한다.
>    * 즉 Redo 로그를 믿고 빠르게 커밋을 완료한다는 의미에서 Fast Commit이라고 부른다.

#### Undo 로깅
* 과거엔 Rollback이라는 용어를 주로 사용했지만, 9i부터 오라클은 Undo라는 용어를 사용하고 있다.
* Redo는 트랜잭션을 재현함으로 과거를 현재 상태로 되돌리는데 사용하고, Undo는 트랜잭션을 롤백함으로써 현재를 과거 상태로 되돌리는데 사용한다.
* 따라서 Undo에는 변경된 블록을 이전 상태로 되돌리는데 필요한 정보를 로깅한다.
* DML을 수행할 때마다 Undo를 생성해야 하기 때문에 Undo 로깅은 DML 성능에 영향을 미친다.
* 가장 오래 전에 커밋한 Undo 공간부터 재사용하기 때문에, 곧바로 사라지진 않지만 언젠가 다른 트랜잭션으로 덮어쓰여지며 사라진다.

> Undo 로그의 용도
> 1) Transaction Rollback
>    * 트랜잭션에 의한 변경사항을 최종 커밋하지 않고 롤백하고자 할 때 Undo 데이터를 이용한다.
> 2) Transaction Recovery (Instance Recovery 시 rollback 단계)
>    * Instance Crash 발생 후 Redo를 이용해 Roll forward 단계가 완료되면 최종 커밋되지 않은 변경사항까지 모두 복구된다.
>    * 따라서 시스템이 셧다운 된 시점에 커밋되지 않았던 데이터를 모두 롤백해야 하는데, 이때 Undo 데이터를 사용한다.
> 3) Read Consistency
>    * SQL 튜닝 관점에서 중요한 것은 읽기 일관성이다.
>    * 읽기 일관성을 위해 Consistent 모드로 데이터를 읽는 오라클에선 동시 트랜잭션이 많을수록 I/O가 증가하며 성능저하로 이어지기 때문이다.

> MVCC(Multi-Version Concurrency Control) 모델
> * MVCC 모델을 사용하는 오라클은 데이터를 두 가지 모드로 읽는다.
> 1) Current 모드
>   * 디스크에서 캐시로 적재된 원본(Current) 블록을 현재 상태 그대로 읽는 방식이다.
> 2) Consistent 모드
>   * 쿼리가 시작된 이후에 다른 트랜잭션에 의해 변경된 블록을 만나면 원본 블록으로부터 복사본 블록을 만들고, 거기에 Undo 데이터를 적용하여 쿼리가 시작된 시점으로 되돌려 읽는 방식이다.
>     * 즉, 원본블록 하나에 여러 복사본이 캐시에 존재할 수 있다.
>   * 오라클은 마지막 커밋이 발생한 시점정보를 SCN(System Commit Number)라는 Global 변수로 관리하는데, 이 값은 기본적으로 각 트랜잭션이 커밋할 때마다 1씩 증가하며, 오라클 백그라운드 프로세서에 의해서도 조금씩 증가한다.
>   * 오라클은 각 블록이 마지막으로 변경된 시점을 관리하기 위해 모든 블록 헤더에 SCN을 기록하는데, 이를 블록 SCN이라고 한다.
>   * 그리고 모든 쿼리는 Global 변수인 SCN 값을 먼저 확인하고 읽기 작접을 시작하는데 이를 쿼리 SCN이라고 한다.
>   * Consistent 모드는 쿼리 SCN과 블록 SCN을 비교함으로써 쿼리 수행 도중 블록이 변경되었는지를 확인하며 데이터를 읽는 방식이다.
> * SELECT 문은 몇가지 예외 케이스를 빼곤 항상 Consistent 모드로 데이터를 읽는다.
> * DML문은 Consistent 모드로 대상 레코드를 찾고 Current 모드로 추가/변경/삭제 한다.

#### Lock
* Lock은 DML성능에 매우 크고 직접적인 영향을 미친다.
* Lock을 필요 이상으로 자주, 길게 사용하거나 레벨을 높일수록 DML 성능은 느려진다.
* 하지만 Lock을 너무 적게, 짧게 사용하고나 필요한 레벨 이하로 낮추면 데이터 품질이 나빠진다.
* 따라서 성능과 데이터 품질 두마리 토끼를 다 잡으려면 매우 세심한 동시성제어가 필요하다.
* 동시성 제어는 동시에 실행되는 트랜잭션 수를 최대화 하면서도 입력, 수정, 삭제, 검색 시 데이터 무결성을 유지하기 위해 노력하는것을 말한다.

#### 커밋
* 커밋은 DML과 별개로 실행하지만 DML을 끝내려면 커밋까지 완료해야 하기 때문에 밀접한 관련이 있다.
* DML이 Lock에 의해 블로킹된 경우 커밋은 DML 성능과 직결되는데, DML을 완료할 수 있게 Lock을 푸는 열쇠가 커밋이기 때문이다.
* 모든 DBMS가 Fast Commit을 구현하고 있지만, 결코 가벼운 작업은 아니다.
1) DB 버퍼캐시
   * 서버 프로세스는 버퍼캐시를 통해 데이터를 읽고 쓴다.
   * 버퍼캐시에서 변경된 블록을 모아 주기적으로 데이터파일에 일괄 기록하는 작업 DBWR 프로세스가 맡는다.
2) Redo 로그버퍼
   * 버퍼캐시는 휘발성이기 때문에 DBWR 프로세스가 Dirty 블록들을 데이터파일에 반영할 때까지 불안한 상태라고 생각할 수 있지만, 버퍼캐시에 가한 변경사항을 Redo 로그에도 기록해두었기 때문에 안전하다.
   * 버퍼캐시 데이터가 유실되더라도 Redo 로그를 이용해 언제든 복구할 수 있다.
   * 하지만 Redo 로그도 파일이기 때문에 Append 방식으로 기록하더라도 디스크 I/O는 느리다.
   * Redo 로깅 성능 문제를 해결하기 위해 오라클은 로그버퍼를 이용한다.
   * Redo 로그 파일에 기록하기 전에 로그버퍼에 기록하는 방식이다.
   * 로그버퍼에 기록한 내용은 나중에 LGWR(LogWriter) 프로세스가 Redo 로그 파일에 일괄기록한다.
3) 트랜잭션 데이터 저장 과정
   1) DML 문을 실행하면 Redo 로그버퍼에 변경사항을 기록한다.
   2) 버퍼블록에서 데이터를 변경하는데, 버퍼캐시에서 블록을 찾지 못하면 데이터파일에서 읽는 작업부터 한다.
   3) 커밋한다.
   4) LGWR 프로세스가 Redo 로그버퍼 내용을 로그파일에 일괄 저장한다.
   5) DBWR 프로세스가 변경된 버퍼블록들을 데이터 파일에 일괄 저장한다.
   * 오라클은 데이터를 변경하기 전 항상 로그부터 기록하는데, 이를 Write Ahead Logging이라고 부른다.
   * DBWR와 LGWR 프로세스는 주기적으로 깨어나 각각 Dirty 블록과 Redo 로그버퍼를 파일에 기록한다.
   * LGWR 프로세스는 서버 프로세스가 커밋을 발행했다고 신호를 보낼 때에도 깨어나서 활동을 시작한다.
   * 적어도 커밋 시점에는 Redeo 로그버퍼 내용을 로그파일에 기록한다는 뜻이며, 이것을 Log Force at Commit이라고 한다.
   * 따라서 서버 프로세스가 변경한 버퍼블록들을 디스크에 기록하지 않았더라도 커밋 시점에 Redo 로그를 디스크에 안전하게 기록했다면 그 순간부터 트랜잭션의 영속성은 보장된다.

4) 커밋 = 저장 버튼
   * 데이터베이스 트랜잭션을 문서 작업에 비유하면 커밋은 워드와 같이 문서 작업 도중 저장 버튼을 누르는 것과 같다.
   * 즉, 서버 프로세스가 그때까지 했던 작업을 디스크에 기록하라는 명령어인 것이다.
   * 저장을 완료할 때까지 서버 프로세스는 다음 작업을 진행할 수 없다.
     * Redo 로그버퍼에 기록된 내용을 디스크에 기록하도록 LGWR 프로세스에 신호를 보낸 후 작업을 완료했다는 신호를 받아야 다음 작업을 할 수 있다. (Sync 방식)
   * LGWR 프로세스가 Redo 로그를 기록하는 작업은 디스크 I/O 작업이다.
     * 따라서 커밋은 생각보다 느리다.
   * 너무 오랫동안 커밋하지 않은 채 데이터를 계속 갱신하면 Undo 공간이 부족해져 시스템 장애 상황을 유발할 수 있다.
   * 루프를 돌면서 너무 자주 커밋을 하면 프로그램 자체 성능이 매우 느려진다.

### 데이터베이스 Call과 성능
* SQL 트레이스 Call 통계를 보면 알 수 있듯이, SQL은 아래 세 단계로 나누어 실행된다.
  * Parse Call : SQL 파싱과 최적화를 수행하는 단계다. SQL과 실행계획을 라이브러리 캐시에서 찾으면, 최적화 단계는 생략할 수 있다.
  * Execute Call : SQL을 실행하는 단계다. DML은 이 단계에서 모든 과정이 끝나지만, SELECT 문은 Fetch 단계를 거친다.
  * Fetch Call : 데이터를 읽어서 사용자에게 결과집합을 전송하는 과정으로 SELECT 문에서만 나타난다. 전송할 데이터가 많을 때는 Fetch Call이 여러 번 발생한다.
* Call이 어디서 발생하느냐에 따라 User Call과 Recursive Call로 나눌 수 있다.
  * User Call : 네트워크를 경유해 DBMS 외부로부터 인입되는 Call이다. 3-Tier 아키텍처에서 User Call은 WAS 서버에서 발생하는 Call이다.
  * Recursive Call : DBMS 내부에서 발생하는 Call이다. SQL 파싱과 최적화 과정에서 발생하는 데이터 딕셔너리 조회, PL/SQL로 작성한 사용자 정의 함수/프로시저/트리거에 내장된 SQL을 실행할 때 발생하는 Call이다.
* User Call이든 Recursive Call이든 SQL을 실행할 때마다 Parse, Execute, Fetch Call 단계를 거친다.
* 데이터베이스 Call이 많으면 성능은 느릴 수밖에 없고, 특히 네트워크를 경유하는 User Call이 성능에 미치는 영향은 매우 크다.

#### 절차적 루프 처리
* 데이터베이스 Call이 성능에 미치는 영향에 대한 테스트 결과이다.
* PL/SQL 프로그램상에서 source 테이블을 읽어 100만번 루프를 돌며 건건이 target 테이블에 입력하는 예시가 있다.
* 루프를 돌면서 건건이 Call이 발생했지만, 네트워크를 경유하지 않는 Recursive Call이기 때문에 29초 정도만에 수행을 마쳤다.
* 같은 로직을 JAVA 프로그램으로 수행하면 네트워크를 경유하는 User Call이므로, 성능히 급격히 나빠지는데 약 218초 정도 걸렸다.

#### One SQL의 중요성
* 위 예시를 아래와 같이 Insert Into Select 구문으로 처리할 수 있다.
```oracle
insert into target
select * from source;
```
* 즉, 단 한번의 Call로 처리하니 1.46초만에 수행을 마쳤다.
* 따라서, 업무로직이 복잡하지 않다면 가급적 One SQL로 구현해야 성능이 잘 나온다.
  * Insert Into Select
  * 수정가능 조인 뷰
  * Merge 문

### Array Processing 활용
* 실무에서 복잡한 절차적 프로그램을 One SQL로 구현하는 일은 절대로 쉽지 않다.
* 이럴 때 Array Processing 기능을 활용하면 One SQL로 구현하지 않고도 Call 부하를 획기적으로 줄일 수 있다.

```oracle
declare 
  cursor c is select * from source;
  type typ_source is table of c%rowtype;
  l_source typ_source;
    
  l_array_size number default 10000;

  procedure insert_target(p_source in typ_source) is
  begin
    forall i in p_source.first..p_source.last
      insert into target values p_source(i);
  end insert_target;

begin
  open c;
  loop 
    fetch c bulk collect into l_source limit l_array_size;
    
    insert_target(l_source);
    
    exit when c%notfound;
  end loop;
  
  close c;
  
  commit;
end;
```
* JAVA에서도 `addBatch()`와 `executeBatch()`를 활용해 insert할 값들을 배열에 저장 후, 한번에 insert 하도록 작성할 수 있다.
* 즉, 100만번 발생하던 Call을 많이 줄여서 생기는 성능 향상으로, PL/SQL에서는 29초 -> 4초, JAVA에서는 218초 -> 12초로 많은 성능향상이 나타난다.

### 인덱스 및 제약 해제를 통한 대량 DML 튜닝
* 인덱스와 무결성 제약조건은 DML 성능에 큰 영향을 끼친다.
* 그렇다고 OLTP 시스템에서 이러한 기능을 섣불리 해제할 순 없다.
* 하지만 동시 트랜잭션 없이 대량 데이터를 적재하는 배치 프로그램에서는, 이 기능을 해제함으로 써 큰 성능개선 효과를 얻을 수 있다.

#### PK 제약과 인덱스 해제 1 - PK 제약에 Unique 인덱스를 사용한 경우
* PK제약을 비활성화 하면서 인덱스도 Drop한다.
  ```oracle
  alter table target modify target_pk disable drop index;
  ```
* 일반 인덱스는 Unusable 상태로 변경한다.
  ```oracle
  alert index T2_x1 unusable;
  ```
  * 인덱스가 Unusable인 상태에서 데이터를 입력하려면 skip_unusable_indexes 파라미터를 true로 설정해야 한다.
    ```oracle
    alter session set skip_unusable_indexes = true;
    ```
* 작업이 완료되면 PK 제약을 활성화하고, 일반 인덱스를 재생성한다.
  ```oracle
  alter table target modify constraint target_pk enable NOVALIDATE;
  alter index target_x1 rebuild;
  ```
* 이렇게 하면 데이터 입력 시간과 제약 활성화 및 인덱스 재생성 시간을 합쳐도 더 빠르게 작업을 마칠 수 있다.
* 즉, 인덱스 및 무결성 제약이 DML 성능에 미치는 영향은 아주 크다.

#### PK 제약과 인덱스 해제 2 - PK 제약에 Non-Unique 인덱스를 사용한 경우
* PK 인덱스를 Drop하지 않고 Unusable 상태에서 데이터를 입력하고 싶다면, PK 제약에 Non-Unique 인덱스를 사용하면 된다.
  ```oracle
  altert table target drop primary key drop index;
  
  create index target_pk on target(no, empno); -- Non-Unique 인덱스 생성
  
  alter table target add constraint target_pk primary key(no, empno) using index target_pk; -- PK 제약에 Non-Unique 인덱스 사용하도록 지정
  ```
* 그 후 아래와 같이 PK 제약을 비활성화하고 인덱스를 Unusable 상태로 변경후 작업하면 된다.
  ```oracle
  altet table target modify constraint target_pk disable keep index;
  
  alter index target_pk unusable;
  
  alter index target_x1 unusable;
  ```
* 작업이 종료되면 인덱스를 재생성하고 PK 제약을 다시 활성화한다.
  ```oracle
  alter index target_x1 rebuild;
  
  alter index target_pk rebuild;
  
  alter table target modify constraint target_pk enable novalidate;
  ```

### 수정가능 조인 뷰
#### 전통적인 방식의 UPDATE
```oracle
update 고객 c
set (최종거래일시, 최근거래횟수, 최근거래금액) = 
    (select max(거래일시), count(*), sum(거래금액)
     from 거래
     where 고객번호 = c.고객번호
       and 거래일시 >= trunc(add_months(sysdate, -1)))
where exists (select 'x' from 거래
              where 고객번호 = c.고객번호
                and 거래일시 >= trunc(add_months(sysdate, -1)))
```
* 각 컬럼을 건건이 서브쿼리를 이용해 set 하는 방식은 비효율적이기 때문에, 위와 같이 쿼리를 작성할 수 있다.
* 하지만 위와같은 방법도 한달 이내 고객별 거래 데이터를 두 번 조회하기 때문에 비효율은 있다.
  * 즉, 총 고객수와 한달 이내 거래 고객 수에 따라 성능이 좌우된다.
* 총 고객 수가 아주 많다면 Exists 서브쿼리를 해시 세미조인으로 유도하는 것을 고려할 수 있다.
```oracle
where exists (select /*+ unnest hash_sj */ 'x' from 거래
              where 고객번호 = c.고객번호
                and 거래일시 >= trunc(add_months(sysdate, -1)))
```
* 한달 이내 거래를 발생시킨 고객이 많아 UPDATE 발생량이 많다면 아래와 같이 변경하는 것을 고려할 수 있다.
```oracle
update 고객 c
set (최종거래일시, 최근거래횟수, 최근거래금액) = 
    (select nvl(max(거래일시), c.최종거래일시)
          , decode(count(*), 0, c.최근거래횟수, count(*))
          , nvl(sum(거래금액), c.최근거래금액)
     from 거래
     where 고객번호 = c.고객번호
       and 거래일시 >= trunc(add_months(sysdate, -1)))
```
* 하지만 모든 레코드에 LOCK이 걸리는 것은 물론, 이전과 같은 값으로 갱신되는 비중이 높을수록 Redo 로그 발생량이 증가해 오히려 비효율적일 수 있다.
* 즉 다른 테이블과 조인이 필요한 경우 전통적인 UPDATE문을 사용하면 비효율을 완전히 없앨 순 없다.

#### 수정가능 조인 뷰
* 수정가능 조인 뷰를 활용하면 참조 테이블과 두 번 조인하는 비효율을 없앨 수 있다.
  ```oracle
  update
  (select /*+ ordered use_hash no_merge(t) */
          c.최종거래일시, c.최근거래횟수, c.최근거래금액
        , t.거래일시, t.거래횟수, t.거래금액
   from (select 고객번호
              , max(거래일시) 거래일시, count(*) 거래횟수, sum(거래금액) 거래금액
         from 거래
         where 거래일시 >= trunc(add_months(sysdate, -1))
         group by 고객번호) t
       , 고객 c
   where c.고객번호 = t.고객번호
  )
  set 최종거래일시 = 거래일시
    , 최근거래횟수 = 거래횟수
    , 최근거래금액 = 거래금액
  ```
  * 해당 쿼리는 12c 이상에서만 정상적으로 실행되며, 10g 이하 버전은 UPDATE 옆에 `bypass_ujvc` 힌트를 사용해야 하고, 11g에서는 실행되지 않는다. 
* 조인 뷰는 FROM 절에 두 개 이상 테이블을 가진 뷰를 가리키며, 수정가능 조인뷰는 말 그대로 입력, 수정, 삭제가 허용되는 조인뷰를 의미한다.
* 하지만 수정가능 조인 뷰를 사용하려면 조건이 필요하다.
* 조인뷰에서 DML이 허용되려면, 그 테이블이 키 보존 테이블 이어야 한다.

#### 키 보존 테이블이란?
* 키 보존 테이블이란 조인된 결과집합을 통해서도 중복 값 없이 Unique하게 식별이 가능한 테이블을 의미한다.
  * Unique한 1쪽 집합과 조인되는 테이블이어야 조인된 결과집합을 통한 식별이 가능하다.
* 뷰에 rowid를 제공하는 테이블이라고 생각하면 된다.

#### ORA-01779 오류 회피
```oracle
update
(select d.deptno, d.avg_sal as d_avg_sal, e.avg_sal as e.avg_sal
 from (select deptno, round(avg(sal), 2) avg_sal from emp group by deptno) e
     , dept d
 where d.deptno = e.deptno)
set d_avg_sal = e.avg_sal
```
* 11g 이하 버전에서는 위 UPDATE문을 실행하면 ORA-01779 에러가 발생한다.
  * `emp` 테이블을 `detpno` 로 group by 했기 때문에, `deptno` 컬럼으로 조인한 `dept` 테이블은 키가 보존되더라도 옵티마이저가 불필요한 제약을 가한 것이다.
* 이럴 경우엔 10g에선 아래와 같이 `bypass_ujvc` 힌트를 이용해 제약을 회피할 수 있다.
  ```oracle
  update /* bypass_ujvc */
  (select d.deptno, d.avg_sal as d_avg_sal, e.avg_sal as e.avg_sal
  from (select deptno, round(avg(sal), 2) avg_sal from emp group by deptno) e
  , dept d
  where d.deptno = e.deptno)
  set d_avg_sal = e.avg_sal
  ```
  * Updateable Join View Check를 생략하라고 옵티마이저에 지시하는 힌트이다.
* 하지만 11g부터 이 힌트를 사용할 수 없게되었기 때문에 MERGE문으로 바꿔줘야 한다.
  * `bypass_ujvc`힌트 사용이 중단되었을 뿐 수정가능 조인 뷰는 사용할 수 있다.
  * 11g에서도 1쪽집합에 Unique 인덱스가 있으면 수정가능 조인 뷰를 이용한 UPDATE가 가능하다.
* 12c부터는 수정가능 조인 뷰가 개선이 되었고, 위 예시가 힌트 없이도 잘 동작하게 되었다.

### MERGE 문 활용
* DW(Data Warehouse)에서 가장 흔히 발생하는 오퍼레이션은, 기간계 시스템에서 가져온 신규 트랜잭션 데이터를 반영함으로써 두 시스템 간 데이터를 동기화하는 작업이다.
* 고객 테이블에 발생한 변경분 데이터를 DW에 반영하는 프로세스는 아래와 같다.
  1) 전일 발생한 변경데이터를 기간계 시스템으로부터 추출(Extraction)
  ```oracle
  create table customer_delta
  as
  select * from customer
  where mod_dt >= trunc(sysdate) - 1
    and mod_dt < trunc(sysdate)
  ```
  2) CUSTOMER_DELTA 테이블을 DW 시스템으로 전송(Transportation)
  3) DW 시스템으로 적재(Loading)
  ```oracle
  merge into customer t using customer_delta s on (t.cust_id = s.cust_id)
  when matched then update
    set t.cust_nm = s.cust_nm, t.email = s.email, ...
  when not matched then insert
    (cust_id, cust_nm, email, tel_no, region, addr, reg_dt) values
    (s.cust_id, s.cust_nm, s.email, s.tel_no, s.region, s.addr, s.reg_dt);
  ```
* 이 중 3번 데이터 적재 작업을 효과적으로 지원하기 위해 오라클 9i에서 MERGE문이 도입되었다.
* MERGE문은 Source 테이블 기준으로 Target 테이블과 Left Outer 방식으로 조인해 조인에 성공하면 UPDATE, 실패하면 INSERT한다.
  * MERGE문을 UPSERT라고도 부르는 이유이다.

#### Optional Clauses
* UPDATE와 INSERT를 선택적으로 처리할 수도 있다.
* 이 기능을 통해 수정가능 조인 뷰 기능을 대체할 수 있게 되었다.
```oracle
-- 수정가능 조인 뷰
update
(select d.deptno, d.avg_sal as d_avg_sal, e.avg_sal as e_avg_sal
 from (select deptno, rount(avg(sal), 2) avg_sal from emp group by deptno) e
     , dept d
 where d.deptno = e.deptno)
set d_avg_sal = e_avg_sal;

-- Merge 문
merge into dept d
using (select deptno, rount(avb(sal), 2) avg_sal from emp group by deptno) e
on (d.deptno = e.deptno)
when matched then update set d.avg_sal = e.avg_sal;
```

#### Conditional Operations
* ON 절에 기술한 조인문 외에 추가로 조건절을 기술할 수도 있다.
```oracle
merge into customer t using customer_delta s on (t.cust_id = s.cust_id)
when matched then update
  set t.cust_nm = s.cust_nm, t.email = s.email, ...
  where reg_dt >= to_date('20000101', 'yyyymmdd')
when not matched then insert
  (cust_id, cust_nm, email, tel_no, region, addr, reg_dt) values
  (s.cust_id, s.cust_nm, s.email, s.tel_no, s.region, s.addr, s.reg_dt)
  where reg_dt < trunc(sysdate);
```

#### DELETE Clause
* 이미 저장된 데이터를 조건에 따라 지우는 기능도 제공한다.
```oracle
merge into customer t using customer_delta s on (t.cust_id = s.cust_id)
when matched then update
  set t.cust_nm = s.cust_nm, t.email = s.email, ...
  delete where t.withdraw_dt is not null -- 탈퇴일시가 null이 아닌 레코드 삭제
when not matched then insert
  (cust_id, cust_nm, email, tel_no, region, addr, reg_dt) values
  (s.cust_id, s.cust_nm, s.email, s.tel_no, s.region, s.addr, s.reg_dt);
```
* 위 예시에서 기억해야할 부분은, UPDATE가 이루어진 결과로 탈퇴일시가 Null이 아닌 레코드만 삭제한다.
  * 따라서, 탈퇴일시가 Null이 아니었어도 MERGE문을 수행한 결과가 Null이면 삭제하지 않는다.
* MERGE 문 DELETE절은 조인에 성공한 데이터만 삭제할 수 있다.
  * Source 테이블에서 삭제된 데이터는 Target 데이터에서도 지우고 싶을 때, MERGE 문 DELETE절이 그 역할까진 할 수 없다.
  * 즉, 조인에 실패한 데이터는 UPDATE도 할 수 없고 DELETE도 할 수 없다.

#### Merge Into 활용 예
* 저장하려는 레코드가 기존에 있던 것이면 UPDATE를 수행하고, 그렇지 않으면 INSERT를 하려고 한다.
* 이럴 때 MERGE 문을 활용하면 SQL을 한번만 수행하도록 할 수 있다.

> 수정가능 조인 뷰 vs MERGE 문
> * UPDATE 대신 MERGE 문을 사용하는 개발자가 늘고 있다.
> * 실행계획만 같다면 UPDATE문을 사용하던 MERGE문을 사용하던 상관은 없다.
> * 하지만 아래와 같은 케이스는 문제가 된다.
> ```oracle
> MERGE INTO EMP T2
> USING (SELECT T.ROWID AS RID, S.ENAME
>        FROM EMP T, EMP_SRC S
>        WHERE T.EMPNO = S.EMPNO
>          AND T2.ENAME <> S.ENAME) S
> ON (T2.ROWID = S.RID)
> WHEN MATCHED THEN UPDATE SET T2.ENAME = S.ENAME; 
> ```
> * 위 패턴은 UPDATE 대상 테이블인 EMP를 두 번 액세스 하기 때문에 성능에 좋지 않다.
> ```oracle
> MERGE INTO EMP T
> USING EMP_SRC S
> ON (T.EMPNO = S.EMPNO)
> WHEN MATCHED THEN UPDATE SET T.ENAME = S.ENAME
> WHERE T.ENAME <> S.ENAME; 
> ```
> * 위와같이 작성하는게 좋고, 차라리 아래와 같이 수정가능 조인 뷰를 이용한 UPDATE 문을 사용하는게 편할 수 있다.
> ```oracle
> UPDATE (
>   SELECT S.ENAME AS S_ENAME, T.ENAME AS T_ENAME
>   FROM EMP T, EMP_SRC S
>   WHERE T.EMPNO = S.EMPNO
>     AND T.ENAME <> S.ENAME
> )
> SET T_ENAME = S_ENAME
> ```
> * 물론 EMP_SRC 테이블 EMPNO 컬럼에 Unique 인덱스가 생성되어있어야 한다.

## Direct Path I/O 활용
* OLTP 시스템에선 버퍼캐시가 성능 향상에 도움을 준다.
* 하지만 정보계 시스템(DW/OLAP 등)이나 배치 프로그램에서 사용하는 SQL은 주로 대량 데이터를 처리하기 때문에 버퍼캐시를 경유하는 I/O 메커니즘이 오히려 성능을 떨어뜨릴 수 있다.
* 그래서 오라클은 버퍼캐시를 경유하지 않고 바로 데이터블록을 읽고 쓸 수 있는 Direct Path I/O 기능을 제공한다.

### Direct Path I/O
* 일반적으로 블록 I/O는 DB 버퍼캐시를 경유한다.
  * 읽고자 하는 블록을 먼저 버퍼캐시에서 찾아보고, 찾지 못할 때만 디스크에서 읽는다.
  * 데이터를 변경할 때도 먼저 블록을 버퍼캐시에서 찾는다.
  * 찾은 버퍼블록에 변경을 가하고 나면, DBWR 프로세스가 변경된 블록들을 주기적으로 찾아 데이터파일에 반영해준다.
* 자주 읽는 블록에 대한 반복저인 I/O Call을 줄임으로써 시스템 전반적인 성능을 높이려고 버퍼캐시를 이용하지만, 대량 데이터를 읽고 쓸 때 건건이 버퍼캐시를 탐색한다면 성능에는 오히려 안좋다.
  * 버퍼캐시에서 블록을 찾을 가능성이 거의 없기 때문이다.
  * 대량 블록을 건건이 디스크로부터 버퍼캐시에 적재하고서 읽어야 하는 부담도 크다.
  * Full Scan 의주로 가끔 수행되는 대용량 처리 프로그램이 읽어들인 데이터는 대개 재사용성이 낮다.
* 이러한 데이터 블록들이 버퍼캐시를 점유한다면 다른 프로그램 성능에도 나쁜 영향을 미친다.
* 그래서 오라클은 버퍼캐시를 경유하지 않고 곧바로 데이터 블록을 읽고 쓸 수 있는 Direct Path I/O 기능을 제공한다.
  * 병령 쿼리로 Full Scan을 수행할 때 (중요)
  * 병렬 DML을 수행할 때 (중요)
  * Direct Path Insert를 수행할 때 (중요)
  * Temp 세그먼트 블록들을 읽고 쓸 때
  * direct 옵션을 지정하고 export를 수행할 때
  * nocache 옵션을 지정한 LOB 컬럼을 읽을 때

> 병렬쿼리
> * 쿼리문에 `parallel` 또는 `parallel_index` 힌트를 사용하면 지정한 병렬도 만큼 병렬 프로세스가 떠서 동시에 작업을 진행한다.
> ```oracle
> select /*+ full(t) parallel(t 4) */ * from big_table t;
> ```
> ```oracle
> select /*+ index_ffs(t big_table_x1) parallel_index(t big_table_x1 4) */ count(*) from big_table t;
> ```
> * 위처럼 병렬도를 4로 지정하면 성능이 네 배 빨라지는게 아니라 수십 배 빨라진다.
> * Direct Path I/O 때문에 나타나는 효과로, 버퍼캐시를 탐색하지 않고 디스크로부터 버퍼캐시에 적재하는 부담도 없으니 빠른 것이다.
> * Order by, Group By, 해시 조인, 소트 머지 조인 등을 처리할 때는 힌트로 지정한 병렬도보다 두 배 많은 프로세스가 사용된다.

### Direct Path Insert
* 일반적인 INSERT가 느린 이유는 다음과 같다.
  * 데이터를 입력할 수 있는 블록을 Freelist에서 찾는다.
    * Freelist : 테이블 HWM(High-Water-Mark) 아래쪽에 있는 블록 중 데이터 입력이 가능한 블록을 목록으로 관리
  * Freelist에서 할당받은 블록을 버퍼캐시에서 찾는다.
  * 버퍼캐시에 없으면, 데이터파일에서 읽어 버퍼캐시에 적재한다.
  * INSERT 내용을 Undo 세그먼트에 기록한다.
  * INSERT 내용을 Redo 로그에 기록한다.
* Direct Path Insert 방식을 사용하면 대량의 데이터를 일반적인 INSERT 보다 훨씬 빠르게 입력할 수 있다.
* Direct Path Insert 방식으로 입력하는 과정은 아래와 같다.
  * INSERT ... SELECT 문에 append 힌트 사용
  * parallel 힌트를 이용해 병렬모드로 INSERT
  * direct 옵션을 지정하고 SQL Loader(sqlldr)로 데이터 적재
  * CTAS(create table ... as select) 문 수행
* Direct Path Insert 방식이 빠른 이유는 다음과 같다.
  * Freelist를 참조하지 않고 HWM 바깥 영역에 데이터를 순차적으로 입력한다.
  * 블록을 버퍼캐시에서 탐색하지 않는다.
  * 버퍼캐시에 적재하지 않고, 데이터파일에 직접 기록한다.
  * Undo 로깅을 안한다.
  * Redo 로깅을 안 하게 할 수 있다. (Direct Path Insert가 아닌 일반 Insert 문에 로깅하지 않게 하는 방법은 없음)
    ```
    alter table t NOLOGGING
    ```
* Array Processing도 Dircet Path Insert 방식으로 처리할 수 있다.
* `append_values` 힌트를 사용하면 된다.
  ```oracle
  ...
    insert /*+ append_values */ into target values p_source(i);
  ...
  ```

#### Direct Path Insert 주의사항
* 성능은 매우 빨라지지만, Exclusive 모드 TM Lock이 걸린다.
  * 커밋하기 전까지 다른 트랜잭션은 해당 테이블에 DML을 수행하지 못한다.
  * 따라서, 트랜잭션이 빈번한 주간에 이 옵션은 절대 사용하면 안된다.
* Freelist를 조회하지 않고 HWM 바깥 영역에 입력하므로 테이블 여유 공간이 있어도 재활용하지 않는다.
  * 과거 데이터를 주기적으로 DELETE 해서 여유공간이 생겨도 이 방식으로만 계속 INSERT하는 테이블은 사이즈가 줄지 않고 계속 늘어만 간다.
  * Range 파티션 테이블이라면 과거 데이터를 DELETE가 아닌 DROP 방식으로 지워야 공간 반환이 제대로 이루어진다.
  * 비파티션 테이블이라면 주기적으로 Reorg 작업을 수행해 줘야 한다.

### 병렬 DML
* 병렬 쿼리와 병렬 DDL은 기본적으로 활성화되어 있어 언제든 바로 병렬 처리 가능하다.
* 하지만 병렬 DML은 기본적으로 비활성화 되어있다.
* 따라서 DML을 병렬로 처리하려면 아래와 같이 병렬 DML을 활성화해야 한다.
  ```oracle
  alter session enable parallel dml;
  ```
* 그리고 `parallel` 힌트를 사용하면, 대상 레코드를 찾는 작업(INSERT는 SELECT 쿼리, UPDATE/DELETE는 조건절 검색)은 물론 데이터 추가/변경/삭제도 병렬로 진행한다.
```oracle
insert /*+ parallel(c 4) */ into 고객 c
select /*+ full(o) parallel(o 4) */ * from 외부가입고객 o;

update /*+ full(c) parallel(c 4) */ 고객 c 
set 고객상태코드 = 'WD'
where 최종거래일시 < '20100101';

delete /*+ full(c) parallel(c 4) */ from 고객 c
where 탈퇴일시 < '20100101';
```

* 힌트를 사용했지만 병렬 DML을 활성화하지 않은 경우 대상 레코드를 찾는 작업은 병렬로 진행하지만, 추가/변경/삭제는 QC(Query Coordinator)가 혼자 담당하기 때문에 병목이 생긴다.
  * 오라클은 DML 문에 두 단계 전략을 사용하는데, Consistent 모드로 대상 레코드를 찾고 Current 모드로 추가/변경/삭제 한다.
* 병렬 INSERT는 `append` 힌트를 지정하지 않아도 Direct Path Insert 방식을 사용한다.
  * 하지만 병렬 DML이 작동하지 않을 경우를 대비해 `append`힌트를 같이 사용하는게 좋다.
  * QC가 Direct Path Insert를 사용하면 어느 정도 만족할 만한 성능을 낼 수 있기 때문이다.
* 12c부터는 `enable_parallel_dml` 힌트도 지원한다.
```oracle
insert /*+ enable_parallel_dml parallel(c 4) */ into 고객 c
select /*+ full(o) parallel(o 4) */ * from 외부가입고객 o;
    
update /*+ enable_parallel_dml full(c) parallel(c 4) */ 고객 c 
set 고객상태코드 = 'WD'
where 최종거래일시 < '20100101';
```
* 병렬 DML을 사용하면 테이블에 Exclusive 모드 TM Lock이 걸리기 때문에 트랜잭션이 빈번한 주간에 이 옵션은 절대 사용하면 안된다.

#### 병렬 DML이 잘 작동하는지 확인하는 방법
* DML 작업을 각 병렬 프로세스가 처리하는지, QC가 처리하는지를 실행계획에서 확인할 수 있다.
* UPDATE(또는 DELETE/INSERT)가 `PX COORDINAOTR` 아래쪽에 나타나면 UPDATE를 각 병렬 프로세스가 처리한다.
* 하지만 UPDATE(또는 DELETE/INSERT)가 `PX COORDINAOTR` 위쪽에 나타나면 UPDATE를 QC가 처리한다.

## 파티션을 활용한 DML 튜닝
* 파티션을 이용하면 대량 추가/변경/삭제 작업을 빠르게 처리할 수 있다.

### 테이블 파티션
* 파티셔닝은 테이블 또는 인덱스 데이터를 특정 컬럼 값에 따라 별도 세그먼트에 나눠서 저장하는 것을 말한다.
* 파티션이 필요한 이유를 관리적 측면과 성능적 측면으로 요약하면 아래와 같다.
  * 관리적 측면 : 파티션 단위 백업, 추가, 삭제, 변경 -> 가용성 향상
  * 성능적 측면 : 파티션 단위 조회 및 DML, 경합 또는 부하 분산
* 파티션에는 Range, 해시, 리스트 세 종류가 있다.

#### Range 파티션
* Range 파티션은 오라클 8 버전부터 제공된 가장 기초적인 방식으로 주로 날짜 컬럼을 기준으로 파티셔닝한다.

```oracle
create table 주문 ( 주문번호 number, 주문일자 varchar2(8), 고객ID varchar2(5)
                  , 배송일자 varchar2(8), 주문금액 number, ... )
partition by range(주문일자) (
  partition P2017_Q1 values less than ('20170401')
, partition P2017_Q2 values less than ('20170701')
, partition P2017_Q3 values less than ('20171001')
, partition P2017_Q4 values less than ('20180101')
, partition P2018_Q1 values less than ('20180401')
, partition P9999_MX values less than (MAXVALUE)
);
```
* 위와 같은 파티션 테이블에 값을 입력하면, 각 레코드를 파티션 키 값에 따라 분할 저장하고, 읽을 때에도 검색 조건을 만족하는 파티션만 골라서 읽을 수 있어 이력성 데이터를 Full Scan 방식으로 조회할 때 성능을 크게 향상한다.
* 보관주기 정책에 따라 과거 데이터가 저장된 파티션 백업하고 삭제하는 등 데이터 관리 작업을 효율적이고 빠르게 수행할 수 있는 장점도 있다.
* 파티션 테이블에 대한 SQL 성능 향상 원리는 파티션 Pruning(=Elimination)에 있다. (prune : 쓸데없는 가지를 치다, 불필요한 부분을 제거하다.)
* 파티션 Pruning은 SQL 하드파싱이나 실행시점에 조건절을 분석해서 읽지 않아도 되는 파티션 세그먼트를 액세스 대상에서 제외하는 기능이다.
* 대량의 테이블을 조회할 때, 파티션과 병렬처리가 만나면 그 효과는 배가 된다.
* 파티션도 클러스터, IOT와 마찬가지로 관련 있는 데이터가 흩어지지 않고 물리적으로 인접하도록 저장하는 클러스터링 기술에 속한다.
  * 클러스터와 다른 점은 세그먼트 단위로 모아서 저장한다는 점이다.
  * 클러스터는 데이터를 블록 단위로 모아 저장한다.
  * IOT는 데이터를 정렬된 순서로 저장하는 구조이다.
    
#### 해시 파티션
* 해시 파티션은 오라클 8i부터 제공하기 시작했다.
* 파티션 키 값을 해시 함수에 입력해 반환받은 값이 같은 데이터를 같은 세그먼트에 저장하는 방법이다.
* 파티션 개수만 사용자가 결정하며, 데이터를 분산하는 알고리즘은 오라클 내부 해시함수가 결정한다.
* 해시 파티션은 고객ID처럼 변별력이 좋고 데이터 분포가 고른 컬럼을 파티션 기준으로 선정해야 효과적이다.
```oracle
create table 고객 ( 고객ID varchar2(5), 고객명 varchar2(10), ... )
partition by hash(고객ID) partitions 4;
```
* 검색할 때는 조건절 비교 값에 똑같은 해시 함수를 적용함으로써 읽을 파티션을 결정한다.
* 해시 알고리즘 특성 상 등치조건 또는 IN-List 조건으로 검색할 때만 파티션 Pruning이 작동한다.

#### 리스트 파티션
* 리스트 파티션은 오라클 9i부터 제공하기 시작했다.
* 사용자가 정의한 그룹핑 기준에 따라 데이터를 분할 저장하는 방식이다.
```oracle
create table 인터넷매물 ( 물건코드 varchar2(5), 지역분류 varchar2(4), ... )
partition by by list(지역분류) (
  partition P_지역1 values ('서울')
, partition P_지역2 values ('경기', '인천')
, partition P_지역3 values ('부산', '대구', '대전', '광주')
, partition P_기타 values (DEFAULT)
);
```
* Range 파티션에서는 값의 순서에 따라 저장할 파티션이 결정되지만, 리스트 파티션에서는 순서와 상관업시 불연속적인 값의 목록에 의해 결정된다.
* 해시 파티션은 오라클이 정한 해시 알고리즘에 따라 임의로 분할하지만, 리스트 파티션은 사용자가 정의한 논리적인 그룹에 따라 분할한다.
* 업무적인 친화도에 따라 그룹핑 기준을 정하되, 될 수 있으면 각 파티션에 값이 고르게 분산되도록 해야 한다.

### 파티션 인덱스
* 파티션 테이블은 아래와 같이 구분된다.
  * 비파티션 테이블
  * 파티션 테이블
* 파티션 인덱스는 아래와 같이 구분되는데, 각 파티션이 커버하는 커버하는 테이블 파티션 범위에 따라 로컬과 글로벌로 나뉜다.
  * 비파티션 인덱스
  * 파티션 인덱스
    * 로컬 파티션 인덱스
    * 글로벌 파티션 인덱스
* 로컬 파티션 인덱스는 각 테이블 파티션과 인덱스 파티션이 서로 1:1 대응관계가 되도록 오라클이 자동으로 관리하는 파티션 인덱스를 말한다.
* 로컬이 아닌 파티션 인덱스는 모두 글로벌 파티션 인덱스이며, 테이블 파티션과 독립적인 구성을 갖는다.
  * 비파티션 테이블에 생성 불가


#### 로컬 파티션 인덱스
* 파티션 테이블에 로컬 파티션 인덱스를 추가하는 방법은, `CREATE INDEX` 문 뒤에 `LOCAL` 옵션을 추가하면 된다.
```oracle
create index 주문_x01 on 주문 ( 주문일자, 주문금액 ) LOCAL;
create index 주문_x02 on 주문 ( 고객ID, 주문일자 ) LOCAL;
```
* 각 로컬 인덱스 파티션은 테이블 파티션 속성을 그대로 상속받는다.
* 테이블 파티션 키가 주문일자이면, 로컬 인덱스 파티션 키도 주문일자가 된다.
* 로컬 파티션 인덱스를 로컬 인덱스라고 줄여서 부르기도 한다.
* 로컬 파티션 인덱스는 테이블과 정확히 1:1 대응 관계를 갖도록 오라클이 파티션을 자동으로 관리해준다.
* 파티션 테이블의 파티션 구성을 변경(add, drop, exchange 등)하더라도 인덱스를 재생성할 필요가 없다.
* 변경작업이 순식간에 끝나기 때문에, 피크 시간대만 피하면 서비스를 중단하지 않고도 작업할 수 있다.

#### 글로벌 파티션 인덱스
* 글로벌 파티션 인덱스는 파티션을 테이블과 다르게 구성한 인덱스이다.
  * 파티션 유형이 다르거나, 파티션 키가 다르거나, 파티션 기준값 정의가 다른 경우
* 비파티션 테이블이어도 인덱스는 파티셔닝할 수 있다.
```oracle
create index 주문_x03 on 주문( 주문금액, 주문일자 ) GLOBAL
partition by range(주문금액) (
  partition P_01 values less than ( 10000 )
, partition P_MX values less than ( MAXVALUE )   
);
```
* 글로벌 파티션 인덱스는 테이블 파티션 구성을 변경하는 순간 Unusable 상태로 바뀌기 때문에 인덱스를 바로 재생성해줘야 한다.
* 그동안 해당 테이블을 사용하는 서비스를 중단해야 한다.

#### 비파티션 인덱스
* 비파티션 인덱스는 말 그대로 파티셔닝하지 않은 인덱스다.
* 비파티션 인덱스는 여러 테이블 파티션을 가리키기 때문에, 글로벌 비파티션 인덱스라고 부르기도 한다.
* 비파티션 인덱스는 테이블 파티션 구성을 변경하는 순간 Unusable 상태로 바뀌므로 곧바로 인덱스를 재생성해줘야 한다.
* 그동안 해당 테이블을 사용하는 서비스를 중단해야 한다.

#### Prefixed vs Nonprefixed
* 파티션 인덱스를 Prefixed와 Nonprefixed로 나눌 수 있다.
  * Prefixed : 인덱스 파티션 키 컬럼이 인덱스 키 컬럼 왼쪽 선두에 있다.
  * Nonprefixed : 인덱스 파티션 키 컬럼이 인덱스 키 컬럼 왼쪽 선두에 위치하지 않는다. (아예 속하지 않는 경우도 포함)
* 인덱스 파티션 키 컬럼이 인덱스 구성상 왼쪽 선두 컬럼에 위치하는지에 따른 구분이다.
* 글로벌 파티션은 Nonprefixed를 사용할 수 없다.
* 글로벌 파티션 인덱스는 Prefixed 파티션만 지원되므로, 비파티션 인덱스를 포함해 아래 4가지 유형으로 정리된다.
  * 로컬 Prefixed 파티션 인덱스
  * 로컬 Nonprefixed 파티션 인덱스
  * 글로벌 Prefixed 파티션 인덱스
  * 비파티션 인덱스

#### 중요한 인덱스 파티션 제약
* **Unique 인덱스를 파티셔닝하려면, 파티션 키가 모두 인덱스 구성 컬럼이어야 한다.**
  * 이 조건은 DML 성능 보장을 위해 당연히 있어야 할 제약조건이다.
  * 파티션 키 조건 없이 PK 인덱스로 액세스하는 수많은 쿼리 성능을 위해서도 필요하다.
* 이 제약으로 인해 PK 인덱스를 로컬 파티셔닝하지 못하면 파티션 Drp, Truncate, Exchange, Split, Merge 같은 파티션 구조 변경 작업도 쉽지 않다.
  * 이 작업을 하는 순간 PK 인덱스가 Unusable 상태로 바뀌기 때문이다.
  * 곧바로 Rebuild를 하면 되지만 그동안 해당 테이블을 사용하는 서비스를 중단해야 한다.
* 따라서 **서비스 중단 없이 파티션 구조를 빠르게 변경하려면, PK를 포함한 모든 인덱스가 로컬 파티션 인덱스이어야 한다.**
* 파티션을 활용한 대량 UPDATE/DELETE/INSERT는 파티션 구조 변경 작업을 수반하며 ILM(Information Lifecycle Management)을 지원하는 아주 중요한 기능이다.
  * 이 기능을 활용해 ILM 관리체계를 효과적으로 운영하려면 가급적 인덱스를 로컬 파티션으로 구성해야 하며, 설계할 때부터 PK를 잘 구성해 줘야 한다.
  * 즉, 대량으로 데이터를 추가/변경/삭제하는 기준 컬럼을 PK에 포함하려고 노력해야 한다.

### 파티션을 활용한 대량 UPDATE 튜닝
* 대량 데이터를 입력/수정/삭제할 때는 인덱스를 Drop 하거나 Unusable 상태로 변경하고 작업하는 방법을 많이 활용한다.
* 이 방법의 손익 분기점은 약 5% 정도(테이블이 클수록 더 낮아짐)로 , 데이터 비중이 5%를 넘는다면 이 방식이 더 빠르다는 의미이다.
* 하지만 이 방법도, 초대용량 테이블일 경우엔 인덱스를 재생성하는 부담도 커서 쉽지 않다.

#### 파티션 Exchange를 이용한 대량 데이터 변경
* 테이블이 파티셔닝 되어 있고,인덱스도 로컬 파티션이라면 좋은 방법이 생긴다.
* 수정된 값을 갖는 임시 세그먼트를 만들어 원본 파티션과 바꿔치기하는 방식이다.

1. 임시테이블을 생성한다. 할 수 있다면 nologging 모드로 생성한다.
```oracle
create table 거래_t
nologging
as
select * from 거래 where 1 = 2;
```
2. 데이터를 읽어 임시 테이블에 입력하면서 상태코드 값을 수정한다.
```oracle
insert /*+ append */ into 거래_t
select 고객번호, 거래일자, 거래순번, ...
     , (case when 상태코드 <> 'ZZZ' then 'ZZZ' else 상태코드 end) 상태코드
from 거래
where 거래일자 < '20250101';
```
3. 임시 테이블에 원본 테이블과 같은 구조로 인덱스를 생성한다. 할 수 있다면 nologging 모드로 생성한다.
```oracle
create unique index 거래_t_pk on 거래_t(고객번호, 거래일자, 거래순번) nologging;
create index 거래_t_x1 on 거래_t(거래일자, 고객번호) nologging;
create index 거래_t_x2 on 거래_t(상태코드, 거래일자) nologging;
```
4. 특정 파티션과 임시 테이블을 Exchange 한다.
```oracle
alter table 거래
exchange partition p202412 with table 거래_t
including indexes without validation;
```
5. 임시 테이블을 Drop 한다.
```oracle
drop table 거래_t;
```
6. nologging 모드로 작업했다면, 파티션을 logging 모드로 전환한다.
```oracle
alter table 거래 modify partition p202412 logging;
alter index 거래_pk modify partition p201412 logging;
alter index 거래_x1 modify partition p201412 logging;
alter index 거래_x2 modify partition p201412 logging;
```

### 파티션을 활용한 대량 DELETE 튜닝
* 아래와 같은 조건절로 수천만 건 데이터를 삭제할 때도, 인덱스를 실시간으로 관리하려면 엄청난 시간이 소요된다.
* 그렇다고 초대용량 테이블 인덱스를 모두 Drop 했다가 다시 생성하는것도 쉽지 않다.
> DELETE가 느린 이유
> 1. 테이블 레코드 삭제
> 2. 테이블 레코드 삭제에 대한 Undo Logging
> 3. 테이블 레코드 삭제에 대한 Redo Logging
> 4. 인덱스 레코드 삭제
> 5. 인덱스 레코드 삭제에 대한 Undo Logging
> 6. 인덱스 레코드 삭제에 대한 Redo Logging
> 7. Undo(2, 5번)에 대한 Redo Logging

#### 파티션 Drop을 이용한 대량 데이터 삭제
* 테이블이 삭제 조건절 컬럼 기준으로 파티셔닝 되어 있고, 인덱스도 로컬 파티션이라면 아래와 같이 대량 데이터를 순식간에 삭제할 수 있다.
```oracle
alter table 거래 drop partition p202412;
```
* 오라클 11g부터 아래와 같이 대상 파티션을 지정할 수도 있다.
```oracle
alter table 거래 drop partition for('20241201');
```

#### 파티션 Truncate를 이용한 대량 데이터 삭제
* 거래일자 조건에 해당하는 데이터를 삭제하지 않고 또 다른 삭제 조건이 있는 경우가 있다.
```oracle
delete from 거래
where 거래일자 < '20250101'
  and (상태코드 <> 'ZZZ' or 상태코드 is null);
```
* 해당 조건을 만족하는 데이터가 대대수라면, 대량 데이터를 지울게 아닌 남길 데이터만 백업하고 재입력하는 방식이 빠르다.
1. 임시 테이블을 생성하고 남길 데이터만 복제한다.
```oracle
create table 거래_t
as
select *
from 거래
where 거래일자 < '20150101'
  and 상태코드 = 'ZZZ'
```
2. 삭제 대상 테이블 파티션을 Truncate 한다.
```oracle
alter table 거래 truncate partition p201412;
```
```oracle
alter table 거래 truncate partition for ('20141201');
```
3. 임시 테이블에 복제해 둔 데이터를 원본 테이블에 입력한다.
```oracle
insert into 거래
select * from 거래_t;
```
4. 임시 테이블을 Drop한다.
```oracle
drop table 거래_t;
```

* 서비스 중단 없이 파티션을 Drop 또는 Truncate 하려면 아래 조건을 모두 만족해야 한다.
  * 파티션 키와 커팅 기준 컬럼이 일치해야 함
    * ex) 파티션 키와 커팅 기준 컬럼이 모두 `신청일자`
  * 파티션 단위와 커팅 주기가 일치해야 함
    * ex) 월 단위 파티션을 월 주기로 커팅
  * 모든 인덱스가 로컬 파티션 인덱스이어야 함
    * ex) 파티션 키는 `신청일자`, PK는 `신청일자 + 신청순번`
    * PK 인덱스는 삭제기준 컬럼이 인덱스 구성 컬럼이어야 로컬 파티셔닝이 가능하다.

### 파티션을 활용한 대량 INSERT 튜닝
#### 비파티션 테이블일 때
* 비파티션 테이블에 손익분기점을 넘는 대량 데이터를 INSERT 하려면 인덱스를 Unusable 시켰다가 재생성하는 방식이 더 빠를 수 있다.
1. 테이블을 nologging 모드로 전환한다.
```oracle
alter table target_t nologging; 
```
2. 인덱스를 Unusable 상태로 전환한다.
```oracle
alter index target_t_x01 unusable; 
```
3. 가능하다면 Direct Path Insert 방식으로 대량 데이터를 입력한다.
```oracle
insert /*+ append */ into target_t
select * from source_t
```
4. nologging 모드로 인덱스를 재생성한다.
```oracle
alter index target_t_x01 rebuild nologging; 
```
5. logging 모드로 전환한다.
```oracle
alter table target_t logging;
alter table target_t_x01 logging;
```

#### 파티션 테이블일 때
* 초대용량 인덱스를 재생성하는 부담이 만만치 않기 때문에, 시간이 더 오래 걸리더라도 실무에서는 웬만하면 인덱스를 그대로 둔 상태로 INSERT 한다.
* 이 경우 테이블이 파티셔닝되어있고, 인덱스도 다행이 로컬 파티션이라면 고민이 해결된다.
  * 파티션 단위로 인덱스를 재생성할 수 있기 때문이다.

1. 작업 대상 테이블 파티션을 nologging 모드로 전환한다.
```oracle
alter table target_t modify partition p_201712 nologging;
```
2. 작업대상 테이블 파티션과 매칭되는 인덱스 파티션을 Unusable 상태로 전환한다.
```oracle
alter index target_t_x01 modify partition p_201712 unusable;
```
3. 가능하다면 Direct Path Insert 방식으로 대량 데이터를 입력한다.
```oracle
insert /*+ append */ into target_t
select * from source_t where dt between '20171201' and '20171231'
```
4. nologging 모드로 인덱스 파티션을 재생성한다.
```oracle
alter index target_t_x01 rebuild partition p_201712 nologging;
```
5. 작업 파티션을 logging 모드로 전환한다.
```oracle
alter table target_t modify partition p_201712 logging;
alter table target_t_x01 modify partition p_201712 logging;
```