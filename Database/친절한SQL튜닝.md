# SQL 처리과정과 I/O
## SQL 파싱과 최적화
### SQL?
* SQL은 Structured Query Language 의 줄임말이다.
* SQL은 깁노적으로 구조적(structured)이고, 집합적(set-based)이고, 선언적(declarative)인 질의 언어이다.
* 원하는 결과 집합을 구조적, 집합적으로 선언하지만 그 결과 집합을 만드는 과정은 절차적일 수 밖에 없다.
* 결국 프로시저가 필요한데, 그 프로시저를 만들어내는 DBMS 내부 엔진이 바로 SQL 옵티마이저이다.
* DBMS 내에서 프로시저를 작성하고 컴파일해 실행 가능한 상태로 만드는 모든 과정을 SQL 최적화라고 한다.

### SQL 최적화 과정
1) SQL 파싱
   * 사용자로부터 SQL을 전달받으면, 가장 먼저 SQL 파서가 파싱을 진행한다.
   * 파싱 트리 생성 : SQL문을 이루는 개별 구성 요소를 분석해 파싱 트리를 생성한다.
   * Syntax 체크 : 문법적 오류가 없는지 확인한다.
   * Semantic 체크 : 의미상 오류가 없는지 확인한다.
2) SQL 최적화
   * SQL최적화는 옵티마이저가 진행한다.
   * 옵티마이저가 미리 수집한 시스템 및 오브젝트 통계정보를 바탕으로 실행 경로를 생성해 비교한 후 가장 효울적인 하나를 선택한다.
3) 로우 소스 생성
   * SQL 옵티마이저가 선택한 실행 경로를 실행가능한 코드나 프로시저 형태로 포맷팅한다.
   * 로우 소스 생성기(Row-Source Generator)가 그 역할을 맡는다.

### SQL 옵티마이저
* SQL옵티마이저는 사용자가 원하는 작업을 가장 효율적으로 수행할 수 있는 최적의 데이터 액세스 경로를 선택해주는 DBMS의 핵심 엔진이다.
* SQL 옵티마이저가 실행계획을 생성할 때에는 아래와 같이 진행된다. 
  1) 사용자로부터 전달받은 쿼리를 수행하는 데 후보군이 될만한 실행계획들을 찾아낸다.
  2) 데이터 딕셔너리에 미리 수집해 둔 오브젝트 통계 및 시스템 통계정보를 이용해 각 실행계획의 예상 비용을 산정한다.
  3) 최저 비용을 나타내는 실행 계획을 선택한다.

### 실행계획과 비용
* DBMS에 SQL 실행경로 미리 보기 기능이 있는데, 실행 계획(Execution Plan)이다.
* 이를 통해 자신이 작성한 SQL이 테이블을 스캔하는지, 인덱스를 스캔하는지, 인덱스를 스캔한다면 어떤 인덱스인지를 확인할 수 있으며, 예상과 다른 방식으로 처리되면 실행경로를 변경할 수 있다.
* SQL 실행계획에 표시되는 Cost도 예상치이며, 실행경로를 선택하기 위해 옵티마이저가 여러 통계 정보를 활용해 계산해낸 값이다.

### 옵티마이저 힌트
* 옵티마이저가 보통 좋은 선택을 하지만, 그 선택이 최선은 아니다.
* 옵티마이저 힌트를 이용해 데이터 액세스 경로를 바꿀 수 있다.
* 중요한 시스템이라면, 옵티마이저의 판단에만 맡기긴 힘들어 힌트를 사용할 순 있지만, 힌트를 쓸거면 빈틈없이 써야 한다.

#### 주의사항
* `--+ INDEX(A TEST_PK)`방식은 가급적 쓰지 말아야 한다.
  * `/*+ INDEX(A TEST_PK)*/` 와 같은 방식이 안전하다.
* 인자를 나열할 땐 `,`를 사용할 수 있지만 힌트와 힌트 사이에 사용하면 안된다.
* 테이블을 지정할 때 스키마 명까지 명시하면 안된다.
* FROM절 옆에 ALIAS를 지정했으면 힌트에도 반드시 ALIAS를 사용해야 한다.

## SQL 공유 및 재사용
### 소프트 파싱 vs 하드 파싱
* SQL 파싱, 최적화, 로우소스 생성 과정을 거쳐 생성한 내부 프로시저를 반복해서 재사용할 수 있도록 캐싱해두는 메모리 공간을 **라이브러리 캐시**라고 한다.
   > SGA(System Global Area)
   >* 서버 프로세스와 백그라운드 프로세스가 공통으로 액세스하는 데이터와 제어 구조를 캐싱하는 메모리 공간
   >* 라이브러리 캐시는 SGA의 구성 요소이다.
* 사용자가 SQL문을 전달하면 DBMS는 SQL을 파싱한 후 해당 SQL이 라이브러리 캐시에 존재하는지부터 확인한다.

1) 소프트 파싱
   * SQL을 캐시에서 찾아 바로 실행단계로 넘어가는것
2) 하드 파싱
   * 찾는데 실패 해 최적화 및 로우 소스 생성 단계까지 모두 거치는것
   * 하드파싱은 CPU를 많이 소비하는 몇 안되는 작업 중 하나이다.
   * 쿼리를 수행할 때, 후보군이 될만한 많은 실행경로를 도출하고, 딕셔너리와 통계정보를 읽어 각각의 효율성을 판단하는 과정은 결코 가볍지 않다.

### 바인드 변수의 중요성
#### 이름이 없는 SQL 문제
* 사용자 정의 함수/프로시저, 트리거, 패키지 등은 생성할 때 부터 이름을 갖기 때문에, 컴파일한 상태로 딕셔너리에 영구적으로 보관된다. 
* SQL은 이름이 따로 없고, 전체 텍스트가 이름 역할을 한다.

#### 공유 가능 SQL
* 바인드 변수를 사용하지 않으면, DBMS에서 발생하는 부하는 대개 I/O가 원인인데, I/O가 거의 발생하지 않음에도 CPU사용률은 급격히 올라갈 수 있다. (SQL 하드파싱 문제)
* 따라서 바인드 변수를 사용해 파라미터 Driven 방식으로 SQL을 작성하면 이러한 하드파싱 문제를 방지할 수 있다.

## 데이터 저장 구조 및 I/O 메커니즘
* I/O 튜닝이 곧 SQL 튜닝이라 해도 과언이 아니다.
* 즉 SQL튜닝 원래를 제대로 이해하려면 I/O에 대한 이해가 필수적이다.

### SQL이 느린 이유
* 프로세스가 잠을 자는 이유는 여러가지가 있지만, I/O가 가장 대표적이고 많은 비중을 차지한다.
* interrupt 없이 일하는 프로세스도 디스크에서 데이터를 읽어야 할 땐 CPU를 OS에 반환하고 waiting 상태에서 I/O가 완료되기를 기다린다.
  * CPU를 반환한 채 대기 큐에서 잠을 자는 것이다.
* 열심히 일해야 할 프로세스가 잠을 자게 되므로, I/O가 많으면 성능이 느릴 수 밖에 없다.
* SQL이 느린 이유가 여기에 있으며, 즉 디스크 I/O 때문이다.

### 데이터베이스 저장 구조

1. 테이블스페이스(Tablespace)
   * 데이터를 저장하려면 먼저 테이블스페이스를 생성해야 한다.
   * 테이블스페이스는 세그먼트를 담는 논리적 컨테이너며, 내부적으로는 여러 개의 데이터 파일로 구성된다.
   * 하나의 테이블 스페이스 안에는 여러 테이블, 인덱스 등의 세그먼트가 함께 저장된다.

2. 세그먼트(Segment)
   * 세그먼트는 실제로 데이터를 저장하는 오브젝트 단위이다.
     * ex) 테이블 세그먼트, 인덱스 세그먼트, LOB 세그먼트, 파티션 세그먼트 등
   * 테이블/인덱스를 생성할 때 어느 테이블스페이스에 저장할 것인지 지정하면, 그 테이블/인덱스는 해당 테이블 스페이스 내의 자신만의 세그먼트를 갖게 된다.
   * 파티션 구조가 아닌 경우 `테이블 1개 = 세그먼트 1개`, `인덱스 1개 = 인덱스세그먼트 1개` 이다.
     * 파티션 구조인 경우 각 파티션이 하나의 세그먼트를 구성한다.
   * LOB 컬럼은 별도의 LOB 세그먼트를 사용해 저장된다.
     * LOB값은 자신이 속한 테이블 세그먼트와 다른 공간에 저장될 수 있다.
     * 설정에 따라 작은 LOB는 테이블 로우 안(in-row)에 저장되기도 한다.

3. 익스텐트(Extent)
   * 하나의 세그먼트는 여러 개의 익스텐트로 구성된다.
   * 익스텐트는 공간을 확장하는 단위이며, 세그먼트의 공간이 부족해지면 해당 세그먼트는 자신이 속한 테이블스페이스로부터 익스텐트를 추가 할당받는다.
   * 익스텐트는 연속된 데이터 블록들의 집합이다.

4. 데이터 블록(Data Block)
   * 실제 레코드가 저장되는 가장 작은 단위가 데이터블록이다.
   * 블록 크기는 DB설정에 따라 8KB, 16KB 등으로 정해진다. (오라클 기본 8KB)
   * 한 블록에는 하나의 세그먼트에 속한 데이터만 저장된다.
     * 마찬가지로 한 익스텐트 안의 블록들도 모두 같은 세그먼트에 속한다.

* 세그먼트 공간이 부족해지면 테이블 스페이스로부터 익스텐트를 추가로 할당받는다.
* 이 때, 세그먼트에 할당된 모든 익스텐트가 반드시 같은 데이터파일에 위치하는것은 아니다.
  * 서로 다른 데이터 파일에 위치할 가능성이 더 높다.
* 하나의 테이블스페이스를 여러 데이터파일로 구성하면, 파일간 I/O 경합을 줄이기 위해 DBMS가 가능한 여러 파일에 익스텐트를 분산해 저장하려고 한다.

### DBA(Data Block Address)
* 데이터 블록은 디스크 상에서 몇 번 데이터 파일의 몇 번째 블록인지를 나타내는 고유 주소값(DBA)을 갖는다.
* 데이터를 읽으려면 DBA부터 확인해야 한다.
* 인덱스를 이용해 테이블 레코드를 읽을 때에는 인덱스 ROWID를 이용한다.
  * ROWID 는 DBA + 로우 번호(블록 내 순번)으로 구성되므로, 이를 통해 읽어야 할 테이블 레코드가 저장된 DBA를 알 수 있다.
* 테이블 스캔 시에는 테이블 세그먼트 헤더에 저장된 익스텐트 맵을 이용한다.
  * 익스텐트 맵을 통해 각 익스텐트의 첫 번째 블록 DBA를 알 수 있다.
* 즉, 익스텐트는 연속된 블록의 집합이므로 테이블을 스캔할 때에는 익스텐트 맵에서 각 익스텐트의 시작 DBA를 얻고, 그 지점부터 연속된 블록들을 순차적으로 읽어 나가면 된다.
  * 오라클은 멀티 블록 I/O를 사용해 각 익스텐트 구간을 효율적으로 스캔한다.

### 블록 단위 I/O
* 데이터베이스에서 데이터를 읽고 쓰는 단위는 블록이다.
* 데이터 I/O 단위가 블록이므로, 특정 레코드를 하나 읽고 싶어도 해당 블록을 통째로 읽는다.
* 오라클은 기본적으로 8KB 크기의 블록을 사용한다. (1Byte를 읽기 위해 8KB를 읽는다)

### 시퀀셜 액세스 vs 랜덤 액세스
#### 시퀀셜 액세스
* 시퀀셜 액세스는 **논리적또는 물리적으로 연결된 순서에 따라 차례대로 블록을 읽는 방식**이다.
* 인덱스 리프 블록은 앞뒤를 가리키는 주소값을 통해 논리적으로 연결되어 있다.
  * 이 주소값에 따라 앞 또는 뒤로 순차적으로 스캔하는 방식이 시퀀셜 액세스이다.
* 테이블 블록 간에는 논리적인 연결고리를 갖고있지 않다.
  * 세그먼트에 할당된 익스텐트 목록을 세그먼트 헤더에서 익스텐트 맵으로 관리한다.
  * 이 익스텐트 맵은 시작 DBA 값을 갖는다.
  * 따라서 읽어야 할 익스텐트 목록을 익스텐트 맵에서 얻고, 각 익스텐트의 첫 번째 블록 뒤에 연속해서 저장된 블록을 순서대로 읽으면, 바로 Full Table Scan이다.

#### 랜덤 액세스
* 랜덤 액세스는 논리적, 물리적 순서를 따르지 않고 **레코드 하나를 읽기 위해 해당 블록만 찍어서 접근하는 방식**이다.
* 보통 인덱스에서 ROWID를 얻은 뒤 각 ROWID가 가리키는 테이블 블록을 하나씩 따로 읽는 Table Access by ROWID가 랜덤 액세스에 해당한다.
  * 인덱스 리프블록 자체는 순차적으로 읽을 수 있지만, ROWID들이 서로 먼 블록을 가리키면 테이블 쪽에서는 랜덤 액세스가 발생한다.

### 논리적 I/O vs 물리적 I/O
#### DB 버퍼캐시
* SGA 구성 요소 중 하나가 **DB 버퍼 캐시**이고, 이 외에 **라이브러리 캐시** 등이 있다.
* **라이브러리 캐시**가 SQL과 실행계획, DB 저장형 함수/프로시저 등을 캐싱하는 **코드캐시**라고 한다면, **DB버퍼캐시는 데이터 캐시**라고 할 수 있다.
* 디스크에서 읽은 데이터 블록을 캐싱해 같은 블록에 대한 반복적인 I/O Call을 줄이는데 목적이 있다.
* 서버 프로세스와 데이터 파일 사이에는 항상 버퍼 캐시가 있으므로, 데이터 블록을 읽을 때에는 **먼저 버퍼캐시를 탐색**한다.
  * 버퍼캐시는 SGA의 공유메모리 영역이므로, 한 세션이 올려둔 블록을 다른 세션도 재사용할 수 있다.

#### 논리적 I/O vs 물리적 I/O
1. 논리적 I/O
   * SQL을 처리하는 과정에서 서버 프로세스가 버퍼 캐시에 대한 요청한 블록 접근 횟수를 말한다.
   * 즉, 이 블록을 읽어야 한다라고 판단해 버퍼 캐시를 찾아본 횟수이다.
2. 물리적 I/O
   * 버퍼 캐시에서 블록을 찾지 못해 실제로 디스크에서 읽어 온 블록 I/O를 말한다.
   * 논리적 I/O중에서 캐시에 없어서 디스크까지까지 간 일부가 물리적 I/O가 된다. 

* 메모리 I/O는 전기적신호이지만, 디스크 I/O는 액세스 암을 통해 물리적 작용이 일어나므로 메모리 I/O에 비해 상당히 느리다. (보통 10000배 느림)
* Direct Path Read방식으로 읽는 경우를 제외하면, 모든 블록은 DB 버퍼캐시를 경유해서 읽는다.
* 즉, 일반적인 버퍼 캐시 방식의 액세스에서는 `논리적 I/O횟수 = 버퍼 캐시에 블록을 읽으려고 시도한 횟수`, `물리적 I/O 횟수 = 그 중 캐시에 없어 디스크에서 실제로 읽은 횟수` 이다.


#### 버퍼캐시 히트율
* 버퍼캐시 효율을 측정하는데 가장 많이 사용해 온 지표는 버퍼캐시 히트율(BCHR)이다.
 ```
BCHR = (캐시에서 찾은 블록 수 / 총 읽은 블록 수 ) * 100
     = ((논리적 I/O - 물리적 I/O) / 논리적 I/O) * 100
     = (1 - (물리적 I/O) / (논리적 I/O)) * 100
 ```
* 애플리케이션 특성에 따라 다르지만, 온라인 트랜잭션(OLTP)을 주로 처리하는 애플리케이션이라면 시스템 레벨에서 평균 99% 히트율을 달성해야 한다.
  * 핵심 트랜잭션이 시스템 전체 부하의 대부분을 차지하므로, 열심히 튜닝하면 99%는 달성하기 어려운 수치가 아니다.
* 논리적 I/O(읽어야 할 블록 수)는 일정하므로, 물리적 I/O(디스크에서 읽을 수)는 버퍼캐시 히트율에 의해 결정된다.
* 결국 SQL 성능을 높이기 위해 할 수 있는 핵심 작업은 논리적 I/O를 줄이는 일이다.
  * SQL을 튜닝해서 읽는 총 블록 수를 줄이면 된다.
  * 논리적 I/O를 줄임으로써 물리적 I/O를 줄이는 것이 SQL 튜닝이다.
* 버퍼캐시 히트율이 SQL 성능을 좌우하지만, 높다고해서 효율적인 SQL을 의미하진 않는다.
  * 같은 블록을 반복해서 읽는 비효율적인 일이었을 수 있다.

### Single Block I/O vs Multiblock I/O
* **한 번에 한 블록씩 요청해서 메모리에 적재하는 방식을 Single Block I/O**라고 한다.
* **한 번에 여러 블록을 요청해서 메모리에 적재하는 방식을 Multiblock I/O**라고 한다.
* 인덱스를 이용할 때에는 기본적으로 인덱스와 테이블 블록 모두 Single Block I/O 방식을 사용한다.
  * 인덱스 루트 블록을 읽을 때
  * 인덱스 루트 블록에서 얻은 주소 정보로 브랜치 블록을 읽을 때
  * 인덱스 브랜치 블록에서 얻은 주소 정보로 리프 블록을 읽을 때
  * 인덱스 리프 블록에서 얻은 주소 정보로 테이블 블록을 읽을 때 (랜덤 액세스 + Single Block I/O)
* 많은 데이터 블록을 읽을 때는 Multiblock I/O 방식이 효율적이다.
  * 대표적으로 Full Table Scan / Full Index Scan에서 사용된다.
  * 특정구간을 한 번의 I/O 호출로 여러 블록씩 요청하는 방식
  * Multiblock I/O는 디스크 상에서 연속된 블록 구간을 한 번에 읽어 캐시에 미리 올려두는 프리페치(prefetch) 역할을 한다. 
  * DBMS의 블록사이즈가 얼마건 간에 OS단에서는 일반적으로 1MB 단위로 I/O를 수행한다. (OS마다 다르다)
    * 오라클 I/O단위가 8KB이므로, db_file_multiblock_read_count를 128로 설정하면 최대가 된다. (8KB * 128)

### Table Full Scan vs Index Range Scan
1. Table Full Scan
   * 테이블에 속한 블록 전체를 읽어 사용자가 원하는 데이터를 찾는 방식이다.
   * **시퀀셜 액세스 + Multiblock I/O** 방식으로 디스크 블록을 읽는다.
   * 순차 대량 스캔이기 때문에 스토리지의 스캔 성능이 좋아질수록 성능이 개선된다.
2. Index Range Scan
   * 인덱스에서 필요한 범위를 스캔하고, 거기서 얻은 ROWID로 테이블 레코드를 찾아가는 방식이다.
   * 인덱스는 범위로 읽지만, 테이블은 ROWID 기반 **랜덤 액세스 + Single Block I/O**방식으로 디스크 블록을 읽는다.
* Table Full Scan을 피해야 한다는 인식과 달리 인덱스가 SQL성능을 떨어뜨리는 경우도 많다.
  * 한번에 많은 데이터를 처리하는 집계용 SQL과 배치프로그램이 그렇고, 이 경우엔 오히려 Full Scan으로 유도하면 성능이 좋아진다.
  * 물론 큰 테이블에서 소량의 데이터를 검색할 때에는 반드시 인덱스를 이용해야 한다.
  * 인덱스는 굉장히 중요하긴 하지만, 항상 옳은 것은 아니므로 모든 성능문제를 인덱스로 해결하려고 하면 안된다.

### 캐시 탐색 메커니즘
* Direct Path I/O를 제외한 모든 블록 I/O는 메모리 버퍼캐시를 경유한다.
  * 인덱스 루트 블록을 읽을 때
  * 인덱스 루트 블록에서 얻은 주소 정보로 브랜치 블록을 읽을 때
  * 인덱스 브랜치 블록에서 얻은 주소 정보로 리프 블록을 읽을 때
  * 인덱스 리프 블록에서 얻은 주소 정보로 테이블 블록을 읽을 때
  * 테이블 블록을 Full Scan 할 때
* 버퍼 캐시에서 블록을 찾을 때 해시 알고리즘으로 버퍼 헤더를 찾고, 거기서 얻은 포인터로 버퍼 블록을 액세스하는 방식을 사용한다.
  * 같은 입력값은 항상 동일한 해시 체인에 연결됨
  * 다른 입력값이 동일한 해시 체인에 연결될 수 있음
  * 해시 체인 내에서는 정렬이 보장되지 않음
* **버퍼캐시에 캐싱된 버퍼블록, 해시 체인, LRU 리스트 등은 모두 공유자원**이기 때문에, 동시에 두 개 이상 프로세스가 접근하려고 할 때 정합성에 문제가 생길 수 있다.
* 따라서 한 프로세스씩 순차적으로 접근하도록 구현해야 하며, 이를 위한 직렬화 메커니즘이 필요한데, 이를 지원하는 메커니즘이 latch이다.

> 캐시버퍼 체인 래치(Cache Buffer Chain Latch)
> * 각 해시 체인마다 대응되는 캐시 버퍼 체인 래치(CBC latch)가 있다.
> * 블록 I/O가 발생하면, DBA를 해시 함수에 입력하고 반한된 값으로 스캔해야 할 해시 체인을 찾는다.
> * 해당 해시 체인에 접근하려면 그 체인의 CBC latch를 획득해야 한다.
> * CBC latch를 가진 세션만 해당 해시 체인을 따라가며 버퍼 헤더를 탐색할 수 있다.
> * 대량의 데이터를 읽을 때 모든 블록에 대해 해시 체인을 스캔하면서 CBC latch 경합이 심해질 수 있다.

> 버퍼 Lock
> * latch는 해시 체인, LRU와 같은 공유 메타데이터 구조를 보호한다.
> * 버퍼 Lock은 특정 버퍼 블록 자체에 대한 동시 접근을 조절한다.
> * 캐시버퍼 체인 래치를 해제하기 전 버퍼 헤더에 Lock을 설정함으로써 버퍼블록 자체에 대한 직렬화 문제를 해결한다.

> LRU 체인 래치 등 기타 래치
> * 버퍼캐시의 LRU 리스트 등도 래치로 보호된다.
> * 버퍼 교체(언핀, 플러시 등) 시 LRU 체인 래치 획득이 필요하고, 이 구간에서도 경합이 발생할 수 있다.
> * 캐시 I/O여도 래치 경합이 심하면 체감 성능이 크게 떨어질 수 있다.

* 캐시 I/O가 생각보다 빠르지 않은 이유는 latch/버퍼 Lock 경합 때문이다.
* 따라서 이러한 직렬화 메커니즘에 의한 캐시 경합을 줄이려면 SQL튜닝을 통해 논리적 I/O 자체를 줄여야 한다.

# 인덱스 기본
## 인덱스 구조 및 탐색
### 미리보는 인덱스 튜닝
#### 데이터를 찾는 두 가지 방법
* 데이터베이스 테이블에서 데이터를 찾는 방법은 두가지 이다.
  * 테이블 전체를 스캔한다.
  * 인덱스를 이용한다.

#### 인덱스 튜닝의 두 가지 핵심 요소
* 인덱스는 큰 테이블에서 소량 데이터를 검색할 때 사용한다.
* 온라인 트랜잭션 처리 시스템에서는 소량 데이터를 주로 검색하므로, 인덱스 튜닝이 무엇보다 중요하다.
* 인덱스 튜닝 방법은 여러가지가 있지만, 크게 두 가지로 나뉜다.
  * 인덱스 스캔 효울화 튜닝 : 인덱스 스캔 과정에서 발생하는 비효율을 줄이는 것
  * 랜덤 액세스 최소화 튜닝 : 테이블 액세스 횟수를 줄이는 것 (인덱스 스캔 후 레코드를 액세스할 때 랜덤 I/O 방식 사용)
* 인덱스 스캔 효율화 튜닝과 랜덤 액세스 최소화 튜닝 모두 중요하지만, 랜덤 액세스 최소화 튜닝이 성능에 미치는 영향이 더 크다.
* 즉, SQL 튜닝에서 가장 중요한 것은 랜덤 I/O를 줄이는 것이다.

#### SQL 튜닝은 랜덤 I/O와의 전쟁
* 성능을 위해 DBMS가 제공하는 많은 기능은 느린 랜덤 I/O를 극복하기 위해 개발되었다.
  * IOT, 클러스터, 파티션, 테이블 Prefetch, Batch I/O 등
* 조인 메서드 중 가장 일방적으로 사용하는 NL조인이 대량 데이터 조인할 때 느린 이유도 랜덤 I/O 때문이다.
  * 그래서 소트머지 조인과 해시 조인이 개발되었다.

### 인덱스 구조
* 인덱스는 대용량 테이블에서 필요한 데이터만 빠르게 효율적으로 액세스하기 위해 사용하는 오브젝트이다.
* 데이터베이스에서 인덱스 없이 데이터를 검색하려면 테이블을 처음부터 끝까지 읽어야 하지만, 인덱스를 이용해 일부만 읽고 멈출 수 있다.
  * 즉 범위스캔이 가능하며, 범위스캔이 가능한 이유는 인덱스가 정렬되어있기 때문이다.
* DBMS는 일반적으로 B-Tree 인덱스를 사용한다.
  * 루트와 브랜치 블록에는 키 값을 갖지 않는 특별한 레코드가 하나 있는데, 첫 번째 레코드이며, LMC(Leftmost Child)라고 한다.
  * LMC는 자식 노드 중 가장 왼쪽 끝에 위치한 블록을 가리킨다.
  * LMC가 기리키는 주소로 찾아간 블록에는, 키값을 가진 첫 번째 레코드보다 작거나 같은 레코드가 저장되어있다.
  * 리프블록에 저장된 레코드는 키값 순으로 정렬되어있을 뿐만 아니라, 테이블 레코드를 가리키는 주소값인 ROWID를 갖는다.
* 인덱스 키값이 같으면 ROW_ID 순으로 정렬된다.
* 인덱스를 스캔하는 이유는 검색조건에 맞는 소량의 데이터를 빨리 찾아 ROWID를 얻기 위함이다.
  * ROWID = 데이터 블록 주소 + 로우번호
  * 데이터 블록 주소 = 데이터 파일 번호 + 블록 번호
  * 블록 번호 : 데이터파일 내에서 부여한 상대적 순번
  * 로우 번호 : 블록 내 순번
* 인덱스 탐색 과적은 수직적 탐색과 수평적 탐색으로 나눌 수 있다.
  * 수직적 탐색 : 인덱스 스캔 시작지점을 찾는 과정
  * 수평적 탐색 : 데이터를 찾는 과정

### 인덱스 수직적 탐색 
* 정렬된 인덱스 레코드 중, 조건을 만족하는 첫 번째 레코드를 찾는 과정으로, 인덱스 스캔 시작 지점을 찾는 과정이다.
* 인덱스 수직적 탐색은 루트블록에서부터 시작한다.
* 루트를 포함해 브랜치 블록에 저장된 각 인덱스 레코드는 하위 블록에 대한 주소값을 갖는다.
  * 루트에서 시작해 리프블록까지 수직적으로 탐색이 가능한 이유다.
* 찾고자 하는 값보다 크거나 같은 값을 만나면 직전 레코드가 가리키는 하위 블록으로 이동한다.
* 수직적 탐색은 레코드를 찾는 과정이 아니라, **조건을 만족하는 첫 번째 레코드를 찾는 과정**이다.

### 인덱스 수평적 탐색
* 수직적 탐색을 통해 스캔 시작점을 찾은 후, 찾고자 하는 데이터가 더 안나타날 때까지 인덱스 리프 블록을 수평적으로 스캔한다.
* 즉, 인덱스에서 본격적으로 데이터를 찾는 과정이다.
* 인덱스 리프 블록끼리는 서로 앞뒤 블록에 대한 주소값을 갖고 있는 양방향 연결 리스트 구조이다.
  * 좌에서 우, 우에서 좌로 수평적 탐색이 가능하다.
* 인덱스를 수평적 탐색하는 이유는 **조건절을 만족하는 데이터를 모두 찾고, ROWID를 얻기 위함**이다.
  * 일반적으로는 인덱스를 스캔하고 테이블도 액세스 하는데, 이 때 ROWID가 필요하다.

### 결합 인덱스 구조와 탐색
* 두 개 이상 컬럼을 결합해 인덱스를 만들 수 있다.
* 인덱스를 `[고객명 + 성별]`로 하던, `[성별 + 고객명]`으로 하던 읽는 인덱스 블록 개수는 똑같다.
* 인덱스 선두 컬럼을 모두 `=` 조건으로 검색할 때에는 어느 컬럼을 앞쪽에 두든 블록 I/O 개수가 같으므로 성능도 같다.
  * 선택도가 낮은 컬럼을 앞쪽에 둬야 성능에 유리하다는 말은 잘못되었다.
  * `=`조건이 아니라 다른 조건들이 섞여있으면 컬럼 순서의 영향이 있다.

> Balanced
> * B-Tree에서의 B는 Balanced의 약자로, 어떤 값으로 탐색하더라도 인덱스 루트에서 리프까지 도달하기 위해 읽는 블록 수는 같다.

## 인덱스 기본 사용법
### 인덱스를 사용한다는 것
* 인덱스 컬럼을 가공하지 않아야 인덱스를 정상적으로 사용할 수 있다.
* **인덱스를 정상적으로 사용한다는 표현은** 리프블록에서 스캔 시작점을 찾아 거기서부터 스캔하다가 멈추는걸 의미하며, 리프 블록 일부만 스캔하는 **Index Range Scan을 의미**한다.
* 인덱스 컬럼을 가공해도 인덱스를 사용할 순 있지만, 스캔 시작점을 찾을 수 없고 멈출 수 없어 리프블록 전체를 스캔해야 하므로, Index Full Scan방식으로 동작하게 된다.

### 인덱스를 Range Scan 할 수 없는 이유
* 인덱스 컬럼을 가공했을 때 인덱스를 정상적으로 사용할 수 없는 이유는, 인덱스 스캔 시작점을 찾을 수 없기 때문이다.
  * 일정 범위를 스캔하려면 시작 지점과 끝 지점을 알 수 있어야 한다.
* 인덱스에는 가공되지 않은 생년월일 값이 저장되어 있는데, 가공된 값을 기준으로 검색을 하려면 스캔 시작점과 끝지점을 찾을 수 없다.
  * `where substr(생년월일, 5, 2) = '05'`
* 값이 NULL이면 0으로 치환한 값 기준으로 100보다 작은 레코드를 찾아달라고 하면 인덱스 스캔 시작지점을 알 수가 없다.
  * `where nvl(주문수량, 0) < 100`
* 대한으로 시작하는 값은 특정 구간에 몰려있겠지만, 대한을 포함하는 값은 전체 구간에 흩어져 있어 Range Scan이 불가능하다.
  * `where 업체명 like '%대한%'`
* `전화번호`가 `'01012341234'` 이거나 `고객명`이 `'홍길동'`인 한 시작점을 찾을 수 없기 때문에 Range Scan이 불가능하다. 
  * `where (전화번호 = :tel_no OR 고객명 = :cust_nm'`

> OR Expansion
> * 아래 쿼리와 같이 변경하면 고객명, 전화번호 인덱스에 대해 Index Range Scan이 가능하다.
> * OR조건식을 SQL옵티마이저가 위와 같은 형태로 변환할 수 있는데, 이것을 OR Expansion이라고 한다.
> * ```
>   select *
>   from 고객
>   where 고객명 = :cust_nm
>   
>   union all
>   
>   select *
>   from 고객
>   where 전화번호 = :tel_no
>   and (고객명 <> :cust_nm or 고객명 is null)
>   ```
> * `use_concat` 힌트를 이용해 OR Expansion을 유도할 수 있다.

> IN-List Iterator
> * IN 조건 또한 한 지점을 찾을 수 없다.
>   * `where 전화번호 in (:tel_no1, :tel_no2)`
> * IN 조건 또한 UNION ALL 방식으로 작성하면 인덱스 스캔 시작점을 찾을 수 있다.
> * 그래서 IN 조건절에 대해서는 SQL 옵티마이저가 IN-List Iterator 방식을 사용한다.
>   * List 개수만큼 Index Range Scan 반복

### 더 중요한 인덱스 사용 조건
* 인덱스를 정상적으로 사용하는 데 있어 중요한 선행조건이 있다.
* **Index Range Scan을 하기 위해선 인덱스 선두 컬럼이 조건절에 있어야 하며, 또한 가공하지 않은 상태여야 한다.**
* 이 조건을 만족하지 않으면, 실행계획에는 인덱스를 탄다고 나오더라도 스캔 범위를 줄이는데 전혀 기여하지 못하는 비효율적인 인덱스 사용이 될 수 있다.

### 인덱스를 이용한 소트 연산 생략
* 테이블과 달리 인덱스는 정렬된 구조를 가진다.
* PK 인덱스를 스캔하면서 출력한 결과 집합은 어차피 PK 순으로 **정렬되어 있기 때문에 ORDER BY가 있어도 별도의 Sort 연산을 따로 수행하지 않는다.**
* 또한 DESC 정렬에도 인덱스를 활용할 수 있다.
  * 인덱스 리프 블록은 양방향 연결 리스트 구조이기 때문에 ASC, DESC 모두 스캔이 가능하다.
  * 따라서 ORDER BY 절에서 DESC정렬을 요구해도, ORDER BY 연산을 하지 않고, `INDEX (RAGE SCAN DESCENDING)`과 같이 인덱스를 사용한다.

### 조건절이 아닌곳에서의 컬럼 가공
* 인덱스 컬럼을 가공하면 인덱스를 정상적으로 사용할 수 없다는 말은 대개 조건절에 사용한 컬럼을 의미한다.
* 그런데 조건절이 아닌 ORDER BY 또는 SELECT-LIST에서 컬럼을 가공함으로 인해 인덱스를 정상적으로 사용할 수 없는 경우도 있다.

#### ORDER BY 에서 컬럼 가공
* 아래 예시는 ORDER BY 연산을 하게 된다. 
  ```oracle
  -- 인덱스 : 주문일자 + 주문번호
  SELECT *
  FROM (
      SELECT TO_CHAR(A.주문번호, 'FM000000') AS 주문번호, 
             A.업체번호, 
             A.주문금액
      FROM 주문 A
      WHERE A.주문일자 = :dt
          AND A.주문번호 > NVL(:next_ord_no, 0)
      ORDER BY 주문번호   -- TO_CHAR의 FM000000옵션이 사용된 Alias 주문번호이기 때문, A.주문번호로 해야 함
      )
  WHERE ROWNUM <= 30
  ```

### SELECT-LIST 에서 컬럼 가공
* 인덱스가 `[장비번호 + 변경일자 + 변경순번]` 순으로 구성되어 있으면서 정렬연산을 생략하는 예시
  ```oracle
  SELECT MIN(변경순번)
  FROM 상태태변경이력
  WHERE 장비번호 = 'C'
    AND 변경일자 = '20180316'
  ```
  ```oracle
  SELECT MAX(변경순번)
  FROM 상태태변경이력
  WHERE 장비번호 = 'C'
    AND 변경일자 = '20180316'
  ```
* 정렬연산을 생략할 수 없는 예시
  ```oracle
  SELECT NVL(MAX(TO_NUMBER(변경순번)), 0)
  FROM 상태변경이력
  WHERE 장비번호 = 'C'
    AND 변경일자 = '20180316'
  ```
  * 인덱스에는 문자열 기준으로 정렬되어 있는데, 이를 숫자값으로 바꾼 값 기준으로 최종 변경순번을 요구했기 때문이다.
  * 아래와 같이 바꿔야 정렬연산을 생략할 수 있다.
    ```oracle
    SELECT NVL(TO_NUMBER(MAX(변경순번)), 0)
    FROM 상태변경이력
    WHERE 장비번호 = 'C'
      AND 변경일자 = '20180316'
    ```

### 자동 형변환
* 생년월일이 선두 컬럼인 인덱스가 있을 때, 아래 SQL은 생년월일 컬럼을 가공하지 않았는데도 테이블 전체 스캔을 선택한다.
  ```oracle
  SELECT * FROM 고객
  WHERE 생년월일 = 19821225
  ```
  * 옵티마이저가 SQL을 아래와 같이 변환했고 결국 인덱스 컬럼이 가공되었기 때문이다.
    ```oracle
    SELECT * FROM 고객
    WHERE TO_NUMBER(생년월일) = 19821225
    ```
* 각 조건절에 양쪽 값의 데이터 타입이 다르면, 타입체크를 엄격하게 하여 컴파일 시점에 에러를 내는 DBMS가 있고, 자동으로 형변환 처리해주는 DBMS가 있다.
  * 오라클은 자동으로 형변환처리 해준다.
    * 오라클에서는 숫자형과 문자형이 만나면 숫자형이 우선되므로, 숫자형 기준이 되고 문자형 컬럼/값이 숫자형으로 변환한다.
    * 연산자가 LIKE일때에는 문자열 비교 연산자이므로, 문자형 기준이 되고 숫자형 컬럼/값이 문자형으로 변환된다.
    * 날짜형과 문자형('01-JAN-2018')이 만나면 날짜형이 우선되므로, 날짜형이 기준이 되고 문자형을 날짜형으로 변환한다.
* 따라서 아래와 같이 타입을 정확히 지정해주는 습관이 좋다.
  ```oracle
  SELECT * FROM 고객
  WHERE 가입일자 = TO_DATE('01-JAN-2018, DD_MON_YYYY')
  ```
  
#### 자동 형변환의 위험성
* LIKE 조건을 옵션 처리 목적으로 사용하는 경우가 종종 있다.
* 아래 예시에서 계좌번호가 숫자형이면 인덱스를 사용할 수 없다.
  ```oracle
  SELECT * FROM 거래
  WHERE 계좌번호 LIKE :acnt_no || '%' -- 계좌번호를 문자형으로 변환하기 때문
  AND 거래일자 between :trd_dt1 and :trd_dt2
  ```
* 위와 같이 인덱스를 탈 수 있는 조건을 인덱스 컬럼 쪽 가공으로 바꿔버려서 Index Range Scan을 못 타는 상황이 생길 수 있다.
* 또한 자동 형변환으로 인해 쿼리의 결과가 의도와는 다르게 나오는 경우도 있다.
  * ex) 문자형 컬럼에 숫자가 아닌 값이 섞여 있는데 숫자 리터럴과 비교 (ORA-01722: invalid number)
* 따라서 자동 형변환의 기능에 의존하면 안된다.
* **인덱스 컬럼 기준으로 반대 편 컬럼 또는 값들을 정확하게 형변환 해줘야 한다.**

## 인덱스 확장기능 사용법
### Index Range Scan
* Index Range Scan은 B-Tree 인덱스의 가장 일반적이고 정상적인 형태의 액세스 방식이다.
* 인덱스 루트에서 리프블록까지 **수직적으로 탐색** 후 필요한 범위만 **수평적으로 스캔**한다.

### Index Full Scan
* Index Full Scan은 수직적 탐색 없이 인덱스 **리프 블록을 처음부터 끝까지 수평적으로 탐색**하는 방식이다.
* 보통 데이터 검색을 위한 최적의 인덱스가 없을 경우 차선으로 선택된다.

#### Index Full Scan의 효용성
* 인덱스 선두컬럼이 조건절에 없으면 옵티마이저는 Table Full Scan을 고려한다.
* 하지만 대용량 테이블이어서 Table Full Scan에 따른 부담이 큰 경우, 옵티마이저는 인덱스 활용을 다시 고려할 수 있다.
* 데이터 저장공간은 `컬럼길이 * 레코드 수`에 의해 결정되므로 인덱스가 차지하는 면적은 테이블보다 훨씬 적다.
* 인덱스 스캔 단계에서는 대부분 레코드를 필터링하고 아주 일부만 테이블을 액세스 하는 상황이라면 면적이 큰 테이블보다 인덱스를 스캔하는쪽이 유리하다.
* 옵티마이저가 이럴경우엔 Index Full Scan 방식을 선택한다.

#### 인덱스를 이용한 소트 연산 생략
* 인덱스를 Full Scan하면 결과 집합이 인덱스 컬럼 순으로 정렬된다.
* 따라서 옵티마이저가 전략적으로 선택한 경우, Sort Order By 연산을 생략할 목적으로 사용할 수도 있다.
* 하지만 대부분이 조건을 만족하는 상황에서 Index Full Scan을 선택하면 거의 모든 레코드에 대해 테이블 액세스(랜덤 액세스)가 발생하므로, Table Full Scan보다 오히려 불리하다.

### Index Unique Scan
* Index Unique Scan은 **수직적 탐색만으로** 데이터를 찾는 스캔 방식으로, Unique 인덱스를 `=` 조건으로 탐색하는 경우 작동한다.
* Unique Index가 존재하는 컬럼은 중복 값이 입력되지 않게 DBMS가 데이터 정합성을 관리해준다.
* 그러므로 해당 인덱스 키 컬럼을 모두 `=` 조건으로 검색할 때는 데이터를 한 건 찾는 순간 더이상 탐색할 필요가 없다.
  * 한 건만 딱 찾고 끝낼 수 있는 가장 효율적인 인덱스 스캔 방식

### Index Skip Scan
* 인덱스 선두 컬럼을 조건절에 사용하지 않으면 옵티마이저는 기본적으로 Table Full Scan을 선택한다.
* 하지만 Table Full Scan 보다 I/O를 줄일 수 있거나, 정렬된 결과를 더 쉽게 얻을 수 있다면 Index Full Scan을 사용하기도 한다.
* 오라클은 9i버전에서 인덱스 선두컬럼이 조건절에 없어도 인덱스를 활용하는 새로운 스캔 방식인 Index Skip Scan을 도입했다.
* Index Skip Scan은 조건절에 빠진 인덱스 선두 컬럼의 Distinct Value 개수가 적고 후행 컬럼의 Distinct Value 개수가 많을 때 유용하다.
* `index_ss`, `no_index_ss` 힌트를 사용해 이 스캔 방식을 유도하거나 방지할 수 있다.

#### Index Skip Scan이 작동하기 위한 조건
* Index Skip Scan은 아래와 같은 상황에서 효과적이다.
  * 선두 컬럼의 Distinct Value 개수가 적으며, 후행 컬럼의 Distinct Value 개수가 많다.
  * 선두 컬럼에 조건이 없거나, 중간컬럼에 조건이 빠져있다.
* 아래는 선두컬럼에 대한 조건절은 있고, 중간컬럼에 대한 조건절이 없는 경우의 예시이다.
  ```oracle
  -- 일별업종별거래_PK : 업종유형코드 + 업종코드 + 기준일자
  SELECT /*+ INDEX_SS(A 일별업종별거래_PK) */
        기준일자, 업종코드, 체결건수, 체결수량, 거래대금
  FROM 일별업종별거래 A
  WHERE 업종유형코드 = '01'
      AND 기준일자 BETWEEN '20080501' AND '20080531'
  ```
  * Index Range Scan을 사용하면, 업종코드유형이 `'01'`인 인덱스 구간을 모두 스캔해야 한다.
  * Index Skip Scan을 사용하면, 업종코드유형이 `'01'`인 구간에서 기준일자 조건의 레코드를 포함할 가능성이 있는 리프 블록만 골라서 액세스할 수 있다.
* 아래는 선두컬럼이 부등호, BETWEEN, LIKE같은 범위 검색 조건일 경우의 예시이다.
  ```oracle
  -- 일별업종별거래_X01 : 기준일자 + 업종유형코드
  SELECT /*+ INDEX_SS(A 일별업종별거래_X01) */
        기준일자, 업종코드, 체결건수, 체결수량, 거래대금
  FROM 일별업종별거래 A
  WHERE 기준일자 BETWEEN '20080501' AND '20080531'
    AND 업종유형코드 = '01'
  ```
  * Index Range Scan을 사용하면, 기준일자 BETWEEN 조건을 만족하는 인덱스 구간을 모두 스캔해야 한다.
  * Index Skip Scan을 사용하면, 기준일자 BETWEEN 조건을 만족하는 인덱스 구간에서 `업종유형코드 = '01'`인 레코드를 포함할 가능성이 있는 리프블록만 골라서 액세스할 수 있다.
* 위 예시들과 같이, Index Range Scan이 불가능하거나 효율적이지 못한 상황에서 Index Skip Scan이 종종 빛을 발할 수 있다.
* **인덱스는 기본적으로 최적의 Index Range Scan을 목표로 설계해야 하며, 수행횟수가 적은 SQL을 위해 인덱스를 추가하는것이 비효율적일 때 이러한 스캔 방식을 차선책으로 활용하는 전략이 바람직하다.**

### Index Fast Full Scan
* Index Fast Full Scan은 일반적으로 **Index Full Scan보다 빠르다.**
  * 논리적인 인덱스 트리구조(루트 > 브랜치 > 리프)를 무시하고 인덱스 세그먼트 전체를 Multiblock I/O 방식으로 스캔하기 때문이다.
* `index_ffs`, `no_index_ffs` 힌트로 이 스캔방식을 유도하거나 방지할 수 있다.
* Index Fast Full Scan은 루트와 브랜치블록은 읽지만 필요없으므로 무시한다.
* Index Fast Full Scan은 Multiblock I/O 방식을 사용하므로, 디스크로부터 **대량의 인덱스 블록을 읽어야 할 때 큰 효과를 발휘**한다.
* 연결리스트 구조를 무시한 채 데이터를 읽기 때문에, **결과 집합이 인덱스 키 순서대로 정렬되지 않는다.**
* 쿼리에 사용한 컬럼이 모두 인덱스에 포함되어 있을 때만 사용할 수 있다.
* Index Range Scan 또는 Index Full Scan과 달리 **인덱스가 파티션 되어있지 않더라도 병렬쿼리가 가능**하다.
  * 병렬 쿼리 시에는 Direct Path I/O 방식을 사용하기 때문에 I/O 속도가 더 빨라진다.

### Index Range Scan Descending
* Index Range Scan과 기본적으로 동일한 스캔 방식으로, 인덱스를 뒤에서 앞쪽으로 스캔하기 때문에 내림차순으로 정렬된 결과집합을 얻는다.
* `index_desc` 힌트를 이용해 유도할 수 있다.
* 예시로, MAX 값을 구하고자 할 때에 해당 컬럼에 인덱스가 있으면 인덱스를 뒤에서부터 한 건만 읽고 멈추는 실행계획이 자동으로 수립된다.

# 인덱스 튜닝
## 테이블 액세스 최소화
### 테이블 랜덤 액세스
#### 인덱스 ROWID는 물리적? 논리적?
* 인덱스 ROWID는 메모리 포인터라기보다는, 디스크 블록을 찾아가기 위한 논리적 주소에 가깝다.
  * 테이블 레코드에 물리적으로 연결된 구조가 아니라, 물리적으로 직접 연결되지 않고 테이블 레코드를 찾아가기 위한 논리적 주소 정보를 담고있기 때문이다.
  * 인덱스 ROWID는 포인터와는 다르며, 테이블 레코드를 찾아가기위한 위치 정보만을 담고 있다. 테이블 레코드와 물리적으로 연결된 구조가 아니다.
* 오라클은 테이블 블록이 수시로 버퍼캐시에서 밀려났다 다시 캐싱되기를 반복한다.
  * 그때마다 다른 공간에 캐싱되기 때문에 인덱스에서 포인터로 직접 연결할 수 없는 구조이다.
* 메모리 주소 정보(포인터)가 아닌, ROWID에 포함된 DBA를 이용해 해시 알고리즘으로 버퍼 블록을 찾아간다.

#### I/O 메커니즘
* DBA는 디스크 상에서 블록을 찾기 위한 주소 정보이다.
* 블록을 읽을 때 디스크로 가기 전 버퍼캐시부터 찾아본다.
  * I/O성능을 높이려면, 매번 디스크에서 블록을 읽을 수 없기 때문에 버퍼캐시를 활용해야 한다.
* 읽고자 하는 DBA를 해시함수에 입력해 해시 체인을 찾고, 거기서 버퍼 헤더를 찾고, 버퍼 헤더에 실제 버퍼 블록 포인터를 찾는다.
  * 캐시에 적재할 때와 읽을 때 같은 해시 함수를 사용하므로, 버퍼 헤더는 항상 같은 해시 체인에 연결된다.
  * 실제 데이터가 담긴 버퍼 블록은 매번 다른 위치에 캐싱되는데, 그 메모리 주소값을 버퍼 헤더가 갖고 있다.
  * 따라서 해싱 알고리즘으로 버퍼 헤더를 찾고 거기서 얻은 포인터로 버퍼블록을 찾아간다.
* 인덱스로 테이블 블록을 액세스할 때에는 리프블록에서 읽은 ROWID를 분해해 DBA정보를 얻고, 테이블을 Full Scan할 때는 익스텐트 맵을 통해 읽은 블록들의 DBA정보를 얻는다.
* **ROWID를 이용한 테이블 액세스는 생각보다 고비용 구조이다.**
  * `TABLE ACCESS BY INDEX ROWID` 에는, 이런한 랜덤 액세스 + Single Block I/O + 래치/락 처리가 항상 따라붙는다.

### 인덱스 클러스터링 팩터
* 클러스터링 팩터(CF)는 군집성 계수로, 특정 컬럼을 기준으로 같은 값을 갖는 데이터가 서로 모여있는 정도를 의미한다.
* CF가 좋은 컬럼에 생성한 인덱스는 검색효율이 매우 좋은데, 테이블 액세스량에 비해 블록 I/O가 적게 발생하기 때문이다.
* 인덱스 ROWID로 테이블을 액세스 할 때, 오라클은 래치 획득과 해시체인 스캔 과정을 거쳐 찾아간 테이블 블록에 대한 포인터를 해제하지 않고 유지한다. (버퍼 Pinning)
* 이 상태에서 다음 인덱스 레코드를 읽었는데, 직전과 같은 테이블 블록을 가리킨 경우, **래치 획득과 해시 체인 스캔 과정을 생략**하고 테이블 블록을 읽을 수 있다.
  * 즉, 논리적 I/O 과정을 생략할 수 있는 것이다.

### 인덱스 손익 분기점
* 읽어야할 데이터가 일정량을 넘는 순간, 테이블 전체를 스캔하는것보다 오히려 느려진다.
  * Table Full Scan은 시퀀셜 액세스인 반면, 인덱스 ROWID를 이용한 테이블 액세스는 랜덤 액세스 방식이다.
  * Table Full Scan은 Multiblock I/O인 반면, 인덱스 ROWID를 이용한 테이블 액세스는 Single Block I/O 방식이다.
* Index Range Scan에 의한 테이블 액세스가 Table Full Scan보다 느려지는 지점을 인덱스 손익분기점이라고 부른다.
* 인덱스 CF가 나쁘면 같은 테이블 블록을 여러번 반복 액세스 하면서 논리적 I/O 횟수가 늘어나고, 물리적 I/O 횟수도 늘어나게 된다.
* 인덱스 손익분기점은 보통 5~20%의 낮은 수준에서 결정된다. (100만건 정도의 테이블 기준)
  * CF가 나쁘면 손익분기점은 5%미만에서 결정되며, CF가 아주 좋을 땐 90%수준까지도 올라간다.
  * 1000만건 수준의 큰 테이블에선 손익분기점이 더 낮아진다.
    * 버퍼캐시에 할당하는 메모리 크기가 점점 커지는 추세이지만, 요즘 기준으로 보통 수백만개 블록을 캐싱하는 수준이다.
    * 따라서 특정 테이블을 인덱스로 100만건 이상 액세스 한다면 캐시 히트율은 극히 낮을수밖에 없다.
    * 1000만건 정도의 테이블이면 CF도 안좋을 가능성이 높고, 손익분기점 자체가 의미가 없어지므로, 만건 정도만 넘게 읽어도 Table Full Scan방식이 빠를 수 있다.
* 따라서 테이블스캔이 항상 나쁜것도 아니고, 인덱스 스캔이 항상 좋은것도 아니다.

#### 온라인 프로그램 튜닝 vs 배치 프로그램 튜닝
1) 온라인 프로그램
   * 보통 소량 데이터를 읽고 갱신하므로 인덱스를 효과적으로 활용하는것이 무엇보다 중요하다.
   * 조인도 대부분 NL 방식을 사용한다.
2) 배치 프로그램
   * 항상 전체 범위 처리 기준으로 튜닝해야 하므로, 인덱스와 NL조인보다 Full Scan과 해시조인이 유리한 경우가 많다.
   * 배치 프로그램에선 인덱스보다 Full Scan이 효과적이지만, 초대용량 테이블을 Full Scan하면 상당히 오래 기다려야 하고 시스템에 주는 부담도 커진다.
   * 따라서 **파티션 + 병렬처리 + Direct Path I/O를 적극 활용**해야 한다.
     * 파티셔닝하는 이유는 결국 Full Scan을 빠르게 처리하기 위해서이다.

### 인덱스 컬럼 추가
* 실 운영 환경에서는 인덱스 구성을 변경하기는 절대 쉽지 않다.
* 그렇다고 인덱스를 새로 만들게 되면, 인덱스를 계속 추가하게 되어 테이블마다 인덱스가 수십개씩 달려 있는 상황이 되어버릴 수 있다.
  * 인덱스 관리 비용이 증가하는것은 물론이며, DML 부하에 따른 트랜잭션 성능 저하가 생길 수 있다.
* 따라서 기존 인덱스에 특정 컬럼만 추가하는것으로 큰 효과를 얻을 수 있다.

### 인덱스만 읽고 처리
* 비효율이 없어도 **인덱스 스캔 과정에서 얻은 데이터가 많다면 테이블 랜덤 액세스가 많이 발생**하므로 성능은 느릴수밖에 없다.
```oracle
SELECT 부서번호, SUM(수량)
FROM 판매집계
WHERE 부서번호 LIKE '12%'
GROUP BY 부서번호;
```
* 위와같은 상황에서 고려해볼 수 있는 것은, 쿼리에 사용된 컬럼을 모두 인덱스에 추가해 테이블 액세스가 아예 발생하지 않도록 할 수 있다.
* 이렇게 인덱스만 읽어서 처리하는 쿼리를 **Covered 쿼리**라고 부르며, 그 쿼리에 사용한 인덱스를 **Covered 인덱스**라고 부른다.
* 하지만 이러한 방법은 추가해야 할 컬럼이 많아 실제로는 적용하기 곤란한 경우가 많다.

#### include 인덱스
* Oracle엔 없고 SQL Server 2005 버전에 추가된 기능이다.
* 인덱스 키 외에 미리 지정한 컬럼을 리프 레벨에 함께 저장하는 기능이다.
* `create index emp_x01 on emp (deptno) include (sal)`
  * 해당 인덱스를 이용하면, 수직적 탐색에는 `DEPTNO`만 사용하고, 수평적 탐색에는 `SAL` 컬럼도 바로 읽어 필터 조건으로 사용할 수 있다.
  * 즉, `SAL` 컬럼은 테이블 랜덤 액세스 횟수를 줄이는 용도로 사용된다.
  * 하지만 `SAL` 컬럼이 정렬 기준 컬럼은 아니기 때문에 소트 연산은 생략할 수 없다.

### 인덱스 구조 테이블 IOT
* 랜덤액세스가 아예 발생하지 않도록 **테이블을 인덱스 구조로 생성**할 수 있는데, 오라클은 **IOT(Index-Organized Table)라고 부른다.**
  * MS-SQL Server는 클러스터형(Clustered) 인덱스라고 부른다.
* 일반 인덱스는 테이블을 찾아가기위한 ROWID를 갖지만, IOT는 그 자리에 테이블 데이터 자체를 저장한다.
  * 테이블 블록에 있어야할 데이터를 리프블록에 모두 저장하고 있다.
  * 즉, IOT에서는 `인덱스 리프블록 = 데이터 블록`이다.
* 아래와 같이 테이블을 인덱스 구조로 만들 수 있다.
  ```oracle
  create table index_org_t(a number, b varchar(10), constraint index_org_t_pk primary key (a))
  organization index; -- 일반적인 힙 테이블은 organization heap과 같이 생성하며 생략 가능
  ```
* 일반 힙구조 테이블에 데이터를 입력할 때에는 랜덤 방식을 사용하지만(Freelist로부터 할당받은 블록에 순서 없이 데이터 입력), IOT는 인덱스 구조 테이블이므로 정렬 상태를 유지하며 데이터를 입력한다.
* IOT는 **의도적으로 좋은 CF를 만들기 위한 방법**이다.

### 클러스터 테이블
* 클러스터 테이블에는 인덱스 클러스터와 해시 클러스터 두 가지가 있다.

#### 인덱스 클러스터 테이블
* 인덱스 클러스 테이블은 **클러스터 키 값이 같은 레코드를 한 블록에 모아서 저장하는 구조**이다.
  * **한 블록에 모두 담을 수 없을 때**는 새로운 블록을 할당해서 **클러스터 체인으로 연결**한다.
* 또한 여러 테이블 레코드를 한 블록에 저장할 수 있는데, 이를 다중 테이블 클러스터라고 부른다.
  * 일반 테이블은 하나의 데이터 블록을 여러 테이블이 공유할 수 없다.
* 아래와 같이 인덱스 클러스터형 테이블을 만들 수 있다.
  ```oracle
  create cluster c_dept# ( deptno number(2) ) index ;
  ```
* 클러스터에 테이블을 담기 전 아래와 같이 클러스터 인덱스를 반드시 정의해야 한다.
  ```oracle
  create index c_dept#idx on cluster c_dept#;
  ```
* 클러스터 인덱스는 **데이터 검색용도**로 사용할 뿐만 아니라 **데이터가 저장된 위치를 찾는 용도**로도 사용한다.
* 클러스터 인덱스도 일반 B-Tree 인덱스 구조를 사용하지만, 테이블 레코드를 일일이 가리키지 않고 해당 키 값을 저장하는 첫 번째 테이블 블록을 가리킨다.
  * **일반 인덱스는 테이블 레코드와 1:1 대응관계**를 갖지만, **클러스터 인덱스는 테이블 레코드와 1:N 관계**를 갖는다.
* 그러므로 **클러스터 인덱스의 키 값은 항상 Unique**하다.
* 클러스터 인덱스를 스캔하면서 값을 찾을 때에는 랜덤액세스가 값 하나당 한번씩 밖에 발생하지 않는다. (클러스터 체인을 스캔하면서 발생하는 랜덤 액세스는 제외)
* **클러스터에 도달해 시퀀셜 방식으로 스캔하기 때문에 넓은 범위를 읽더라도 랜덤 I/O 비효율이 줄어든다.**

#### 해시 클러스터 테이블
* 해시 클러스터는 인덱스를 사용하지 않고 해시 알고리즘을 사용해 클러스터를 찾아간다.
* 아래와 같이 해시 클러스터를 생성할 수 있다.
  ```oracle
  create cluster c_dept# ( deptno number(2) ) hashkeys 4 ;
  ```
* 그리고 아래와 같이 클러스터 테이블을 생성한다.
  ```oracle
  create table dept (
    deptno number(2) not null
  , dname varchar2(14) not null
  , loc varchar2(13))
  cluster c_dept#( deptno );
  ```
* 해시 함수로 해시 버킷 > 클러스터 블록을 바로 찾기 때문에 `=` 조건 검색에 매우 빠르다.
  * 대신 범위검색, LIKE 검색 등에는 구조적으로 잘 맞지 않는다.

## 부분범위 처리 활용
* 부분범위 처리를 활용하면 인덱스로 액세스할 대상 레코드가 아무리 많아도 아주 빠른 응답속도를 낼 수 있다.

### 부분범위 처리
* 아무리 많은건을 요청을 해도, DBMS가 데이터를 모두 읽고 한번에 전송하지 않고 먼저 읽는 데이터부터 일정량을 전송하고 멈춘다.
* 데이터를 전송하고 나면 서버 프로세스는 CPU를 OS에 반환하고 대기 큐에서 잠을 잔다.
* 다음 Fetch Call을 받으면 대기 큐에서 나와 그다음 데이터부터 일정량(Array Size) 읽어서 전송하기 다시 잠을 잔다.
  * * JAVA에서 Array Size 기본 값은 10이고, Statement 객체 setFetchSize 메소드를 통해 설정을 변경할 수 있다.
* 이렇게 전체 쿼리 결과집합을 **사용자로부터 Fetch Call이 있을 때마다 일정량씩 나눠서 보내는 방식**을 부분범위 처리라고 한다.

> 잘못된 상식
> * 쿼리 수행 시 결과 집합을 버퍼캐시에 모두 적재하고 사용자에게 전송한다고 알고있는 사람이 있는데, 잘못되었다.
> * DBMS가 필요한 만큼 읽고 필요할 때마다 조금 씩 더 읽어오는 구조이다.

#### 정렬 조건이 있을 때 부분 범위 처리
* 정렬조건이 있으면, DB서버는 **모든 데이터를 다 읽어 정렬을 마치고 나서야 클라이언트에게 데이터 전송**을 시작할 수 있다.
  * Sort Area와 Temp 테이블스페이스까지 이용해 데이터 정렬을 마치고 나면 그때부터 일정량씩 나눠 클라이언트에게 데이터를 전송한다.
* 정렬기준 컬럼이 선두인 인덱스가 있으면 부분범위처리가 가능하다.
  * 인덱스는 항상 정렬된 상태를 유지하므로, 전체 데이터를 정렬하지 않고도 정렬된 상태의 결과집합을 바로 전송할 수 있다.

#### Array Size 조정을 통한 Fetch Call 최소화
* 대량의 데이터를 파일로 내려받는다면 어차피 데이터를 모두 전송해야하기 때문에 가급적 Array Size를 크게 해야 한다.
  * Fetch Call 횟수를 그만큼 줄여 네트워크 비용을 줄일 수 있다.
* 하지만 앞 쪽 일부 데이터만 Fetch하다 멈추는 프로그램이라면 Array Size를 작게 설정하는 것이 좋다.
  * 불필요하게 많은 데이터를 전송하고 버리는 비효율을 줄일 수 있다.

#### 쿼리 툴에서 부분 범위 처리
* 토드나 오렌지같은 쿼리 툴에서도 쿼리의 결과가 아무리 많아도 일찍 끝나는 것을 볼 수 있다.
* 그 이유는 전체 데이터 중 먼저 읽은 일정 수의 데이터만 출력하도록 해놓았기 때문이다. 즉, Array Size가 기본적으로 설정되어있기 때문이다.
* 모든 DBMS는 기본적으로 부분범위 처리 방식으로 결과집합을 전송한다. 
* 이 특징을 이용해 중간에 멈췄다 사용자의 추가 요청이 있을 때마다 데이터를 가져오도록 구현하고 안하고는 클라이언트 프로그램을 개발하는 개발자의 몫이다.

### 부분범위 처리 구현
* 부분범위 처리와 관련된 부분을 개발자가 일일이 구현할 순 없기 때문에, 개발 프레임워크에 미리 구현되어있는 기능을 활용한다.

### OLTP 환경에서 부분범위 처리에 의한 성능개선 원리
* OLTP 시스템도 수천 수만건을 조회하는 경우도 있다.
* 인덱스를 이용해 수천 수만건을 조회하려고 하면 성능을 내기 어려울 수 있다.
  * 많은 랜덤액세스가 발생하기 때문이다.
* 인덱스와 부분범위 처리 원리를 잘 활용하면 OLTP 환경에서 좋은 성능개선 효과를 얻을 수 있다. (페이징, 스크롤, Top N 조회 등)

#### 멈출 수 있어야 의미있는 부분범위 처리
* 클라이언트와 DB 서버 사이에 WAS, AP 서버 등이 존재하는 n-Tier 아키텍처에서는 클라이언트가 특정 DB 커넥션을 독점할 수 없다.
* 단위작업을 마치면 DB 커넥션을 커넥션 풀에 반환해야 하므로, 그 전에 SQL조회 결과를 클라이언트에 모두 전송하고 커서를 닫아야 한다.
* 그렇다고 해서 부분범위 처리는 n-Tier 환경에서 의미없는 개념은 아니다.

> 배치 I/O
> * 디스크 랜덤 I/O 성능을 높애기 위해, 오라클에서는 배치 I/O 기능을 제공한다.
> * 배치 I/O는 블록마다 I/O Call을 한 번씩 발생시키지 않고, 읽을 블록이 일정량 쌓일 때까지 **I/O 요청을 모아두었다가 한꺼번에 처리**한다.
>   * 블록마다 I/O Call을 발생시키는 비효율을 줄이기 위해 고안한 기능이다.
> * 11g에서는 NL 조인 Inner 쪽 테이블 액세스할 때에만 이 기능이 작동했지만, 12c부터는 인덱스 ROWID로 테이블 액세스하는 어떤 부분에서든 이 기능이 작동할 수 있다.
> * 배치 I/O 기능이 작동하면 인덱스를 이용해 출력하는 데이터 정렬 순서가 매번 다를 수 있다.
>   * 따라서 배치 I/O 기능이 작동하면 데이터 정렬순서를 보장할 수 없기 때문에 **소트연산을 생략할 수 없게되는 문제**가 생길 수 있다.
>   * 이러한 이유로 ,배치 I/O를 통해 얻을 수 있는 이점이 많음에도 **정렬/순서 보장이 중요한 환경에서는 이 기능을 비활성화**한다.

## 인덱스 스캔 효율화
### 인덱스 탐색
* 인덱스가 어떻게 설정되어있고, 어떤식으로 조건을 걸 때 인덱스를 어떤식으로 스캔할지 알아야 한다.
* 인덱스 컬럼 순서, 비교 연산자 종류, 조건절에 등장하는 컬럼 위치가 수직/수평 스캔 방식에 어떤 영향을 주는지 알아야 한다.
* p174 ~ p179 참고

### 인덱스 스캔 효율성
* 인덱스를 타는지 안타는지만 보는 것이 아니라, 얼마나 좋은 범위를 스캔하고 멈추는지, 얼마나 적은 블록을 읽는지를 봐야 한다.
* 인덱스 선행 컬럼이 조건에 없는 경우 스캔 범위가 불필요하게 늘어날 수 있다.
* p180 ~ p184 참고

#### 인덱스 스캔 효율성 측정
* 조건절 데이터를 일일이 조회해보는 방법도 있지만, SQL 트레이스를 통해 쉽게 알 수 있다.
* 예시로, **인덱스를 스캔하고 얻은 레코드가 적은데 그 과정에 읽은 블록수가 많으면 비효율적**이라고 볼 수 있다.

### 액세스 조건과 필터 조건
* 인덱스를 스캔하는 단계에 처리하는 조건절은 액세스 조건과 필터 조건으로 나뉜다.
1) 인덱스 액세스 조건
   * 인덱스 스캔 범위를 결정하는 조건절이다.
   * 인덱스 수직적 탐색을 통해 스캔 시작점을 결정하는데 영향을 미친다.
   * 인덱스 리프 블록을 스캔하다가 어디서 멈출지를 결정하는데 영향을 미친다.
2) 인덱스 필터 조건
   * 인덱스를 스캔하는 동안 이 레코드를 가지고 테이블로 액세스할지를 결정하는 조건절이다.
   * 인덱스 레벨에서 먼저 체크 후 인덱스 필터 조건을 만족하지 않으면 테이블 액세스를 생략할 수 있다.
     * 인덱스 스캔 범위를 줄여주지는 못한다.
3) 테이블 필터 조건 
   * 실제 테이블 블록을 읽은 뒤 적용되는 조건절이다. 
   * 쿼리 수행 다음 단계로 전달하거나, 최종 결과 집합에 포함할지를 결정한다.

### 비교 연산자 종류와 컬럼 순서에 따른 군집성
* 선행 컬럼이 모두 `=` 조건인 상태에서 첫 번째 나타나는 범위검색 조건까지만 만족하는 인덱스 레코드는 모두 연속해서 모여있다.
* 하지만 그 이후 컬럼들에 대한 조건은 인덱스 상에서 레코드들이 흩어지게 된다.
  * 즉 스캔 효율에 크게 기여하지 못한다.

#### 범위검색 조건 맨 처음과 마지막 구간에서의 액세스 조건
```oracle
where C1 between 1 and 3
  and C2 = 'A'
  and C3 = '나'
  and C4 = 'a'
```
* `C1, C2, C3, C4`로 인덱스가 구성되어있을 때 위 조건일 경우엔 액세스 조건은 C1, 필터조건은 C2, C3, C4이다.
* 하지만 `C1 = 1`, `C1 = 3`인 구간에서 C2, C3, C4도 인덱스 스캔량을 줄이는데 어느정도 역할을 할 수 있다.
  * `C1 = 3` 구간에서 `C2 = 'A'`, `C3 = '나'`, `C4 = 'a'`보다 큰 값을 만나는순간 멈추기 때문이다.
* **즉, 첫 범위검색 이후 조건절 컬럼은 스캔범위를 줄이는데 큰 역할을 못함에도, 인덱스 컬럼에 대한 조건절은 모두 액세스 조건에 표시**된다.
  * 아래와 같은 몇몇 케이스는 제외
    * 좌변 컬럼을 가공한 조건절
    * 왼쪽 `%` 또는 양쪽 `%` 기호를 사용한 `like` 조건절
    * 같은 컬럼에 대한 조건절이 두 개 이상일 때 인덱스 액세스 조건으로 선택되지 못한 조건절
    * OR Expansion 또는 INLIST ITERATOR로 선택되지 못한 OR 또는 IN 조건절
* 하지만 너무 복잡하게 생각할 것 없이, **첫 번째 나타나는 범위검색 조건까지가 인덱스 액세스 조건이고, 나머지는 필터조건이라고 이해하는게 좋다.**

### 인덱스 선행 컬럼이 등치(`=`) 조건이 아닐 때 생기는 비효율
* 인덱스 스캔 효율성은 인덱스 컬럼을 조건절에 모두 `=` 조건으로 사용할 때 가장 좋다.
* 인덱스 컬럼 중 일부가 조건절에 없거나 `=` 조건이 아니더라도, 그것이 뒤 쪽 컬럼일 때에는 비효율이 전혀 없다.
  * 조건을 만족하는 레코드들이 인덱스 상에서 한 곳에 모여있기 때문이다. 
* 결국 핵심은 선두 컬럼을 최대한 `=`조건으로 묶어서 필요한 구간만 스캔하고 멈추는 구조를 만드는 것이다. 

### BETWEEN을 IN-List로 전환
* 범위검색 컬럼이 맨 뒤로 가도록 인데스를 설계하는것이 좋다.
* 하지만 운영 환경에서 인덱스 구성을 바꾸기는 쉽지 않다.
* 이럴 때에는 BETWEEN 조건을 IN-List로 바꿔주면 큰 효과를 얻는 경우가 있다.
  * IN-LIST 개수 만큼 UNION ALL 브랜치가 생성되고 각 브랜치마다 모든 컬럼을 `=` 조건으로 검색한다.
  * 옵티마이저가 IN-List Iterator 방식으로 실행하여 넓게 스캔하던 비효율을 줄일 수 있다.
* IN-List 항목 개수가 늘어날 수 있는 경우 BETWEEN을 IN-List로 전환하는 방식은 사용하기 곤란할 수 있다.
  * IN-List 값들을 코드 테이블로 관리하고 있는 경우라면, NL의 조인문이나 서브쿼리로 구현하면 된다.
* 또한 Index Skip Scan 방식으로 유도해도 BETWEEN 비효율을 줄이는데 비슷한 효과를 낼 수 있다.

#### BETWEEN 조건을 IN-List로 전환할 때 주의사항
* **IN-List의 개수가 많지 않아야 한다.**
* IN-List의 개수가 많으면 수직적 탐색이 많이 발생한다.
  * 이런 경우엔 리프블록을 많이 스캔하는 비효율보다, IN-List 개수만큼 브랜치 블록을 반복 탐색하는 비효율이 더 클 수 있다.
  * 루트에서 브랜치 블록까지 Depth가 더 깊을 경우 특히 그렇다.
  * IN-List 개수가 많으면 수직적 탐색 과정에서 많은 블록을 읽게 된다.
* BETWEEN 조건 때문에 인덱스를 비효율적으로 스캔하더라도, 실제 블록 I/O는 생각보다 적은 경우도 많다.
  * 인덱스 리프 블록에는 테이블 블록보다 더 많은 레코드가 담기기 때문이다.
* **데이터 분포나 수직적 탐색 비용을 따져보지도 않고 BETWEEN을 IN-List로 변환하면 안된다.**

#### Index Skip Scan 활용
* BETWEEN 조건을 IN-List 조건으로 변환하면 도움이 되는 상황에서 굳이 조건절을 바꾸지 않고도 효과를 낼 방법이 있는데, Index Skip Scan을 활용하면 된다.
* 선두 컬럼이 BETWEEN이어서 나머지 검색 조건을 만족하는 데이터들이 서로 멀리 떨어져 있을 때 Index Skip Scan이 효율적이다.

### IN 조건은 `=`인가?
* IN 조건은 `=`이 아니다.
  * 옵티마이저 입장에서 항상 `=`과 동일하게 동작하지 않는다.
* 인덱스를 어떻게 구성하냐에 따라 성능도 달라질 수 있다.
* **IN 조건이 `=`이 되려면, 즉 액세스 조건이 되려면 IN-List Iterator 방식으로 풀려야만 한다.**
  * 그렇지 않으면 IN 조건은 필터 조건이다.
* 또한 IN 조건을 액세스 조건으로 만들기 위해 `=`조건으로 만들어 IN-List Iterator 방식으로 푸는게 항상 효과적이지도 않다.
* p204~p205 참고

#### NUM_INDEX_KEYS
* IN-List를 액세스조건 또는 필터조건으로 유도하는 방법이 있는데, `num_index_keys` 힌트를 사용하는 것이다.
* 인덱스가 `[고객번호 + 상품ID]` 순으로 구성된 상황에서 고객번호만 인덱스 액세스 조건으로 사용하려면 아래와 같이 힌트를 사용하면 된다.
  ```oracle
  select /*+ num_index_keys(a 고객별가입상품_X1 1)*/ 
      *
  from 고객별가입상품 a
  where 고객번호 = :cust_no
  and 상품ID in ('T01', 'T05', 'T52');
  ```
  * 3번째 인자 1은 인덱스의 첫 번째 컬럼까지만 액세스 조건으로 사용하라는 의미이다.
* 또한 힌트 사용 없이, 인덱스 컬럼을 일부로 가공하는 방법도 있다.
  ```oracle
  select *
  from 고객별가입상품 a
  where 고객번호 = :cust_no
  and RTRIM(상품ID) in ('T01', 'T05', 'T52');
  
  select *
  from 고객별가입상품 a
  where 고객번호 = :cust_no
    and 상품ID || ''  in ('T01', 'T05', 'T52');
  ```
  * 이렇게 컬럼을 가공하면 `상품ID`는 인덱스 액세스 조건이 되기 어렵기 때문에 `고객번호`만 인덱스 액세스 조건으로 사용되게 만들 수 있다.

### BETWEEN과 LIKE 스캔 범위 비교
* LIKE와 BETWEEN은 둘 다 범위검색 조건으로, 범위검색 조건을 사용할 때의 비효율 원리가 똑같이 적용된다.
* 일반적으로 **LIKE보다는 BETWEEN을 사용하는게 좋다.**
  * LIKE는 패턴에 따라 액세스 조건이 깨지는 경우가 많고, BETWEEN은 비교 범위가 명확해 옵티마이저가 스캔 범위를 계산하기 쉽다.
* 물론 사용하기엔 BETWEEN이 불편하지만, BETWEEN을 사용하면 손해보는 경우는 없다.

### 범위검색 조건을 남용할 때 생기는 비효율
* 사용자 입력에 따라 조건절이 다양하게 바뀌는 경우, SQL을 간편하게 작성하려고 조건절을 모두 LIKE로 개발해버리는 경우가 있다.
* 이럴 경우 **액세스 조건이 필터 조건으로 변해버릴 수도 있다.**
* 대량의 테이블을 넓은 범위로 검색할 경우 더더욱 영향이 심해진다.
* 따라서 사용자 입력이 다양하다는 이유만으로 범위검색을 남용해서는 안된다.
* 데이터 분포, 카디널리티, 인덱스 구성을 고려해 비교 연산자를 신중하게 선택해야 한다.

### 다양한 옵션 조건 처리 방식의 장단점 비교
#### OR 조건 활용
```oracle
select * from 거래
where (:cust_id is null or 고객ID = :cust_id)
    and 거래일자 between :dt1 and :dt2
```
* 인덱스가 `[고객ID + 거래일자]`로 구성되어있는 경우, 옵션 조건 컬럼을 선두에 두어도 인덱스를 사용할 수 없다.
* `[거래일자 + 고객ID]`로 구성한 인덱스는 사용할 순 있지만, 고객 ID를 필터조건으로 사용하는 문제가 있다.
* 인덱스에 포함되지 않은 컬럼에 대한 옵션 조건은 어차피 테이블에서 필터링할 수 밖에 없으므로 그럴 때에는 위와같은 방식을 사용해도 무방하다.
* 따라서 OR 조건을 활용한 옵션 조건 처리는 아래를 고려해야 한다.
  * 인덱스 액세스 조건으로 사용 불가
  * 인덱스 필터 조건으로 사용 불가
  * 테이블 필터 조건으로만 사용 가능
  * 인덱스 구성 컬럼 중 하나 이상이 Not Null 컬럼이면 18c부터 인덱스 필터조건으로 사용 가능
* 이런 특성을 항상 고려할 수 있는게 아니라면, **OR 조건을 이용한 옵션 조건 처리는 가급적 사용하지 않아야 한다.**
  * 이방식의 **유일한 장점**은 옵션 조건 컬럼이 **NULL 허용 컬럼이어도 결과 집합을 보장**한다는 것이다.

#### LIKE/BETWEEN 조건 활용
* **필수조건의 변별력이 좋은 경우**엔 아래와 같은 패턴을 사용하는것도 나쁘지 않다.
  ```oracle
  -- 인덱스 : 등록일시 + 상품분류코드
  select * from 상품
  where 등록일시 >= trunc(sysdate)             -- 필수 조건
      and 상품분류코드 like :prd_cls_cd || '%'  -- 옵션 조건
  ```
  * 또한 필수 조건이 `=` 이면 옵션조건까지도 인덱스 액세스 조건이므로 최적의 성능을 낼 수 있다.

* 아래 예시와 같이, 필수조건의 변별력이 좋지 않을 때에는 문제가 된다.
  ```oracle
  -- 인덱스 : 상품대분류코드 + 상품코드
  select * from 상품
  where 상품대분류코드 = :prd_lcls_cd    -- 필수 조건
      and 상품코드 like :prd_cd || '%' -- 옵션조건
  ```
  * 상품 대분류 코드만으로 조회할 때에는 Table Full Scan이 유리하지만, 옵티마이저가 상품코드까지 입력할 때를 기준으로 Index Range Scan을 선택한다.
* 따라서 LIKE/BETWEEN 조건을 활용한 옵션 조건 처리는 아래를 고려해야 한다.
  * 인덱스 선두 컬럼인지
    * 인덱스 선두컬럼인 경우, 모든 데이터를 스캔하는 일이 생김
  * NULL 허용 컬럼인지
    * NULL 허용컬럼인데 실제 NULL 값이 입력되어 있다면 그 데이터는 결과집합에서 누락된다. (BETWEEN도 같음)
  * 숫자형 컬럼인지
    * 자동형변환이 일어나 액세스 조건이 아닌 필터조건으로 사용될 수 있다.
  * 가변 길이 컬럼인지
    * 가변컬럼이면 길이가 다른 값이 같이 조회되어 생각보다 넓은 범위를 읽게 될 수 있다.

#### UNION ALL 활용
```oracle
-- 인덱스 : 고객ID + 거래일자
select * from 거래
where :cust_id is null
    and 거래일자 between :dt1 and :dt2
union all
select * from 거래
where :cust_id is not null
    and 거래일자 between :dt1 and :dt2
```
* 위 쿼리는 `:cust_id` 변수에 값에 입력을 하던 안하던 인덱스를 최적으로 활용한다.
* 이 방식은 옵션 조건 컬럼도 인덱스 액세스 조건으로 사용한다.
* 또한 고객ID가 NULL 허용 컬럼이어도 사용하는데 문제가 없다.
* **유일한 단점은 SQL 코딩량이 길어진다**는 점이다.

#### NVL/DECODE 함수 활용
```oracle
select * from 거래
where 고객ID = nvl(:cust_id, 고객ID)
and 거래일자 between :dt1 and :dt2;

select * from 거래
where 고객ID = decode(:cust_id, null, 고객ID, :cust_id)
  and 거래일자 between :dt1 and :dt2;
```
* `고객ID` 컬럼을 함수 인자로 사용했는데도 인덱스를 사용할 수 있는 것은 OR Expansion 쿼리변환이 일어났기 때문이다. (UNION ALL 방식으로 변환)
  * OR Expansion 쿼리변환이 일어나지 않으면 않으면 액세스조건으로 사용 불가능하다.
  * `고객ID = 고객ID` 형태가 되기 때문에 인덱스에서 이 조건을 만족하는 한 시작점을 찾을 수 없다.
* 이 방식의 큰 **장점은 옵션 조건 컬럼을 인덱스 액세스 조건으로 사용할 수 있다**는 점이다.
* **UNION ALL 보다 단순하면서도 같은 성능**을 낼 수 있다.
* **옵션 조건 처리용 NVL/DECODE 함수를 여러개 사용하면 그중 변별력이 가장 좋은 컬럼 기준으로 한 번만 OR Expansion이 일어난다.**
  * OR Expansion 기준으로 선택되지 않으면 인덱스 구성컬럼이어도 모두 필터 조건으로 처리된다.
* 즉, NVL/DECODE 함수의 장점에도 불구하고 **모든 옵션 조건을 이 방식으로 처리할 수 없다.**

> Dynamic SQL
> * Dynamic SQL을 이용해 조건절을 동적으로 구성할 수 있는 환경에서는 위와같은 내용에 대해 공감을 하지 못할 수 있다.
> * Dynamic SQL을 이용해 `=` 연산자를 활용할 경우 변별력있는 컬럼을 액세스 조건으로 사용할 수 있게 인덱스만 잘 구성해주면 된다.
> * 하지만 힌트로 액세스 경로를 고정하려고 할 때 위와같은 튜닝 기법을 적절히 활용해야 한다.
> * Dynamic SQL에 힌트를 명시하면 동적으로 구성된 조건절과 서로 상충하여 성능 문제를 야기할 수 있다.
> * Dynamic SQL을 허용하지 않는 시스템도 있다.(특히나 금융권)

### 함수호출부하 해소를 위한 인덱스 구성
#### PL/SQL 함수의 성능적 특성
* PL/SQL 사용자 정의 함수는 생각보다 **매우 느리다.**
* PL/SQL 사용자 정의 함수가 느린 이유가 있다.
  * 가상머신상에서 실행되는 인터프리터 언어
  * 호출 시마다 컨텍스트 스위칭 발생
  * 내장 SQL에 대한 Recursive Call 발생
* 오라클은 오라클 서버가 아닌 Oracle Forms, Oracle Reports 같은 제품에서도 수행될 수 있도록 PL/SQL을 설계하였다.
  * 따라서 PL/SQL로 작성한 함수와 프로시저를 컴파일하면 JAVA 언어처럼 바이트코드를 생성해 데이터 딕셔너리에 저장하며, 이를 해석할 수 있는 PL/SQL 엔진만 있으면 어디서든 실행할 수 있다.
  * 즉, **런타임에 바이트코드를 해석하면서 실행하는 인터프리터 방식**이다.
* 결국 PL/SQL도 인터프리터 언이이기 때문에 Native 코드로 완전 컴파일된 내장 함수에 비해 많이 느리다.
* PL/SQL 함수는 실행 시 SQL 실행엔진과 PL/SQL 가상머신 사이에 **컨텍스트 스위칭이 일어난다.**
* 따라서 **PL/SQL 함수를 작은단위로 모듈화, 공용화하면 안된다.**
* **PL/SQL 사용자 정의 함수의 성능을 떨어뜨리는 가장 결정적인 요소는 Recursive Call이다.**
  ```oracle
  select 회우너번호, 회원명, GET_ADDR(우편번호) as 기본주소
  from 회원
  where 생월일 like '01%'
  ```
  * 위 예시에서, 조건을 만족하는 회원이 100만명이면 GET_ADDR도 100만번 실행하고, SQL이 내장되어있다면 SQL도 100만번 실행하게 된다.

#### 효과적인 인덱스 구성을 통한 함수 호출 최소화
```oracle
select /*+ full(a) */ 회원번호, 회원명, 생년, 생월일
from 회원 a
where 생년 = '1994' 
  and 암호화된전화번호 = encryption( :phone_no )
```
* 위 예시에서는, `encryption` 함수는 `생년`조건절을 만족하는 건수 만큼 수행된다.

```oracle
select /*+ index(a 회원_x01) */ 회원번호, 회원명, 생년, 생월일
from 회원 a
where 생년 = '1994' 
  and 암호화된전화번호 = encryption( :phone_no )
```
* 위 예시에서는, `암호화된전화번호` 컬럼에 대한 비교가 인덱스를 어떻게 타냐에 따라 `암호화된전화번호` 기준으로 몇 건이나 읽게 되는지가 달라진다는 점이다. 
  1. `create index 회원_X01 on 회원(생년);`
     * encryption 함수는 생년 조건을 만족하는 건수만큼 수행된다.
  2. `create index 회원_X01 on 회원(생년, 생월일);`
     * encryption 함수는 생년 조건을 만족하는 건수만큼 수행된다.
  3. `create index 회원_X01 on 회원(생년, 암호화된전화번호);`
     * encryption 함수는 단 한번만 수행된다.
     * 생년과 함께 암호화된전화번호도 인덱스 액세스 조건으로 사용되기 때문이다.

## 인덱스 설계
* OLTP 시스템에서 인덱스 설계는 매우 중요하다.

### 인덱스 설계가 어려운 이유
* SQL 각각에 최적화된 인덱스를 계속 생성할 수 있다면 SQL튜닝과 인덱스 설계는 매우 쉬운일이 된다.
* 하지만 이렇게 인덱스를 생성하다보면 테이블마다 인덱스가 수십 개씩 달리게 되며, 관리비용 뿐만 아니라 시스템 부하를 증가시키게 된다.
* 인덱스가 많을 경우 아래와같은 문제가 생긴다.
  * DML 성능 저하 (TPS 저하)
  * 데이터베이스 사이즈 증가 (디스크 공간 낭비)
  * 데이터베이스 관리 및 운영 비용 상승
* 인덱스가 여러개 있는 경우, INSERT 시 여러 인덱스에도 데이터를 입력해야 한다.
  * 테이블과 달리 정렬상태를 유지하기 때문에, 수직적 탐색을 통해 입력할 블록을 찾아야 한다.
  * 찾은 블록에 여유 공간이 없다면 인덱스 분할(Index split)도 발생한다.
* DELETE시에도 여러 인덱스에서 일일이 찾아 지워줘야 한다.
* 따라서 개별 쿼리성능 뿐만 아니라, 그 개수를 최소화해야 하므로 인덱스 설계는 어렵다.

> 개발단계에서의 인덱스 설계
> * 운영환경이 되는 순간 인덱스를 변경하는건 쉽지 않다.
> * 그나마 신규 인덱스 추가는 변경 영향도가 작지만, 그럴수록 시스템 수준 TPS는 나빠질 수 밖에 없다.
> * 따라서 **개발단계에서 인덱스를 정교하게 설계**해야 한다.

### 가장 중요한 두 가지 선택 기준
1. **조건절에 항상 사용하거나 자주 사용하는 컬럼을 선정**해야 한다.
2. 이렇게 선정한 컬럼 중 **`=`조건으로 자주 조회하는 컬럼을 앞 쪽에 두어야 한다.**
* 위 두 선택 기준은 **인덱스 스캔 효율성**을 극대화하는 것이다.

### 스캔 효율성 이외의 판단 기준
* 가장 중요한 두 가지 선택 기준만으로 설계를 하다보면, 오히려 인덱스 개수만 더 늘어나는 현상이 발생할 수 있다.
* 즉, 인덱스 스캔 효율성 외 고려해야 할 판단 기준들이 있다.
  * 수행 빈도
  * 업무상 중요도
  * 클러스터링 팩터
  * 데이터량
  * DML 부하
  * 저장 공간
  * 인덱스 관리 비용 등
* 이러한 다양한 판단 기준에 대한 해석이 서로 다르기 때문에, 설계자의 성향이나 스타일에 따라 결과물도 많이 달라진다.

#### 수행빈도
* 여러 판단 기준 중 가장 중요한 하나를 꼽자면 바로 수행빈도이다.
  * 자주 수행하지 않는 SQL이면 인덱스 스캔 과정에 약간의 비효율은 있어도 큰 문제가 안될 수 있기 때문이다.
* 수행빈도가 가장 높은 SQL에는 최적의 인덱스를 구성해줘야 한다.
* NL 조인할 때 어느 쪽에서 자주 액세스되는지도 중요한 판단 기준이 된다.
  * NL 조인할 때, Outer쪽에서 액세스하는 인덱스는 스캔과정에 비효율이 있더라도 큰 문제가 아닐 수 있다.
  * 하지만 NL 조인에서 Inner 쪽 인덱스 스캔 과정에 비효율이 있다면, 이것은 성능에 큰 문제를 야기할 수 있다.
  * NL조인 Inner 쪽 인덱스는 `=` 조건 컬럼을 선두에 두는것이 중요하고, 될 수 있으면 테이블 액세스 없이 인덱스에서 필터링을 마치도록 구성해야 한다.
* 따라서 **수행빈도가 매우 높은 SQL이라면 인덱스를 최적으로 구성해줘야 한다.**

#### 데이터량
1) 데이터가 적은 경우
   * 굳이 인덱스를 많이 만들 필요가 없고, Full Scan으로도 충분히 빠르다.
   * 또한 인덱스를 많이 만들어도 저장공간이나 트랜잭션 부하 측면에서 그다지 문제될 것이 없다.
   * 따라서, 테이블의 데이터가 적은 경우 깊게 고민할 필요가 없다.
2) 데이터가 굉장히 많은 경우
   * 인덱스 설계가 굉장히 중요하다.
   * 초당 DML 발생량은 TPS에 직접적인 영향을 준다.
   * 따라서 인덱스를 하나라도 줄였을 때 성능은 많이 향상된다.

### 인덱스의 전략적인 설계
* SQL 튜닝 전문가라면 가장 핵심적인 액세스 경로 한 두개를 전략적으로 선택해 최적 인덱스를 설계한다. 
* 그 외 나머지 액세스 경로들에 대해서는 약간의 비효율이 있더라도 목표 성능을 만족하는 수준으로 인덱스를 구성할 줄 알아야 한다.
* 설계 시 중요한 점은, **왜 그렇게 설계를 했고 근거가 무엇인지** 답할 수 있어야 하며, 업무 상황을 이해하고 나름의 판단 기준을 가지고 결정을 내려야 한다.

#### 가계약 테이블 예시
* 인덱스 설계 시, 일자/일시 조건을 선두에 두고 자주 사용하는 필터 조건을 뒤쪽에 추가한다.
  * 일자 조회 구간이 길지 않으면 인덱스 스캔 비효율이 성능에 미치는 영향이 크지 않다.
  * 이 경우엔 인덱스 스캔 효율보다 테이블 액세스가 더 큰 부하 요소이다.
    * 가계약은 보통 최근 데이터만 조회하므로, 인덱스 스캔량이 그리 많지 않다.
* 또한, 추가로 가장 많이 사용하는 패턴에 최적의 스캔 효율을 제공하는 인덱스를 추가한다.   

### 소트 연산을 생략하기 위한 컬럼 추가
* 조건절에 사용하지 않는 컬럼이더라도 소트 연산을 생략할 목적으로 인덱스 구성에 포함시킴으로써 성능 개선을 할 수 있다.
```oracle
select 계약ID, 청약일자, 입력자ID, 계약상태코드, 보험시작일자, 보험종료일자
from 계약
where 취급지점ID = :trt_brch_id
  and 청약일자 between :sbcp_dt1 and :sbcp_dt2
  and 입력일자 >= trunc(sysdate - 3)
  and 계약상태코드 in (:crt_stat_cd1, :crt_stat_cd2, :crt_stat_cd3)
order by 청약일자, 입력자ID
```
* 위 예시에서 성능을 고려하지 않고 단순히 소트연산을 생략하려고 하면 `[청약일자 + 입력자ID]`로 인덱스를 구성하면 된다.
* 여기에 추가로, `=`조건절 컬럼은 ORDER BY 절에 없더라도 인덱스 구성에 포함할 수 있다.
  * `[청약일자 + 취급지점ID + 입력자ID]` 순으로 구성해도 소트연산을 생략할 수 있다. (앞뒤 중간 어디에 두어도 상관 없음)  
* `=`이 아닌 조건절들은 반드시 ORDER BY 컬럼보다 뒤쪽에 두어야 소트연산을 생략할 수 있다.
* I/O를 최소화하면서 소트 연산을 생략하려면 아래와 같은 공식으로 인덱스를 구성하면 된다.
  * `=` 연산자로 사용한 조건절 컬럼 선정
  * ORDER BY 절에 기술한 컬럼 추가
  * `=` 연산자가 아닌 조건절 컬럼은 데이터 분포를 고려해 추가 여부 결정
* 따라서 인덱스를 `[취급지점ID + 청약일자 + 입력자ID]` 순으로 구성하면 된다.

#### IN 조건은 `=`이 아니다.
```oracle
-- 인덱스 : 거주지역 + 혈액형 + 연령
select 고객번호, 고객명, 거주지역, 혈액형, 연령
from 고객
where 거주지역 = '서울'
  and 혈액형 in ('A', 'O')
order by 연령
```
* IN 조건이 `=`이 되려면 IN-List Iterator 방식으로 풀려야 한다.
* 그러면 IN 조건이 `=`이 되었지만 UNION ALL 위아래 두 집합을 묶어 연령 순으로 정렬하는 문제가 남게 된다.
* 따라서 **소트연산을 생략하려면 IN 조건절이 IN-List Iterator 방식으로 풀려선 안된다.**
* 즉, 위 예시는 `[거주지역 + 연령 + 혈액형]`이 되어야 한다.
  * IN 조건에 들어간 `혈액형`이 필터 조건으로 동작하도록 되어야 한다.

### 결합 인덱스 선택도
* 인덱스 생성 여부를 결정할 때, 조건 선택도가 충분히 낮은지가 중요한 판단 기준이다.
* 조건 선택도가 높은 인덱스는 생성해봐야 효용가치가 별로 없다.
  * 테이블액세스가 많이 발생하기 때문이다.

#### 컬럼 순서 결정 시 선택도
* 인덱스 생성 여부를 결정할 때 조건 선택도가 매우 중요하지만, 결합인덱스의 컬럼간 순서를 결정할 때 각 컬럼의 조건 선택도보다는 필수조건의 여부나 연산자의 형태가 더 중요하다.
  * 모두 `=` 조건이라면 선택도가 낮은 컬럼이 앞에 오든 뒤에오든 효율에 영향이 없다. 

### 중복 인덱스 제거
* 아래 3개의 인덱스는 완전 중복이므로, 위 2개의 인덱스는 모두 지워도 된다.
  * `[계약ID + 청약일자]`
  * `[계약ID + 청약일자 + 보험개시일자]`
  * `[계약ID + 청약일자 + 보험개시일자 + 보험종료일자]`

* 아래 4개의 인덱스는 얼핏 보기엔 중복은 아니다.
  * `[계약ID + 청약일자]`
  * `[계약ID + 보험개시일자]`
  * `[계약ID + 보험종료일자]`
  * `[계약ID + 데이터생성일시]`
* 하지만 `계약ID`의 카디널리티가 매우 낮으면 사실상 중복이다. 
  * `[계약ID + 청약일자 + 보험개시일자 + 보험종료일자 + 데이터생성일시]` 와 같이 하나만 만들어도 충분하다.

#### 중복인덱스 실습예시
* p245 ~ p249 참고

### 인덱스 설계도 작성
* 인덱스 설계 시 시스템 전체 효율을 고려해야 한다.
* 따라서 인덱스 설계에도 설계도면이 필요하다.
* p249 참고

# 조인 튜닝
## NL 조인
### 기본 메커니즘
* NL조인은 말그대로 중첩 루프문과 같은 수행구조를 사용한다.
* 일반적으로 NL 조인은 Outer와 Inner 양쪽 테이블 모두 인덱스를 이용한다.
  * Outer쪽 테이블은 사이즈가 크지 않으면 인덱스를 이용하지 않을 수 있다.
    * Table Full Scan 하더라도 한번에 그치기 때문이다.
  * 반면에, **Inner쪽 테이블은 인덱스를 사용해야 한다.**
    * Inner 루프에서 인덱스를 이용하지 않으면 Outer 루프에서 읽은 건수만큼 Table Full Scan을 반복하기 때문이다.
* 따라서 **NL조인은 인덱스를 이용한 조인 방식**이라고 할 수 있다.
* 소트 머지 조인과 해시 조인도 가공해둔 데이터를 이용한다는 점만 다를 뿐 기본적인 프로세싱은 다르지 않다.

### NL 조인 실행계획 제어
* NL조인은 각 테이블을 액세스할 때 인덱스를 이용한다.
* NL조인을 제어할 때에는 `use_nl` 힌트를 사용하면 된다.
  * 추가로 `ordered`, `leading` 힌트는 FROM절에 기술한 순서대로 조인하라고 지시할 때 사용한다.

* 아래는 A, B, C, D순으로 조인하되 B, C와조인할때는 NL 방식으로 조인하고, D와 조인할 때는 해시 방식으로 조인하라는 뜻이다.
  ```oracle
  select /*+ ordered use_nl(B) use_nl(C) use_hash(D) */ 
         ...
  from A, B, C, D
  where ...
  ```
  * `ordered` 대신 `leading` 힌트를 사용할 수도 있다.
    ```oracle
    select /*+ leading(C, A, D, B) use_nl(A) use_nl(D) use_hash(B) */ 
           ...
    from A, B, C, D
    where ...
    ```
* 아래 예시는 네 개의 테이블을 NL방식으로 조인하되 순서는 옵티마이저가 정하도록 맡긴 것이다.
  ```oracle
  select /*+ ordered use_nl(A, B, C, D) */
  from A, B, C, D
  where ...
  ```

### Nl 조인 수행 과정 분석
```oracle
select /*+ ordered use_nl(c) index(e) index(c) */
       e.사원번호, e.사원명, e.입사일자
     , c.고객번호, c.고객명, c.전화번호, c.최종주문금액
from 사원 e, 고객 c
where c.관리사원번호 = e.사원번호
  and e.입사일자 >= '19960101'
  and e.부서코드  = 'Z123'
  and c.최종주문금액 >= 20000
```
* 인덱스 구성
  * `사원_PK` : `[사원번호]`
  * `사원_X1` : `[입사일자]`
  * `고객_PK` : `[고객번호]`
  * `고객_X1` : `[관리사원번호]`
  * `고객_X2` : `[최중주문금액]`
* 위 예시에서 사용되는 인덱스는 사원_X1, 고객_X1이다.
* p263 그림 참고


### NL 조인 튜닝 포인트
* 위 예시의 NL조인 튜닝포인트들이 있다.
* 튜닝포인트 1
  * **`사원_X1` 인덱스를 읽고 나서 테이블을 액세스 하는 부분**이다.
  * 사원테이블이 아주 많은 양의 랜덤액세스가 발생할 경우 테이블에서 부서코드에 의해 필터링되는 비율이 높을 수 있다.
  * 인덱스에서 `부서코드` 컬럼을 추가하는 방안을 고려해야 한다.
* 튜닝포인트 2
  * **`고객_X1` 인덱스 탐색 부분**이다.
  * 조인 액세스 횟수는 Outer 테이블을 읽고 필터링한 결과 건수에 의해 결정된다.
  * 예시로, `부서코드`까지 만족하는 레코드가 10만건이고, `고객_X1`의 인덱스 Depth가 3이라면 수직적 탐색 과정에서만 30만개 블록을 읽어야 하며, 수평적 스캔 과정에 추가적인 블록 I/O가 생긴다.
* 튜닝포인트 3
  * **`고객_X1` 인덱스를 읽고 고객 테이블을 액세스하는 부분**이다.
  * 최종주문금액 조건에 의해 필터링되는 비율이 높다면 `고객_X1` 인덱스에 최종주문금액 컬럼을 추가하는 방안을 고려해야 한다.
* 튜닝포인트 4
  * 맨 처음 액세스하는 **`사원_X1` 인덱스에서 얻은 결과 건수에 의해 전체 일량이 좌우**된다.
  * 따라서, `사원_X1` 인덱스를 스캔하면서 추출한 레코드가 많으면, 사원 테이블로 랜덤액세스하는 횟수, `고객_X1` 인덱스를 탐색하는 횟수, 고객테이블로 랜덤액세스하는 횟수가 많아질 수 밖에 없다.

#### 올바른 조인 메소드 선택
* OLTP 시스템에서 튜닝할 때는 일차적으로 NL조인부터 고려하는것이 올바른 순서이다.
* 성능이 느리다면 위 튜닝포인트들을 기준으로, 각 단계의 수행 일량을 분석해서 과도한 랜덤액세스가 발생하는 지점을 우선 파악해야 한다.
  * 조인순서를 변경해 랜덤액세스 발생량을 줄일 수 있는지, 더 효과적인 다른 인덱스가 있는지 등을 검토해야 하며, 인덱스 추가 및 구성 변경도 고려해본다.
* 여러 방안을 검토 후, NL 조인으로 결코 좋은 성능을 내기 어렵다고 판단되면, 소트머지 조인이나 해시조인을 검토한다.

### NL 조인 특징 요약
1. **랜덤 액세스 위주의 조인 방식**이다.
   * 대량 데이터 처리 시 매우 치명적인 한계가 있다.
2. **조인을 한 레코드씩 순차적으로 진행**한다.
   * 아무리 큰 테이블을 조인하더라도 매우 빠른 응답속도를 낼 수 있다.
   * 부분범위처리가 가능한 상황에서 더더욱 그렇다.
3. **다른 조인방식과 비교할 때 인덱스 구성 전략이 특히 중요**하다.
   * 인덱스에 따라 조인 효율이 크게 달라진다.
* 따라서 **NL 조인은 소량 데이터를 주로 처리하거나, 부분범위 처리가 가능한 시스템(OLTP)에 적합한 조인방식이다.

### NL 조인 튜닝 실습
```oracle
select /*+ ordered use_nl(c) index(e) index(c) */
       e.사원번호, e.사원명, e.입사일자
     , c.고객번호, c.고객명, c.전화번호, c.최종주문금액
from 사원 e, 고객 c
where c.관리사원번호 = e.사원번호
  and e.입사일자 >= '19960101'
  and e.부서코드 = 'Z123'
  and c.최종주문금액 >= 20000
```

#### 인덱스 튜닝 관점
* 위 예시에서, 사원 테이블에 액세스한 횟수가 많고, 테이블에서 필터링한 결과가 적을 경우 인덱스에 테이블 필터 조건인 `부서코드`를 추가하는 것을 고려할 필요가 있다.
* 하지만 인덱스 스캔 단계에서의 일량도 확인해야 한다.
* `입사일자 >= '19960101'` 조건에 해당하는 레코드가 아주 많다면, 그만큼 인덱스 블록을 스캔하면서 `부서코드 = 'Z123'` 조건을 필터링했을 것이다.
* 따라서 이럴 경우엔, 인덱스를 `[부서코드 + 입사일자]` 순으로 구성해주면 된다. (물론 쿼리에 미치는 영향도 분석이 선행되어야 함)

#### 조인 튜닝 관점
* 사원테이블을 비효율 없이 읽고 고객테이블과 조인하는 과정에서, **조인횟수는 많지만 필터링까지 마친 결과 집합은 적을 수도 있다.**
* 이럴 때는 **조인 순서 변경을 고려해볼 수 있다.**
  * 예를들면 최종주문금액 조건을 만족하는 레코드가 별로 없다면 튜닝에 성공할 가능성이 있다.
  * 하지만 그 반대의 결과가 나타날 수도 있다.(최종주문금액 단독으로 조회 시 데이터량이 훨씬 많아질 수도 있음)
* **조인 순서를 바꿔도 별 소득이 없다면, 소트 머지 조인과 해시 조인을 검토**해야 한다.

### NL 조인 확장 메커니즘
* 버전이 올라가면서 오라클은 NL조인 성능을 높이기 위해 테이블 Prefetch, 배치 I/O 기능을 도입했다.
  * **테이블 Prefetch** : 인덱스를 이용해 테이블을 액세스하다가 디스크 I/O가 필요해지면 이어서 곧 읽게 될 블록까지 미리 읽어 버퍼캐시에 적재하는 기능
  * **배치 I/O** : 디스크 I/O Call을 미뤘다가 읽을 블록이 일정량 쌓이면 한꺼번에 처리하는 기능
* 튜닝하는 과정에서 이 기능을 표현한 실행계획들을 자주 볼 수 있다.

  1) 정통적인 실행계획
     ```
     5 NESTED LOOPS
     3   TABLE ACCESS BY INDEX ROWID OF 사원
     5     INDEX RANGE SCAN OF 사원_X1
     5   TABLE ACCESS BY INDEX ROWID OF 고객
     8     INDEX RANGE SCAN OF 고객_X1
     ```

  2) 테이블 Prefetch 실행계획
     ```
     5  TABLE ACCESS BY INDEX ROWID OF 고객
     12   NESTED LOOPS
     3      TABLE ACCESS BY INDEX ROWID OF 사원
     3        INDEX RANGE SCAN OF 사원_X1
     8      INDEX RANGE SCAN OF 고객_X1
     ```
     * Outer에 대한 `TABLE ACCESS`가 위에 올라와 있고, 그 아래에 `NESTED LOOPS` 및 Outer 액세스가 들어가있다.
     * Inner 쪽 테이블에 대한 디스크 I/O 과정에 테이블 Prefetch 기능이 작동할 수 있음을 나타낸다.
     * `nlj_prefetch`, `no_nlj_prefetch` 힌트를 이용해 이 실행계획을 제어할 수 있다.

  3) 배치 I/O 실행계획
     ```
     5  NESTED LOOPS
     12   NESTED LOOPS
     3      TABLE ACCESS BY INDEX ROWID OF 사원
     3        INDEX RANGE SCAN OF 사원_X1
     8      INDEX RANGE SCAN OF 고객_X1
     5    TABLE ACCESS BY INDEX ROWID OF 고객
     ```
     * `NESTED LOOPS`가 두 번 중첩된 형태로 보인다.
     * 안쪽 `NESTED LOOPS`는 인덱스 액세스 중심, 바깥쪽 `TABLE ACCESS BY INDEX ROWID OF 고객`이 분리되어 있다.
     * 즉, `고객_X1`으로 ROWID들을 모았다가 테이블 블록 디스크 I/O를 한 번에 묶어서 처리하는 구조이다.
     * `nlj_batching`, `no_nlj_batching` 힌트를 이용해 이 실행계획을 제어할 수 있다. 

#### Prefetch / 배치 I/O의 성능 및 정렬 순서
* Inner 쪽 테이블 블록을 모두 버퍼캐시에서 읽는다면 어떤 방식으로 수행하든 성능에 차이는 없고, 데이터 출력 순서도 100% 같다.
* 하지만 **일부를 디스크에서 읽게 되면 성능에 차이가 나타날 수 있고**, 배치 I/O 실행계획이 나타날 때는 **결과집합의 정렬 순서도 다를 수 있어 주의가 필요**하다.
  * 12c에 도입된 일반 배치 I/O는 데이터 정렬 순서를 보장하지 않는다.
  * 11g부터 Inner 쪽 테이블에 작동하는 NL 조인용 배치 I/O도 결과집합의 정렬 순서를 보장하지 않는다.
* 11g부터 NL 조인 결과집합이 항상 일정한 순서로 출력되기를 원하면 배치 I/O 기능이 작동하지 못하도록 `no_nlj_batching` 힌트를 추가하거나, 맨 바깥쪽 **ORDER BY 절에 정렬 기준을 명시**해야 한다.
  * 11g에서 바깥쪽 메인 쿼리에 ORDER BY를 추가했어도 안쪽 ORDER BY를 함부로 제거하면 안된다.
  * Top N 쿼리를 구현하기 위한 것일 수도 있기 때문이다.

## 소트 머지 조인
* 조인 컬럼에 인덱스가 없을 때, **대량 데이터 조인**이어서 인덱스가 효과적이지 않을 때 옵티마이저는 **NL 조인 대신 소트 머지 조인이나 해시 조인을 선택**한다.
* 해시 조인 등장 이후 쓰임새는 줄었지만, **해시 조인을 쓸 수 없는 상황에서 대량 데이터를 조인할 때 여전히 유용**하다.

### SGA vs PGA
1) SGA
   * 여러 세션/프로세스가 공유하는 메모리이다.
   * 테이블/인덱스 블록을 캐싱하는 DB 버퍼 캐시가 SGA의 핵심적인 구성 요소이다.
   * 공유자원이기 때문에, 동시에 적븐하려는 세션들 사이에 latch, 버퍼 Lock같은 직렬화 메커니즘이 필요하다.
2) PGA
   * 오라클 서버 프로세스에 개별적으로 할당된 전용 메모리이다.
   * 정렬 결과, 해시 테이블 등 프로세스 전용 작업 메모리로 사용된다.
   * 다른 프로세스와 공유하지 않으므로 latch와 같은 동기화 메커니즘이 필요 없다.
   * PGA가 부족하면 Temp 테이블스페이스를 이용한다.

* **같은 양의 데이터를 읽더라도 PGA에서 읽는 것이 SGA에서 읽는 것보다 훨씬 빠르다.**
* **소트 머지 조인은 이 PGA를 사용**한다.

### 기본 메커니즘
* 소트 머지 조인은 이름이 의미하는것처럼 두 단계로 진행한다.
  * **소트 단계** : 양쪽 집합을 조인 컬럼 기준으로 정렬한다.
  * **머지 단계** : 정렬한 양쪽 집합을 서로 머지한다.
* 소트 머지 조인은 `use_merge` 힌트로 유도할 수 있다.
  ```oracle
  select /*+ ordered use_merge(c) */
         e.사원번호, e.사원명, e.입사일자
       , c.고객번호, e.고객명, e.전화번호, e.최종주문금액
  from 사원 e, 고객 c
  where c.관리사원번호 = e.사원번호
    and e.입사일자 >= '19960101'
    and e.부서코드 = 'Z123'
    and c.최종주문금액 >= 20000
  ```

1) 조건에 해당하는 사원 데이터를 읽어 조인컬럼인 `사원번호` 순으로 정렬한다.
   * 정렬한 결과집합은 PGA영역에 할당된 Sort Area에 저장한다.
   * PGA에 담을 수 없도록 크면 Temp 테이블스페이스에 저장한다.

2) 조건에 해당하는 고객데이터를 읽어 조인컬럼인 `관리사원번호` 순으로 정렬한다.
   * 정렬한 결과집합은 PGA영역에 할당된 Sort Area에 저장한다.
   * PGA에 담을 수 없도록 크면 Temp 테이블스페이스에 저장한다.
3) PGA에 저장한 사원 데이터를 스캔하면서, PGA에 저장된 고객데이터와 조인한다.

* 이 때 사원데이터를 기준으로 고객 데이터를 매번 Full Scan하지 않는다.
* Sort Area에 저장한 데이터 자체가 인덱스 역할을 하기 때문에, 소트 머지 조인은 조인 컬럼에 **인덱스가 없어도 사용할 수 있는 조인 방식**이다.
* 조인 컬럼에 **인덱스가 있어도 NL 조인은 대량 데이터를 조인할 때 불리**하므로, 소트 머지 조인을 사용할 수 있다.

### 소트 머지 조인이 빠른 이유
1) NL 조인
   * NL 조인의 치명적인 단점은, **대량 데이터를 조인할 때 성능이 매우 느리다**는 것이다.
     * 소트 머지 조인과 해시 조인이 개발된 이유이다.
   * NL 조인은 인덱스를 이용하는 조인 방식으로, 조인 과정에서 액세스하는 모든 블록을 랜덤 액세스 방식으로 건건이 **DB 버퍼캐시를 경유**해 읽는다.
     * 즉, 인덱스든 테이블이든 읽는 모든 블록에 래치획득 및 캐시버퍼 체인 스캔 과정을 거친다.
   * 버퍼캐시에서 찾지 못한 블록은 건건이 디스크에서 읽어 들인다.
   * 따라서 인덱스를 이용하기 때문에 인덱스 손익분기점 한계를 그대로 드러낸다.
   * 이것이 대량 데이터 조인에 NL조인이 불리한 이유이다.
2. 소트 머지 조인
   * 소트 머지 조인은 양쪽 테이블로부터 조인 대상 집합을 읽어 **PGA에 저장한 후 조인**한다.
   * PGA는 프로세스만을 위한 독립적인 메모리 공간이므로, **데이터를 읽을 때 래치 획득 과정이 없다.**
   * 소트 머지 조인이 대량 데이터 조인에 유리한 이유이다.

* 양쪽 집합에 대한 소트 연산을 추가로 수행하므로 NL조인보다 느릴 수 있다고 생각하겠지만, 이것이 오히려 소트머지 조인을 대량 데이터 조인에 유리하게 만든 핵심 요인이다.
* 하지만 **소트머지 조인도 양쪽 테이블로부터 조인 대상 집합을 읽을 때는 DB 버퍼캐시를 경유한다.**
  * 이 때 인덱스를 이용하기도 하고, 이 과정에서 생기는** 버퍼캐시 탐색 비용과 랜덤액세스 부하는 소트머지 조인도 완전히 피할 순 없다.**

### 소트 머지 조인의 주용도
* 하지만 해시 조인의 등장으로 이제 소트 머지 조인의 쓰임새는 예전만 못한데, **대부분 해시조인이 더 빠르기 때문**이다.
* 하지만 **해시조인은 조건이 등치(`=`) 조건이 아닐 때 사용할 수 없다는 단점이 있다.**
* 따라서 아래와 같은 상황에서 많이 사용된다.
  * **조인 조건식이 등치(`=`) 조건이 아닌 대량 데이터 조인**
  * **조인 조건식이 아예 없는 조인**(Cross Join, 카테시안 곱)

### 소트 머지 조인 제어하기
* 아래는 소트 머지 조인의 실행 계획 예시이다.
  ```
  0    SELECT STATEMENT Optimizer=ALL_ROWS
  1  0   MERGE JOIN
  2  1     SORT (JOIN)
  3  2       TABLE ACCESS (BY INDEX ROWID) OF '사원' (TABLE)
  4  3         INDEX (RAGE SCAN) OF '사원_X1' (INDEX)
  5  4     SORT (JOIN)
  6  5       TABLE ACCESS (BY INDEX ROWID) OF '고객' (TABLE)
  7  6         INDEX (RAGE SCAN) OF '고 객_X1' (INDEX)
  ```
* 소트 머지 조인 실행계획을 제어할 때에는 `use_merge` 힌트를 사용하면 된다.

### 소트머지 조인 특징 요약
* 소트 머지 조인은 **조인을 위해 실시간으로 인덱스를 생성하는 것과 비슷**하다.
  * 양쪽 집합을 정렬한 다음 NL 조인과 같은 방식으로 진행하지만, PGA에 저장한 데이터를 이용하기 때문에 빠르다.
  * 따라서 소트 부하만 감수한다면 건건이 버퍼캐시를 경유하는 NL 조인보다 빠르다.
* NL 조인은 조인 컬럼에 대한 인덱스 유무에 크게 영향을 받지만, 소트 머지 조인은 그렇지 않다.
  * 양쪽 집합을 개별적으로 읽고 나서 조인을 시작한다.
  * 즉, 조인 컬럼에 인덱스가 없는 상황에서 두 테이블을 읽어 조인 대상 집합을 줄일 수 있을 때 유리하다.
* **스캔 위주의 액세스 방식을 사용한다**는 점도 중요한 특징이다.
  * 하지만 모든 처리가 스캔 방식으로 이루어지진 않는다.
  * 양쪽 소스 집합으로부터 조인 대상 레코드를 찾는 데 인덱스를 이용할 수 있고, 그때는 랜덤 액세스가 일어난다. (해시 조인도 마찬가지)
* 즉 소트 머지 조인은 정렬비용을 내는 대신, 대량 조인에서의 NL 조인의 한계를 피할 수 있는 조인방식이다.
* **`=`조인이 아닌 범위 조인이나 인덱스 부재 상황에서 특히 유용**하다.

## 해시조인
* 소트 머지 조인과 해시 조인은 조인 과정에 인덱스를 이용하지 않기 때문에 대량 데이터 조인할 때 NL 조인보다 훨씬 빠르고 일정한 성능을 보인다.
* 소트 머지 조인은 항상 양쪽 테이블을 정렬하는 부담이 있는데 해시 조인은 그런 부담이 없다.
  * 정렬 대신 해시 테이블(해시 맵)을 사용한다.
* 하지만 모든 조인을 해시 조인으로 처리할 순 없다.

### 기본 메커니즘
* 해시 조인은 아래와 같이 두 단계로 진행된다.
  1) Build 단계
     * 조건에 해당하는 작은쪽 집합(Build Input)을 읽어 해시 테이블을 생성한다.
     * 조인 키를 해시 함수에 넣어 나온 해시 값을 기준으로, 해시 버킷/ 해시 체인에 로우를 연결한다.
     * 이 해시 테이블은 PGA의 Hash Area에 저장되며, 크기가 너무 커 PGA에 담을 수 없다면 Temp 테이블스페이스를 이용한다.
  2) Probe 단계
     * 큰 쪽 집합(Probe Input)을 한 로우씩 읽는다.
     * 해당 로우의 조인 키를 같은 해시 함수에 넣어 같은 해시 값을 얻고, 그 해시 값에 해당하는 해시 체인만 스캔해서 매칭되는 로우를 찾는다.

> 해시 테이블에 담기는 정보
> * 해시 테이블에는 조인 키값만 저장하지 않는다.
> * 조인 키값뿐만 아니라 SQL에 사용한 모든 컬럼을 저장한다.

* 해시 조인은 `use_hash` 힌트로 유도할 수 있다.
  ```oracle
  select /*+ ordered use_hash(c) */
         e.사원번호, e.사원명, e.입사일자
       , c.고객번호, c.고객명, c.전화번호, c.최종주문금액
  from 사원 e, 고객 c
  where c.고객사원번호 = e.사원번호
    and e.입사일자 >= '19960101'
    and e.부서코드 = 'Z123'
    and c.최종주문금액 >= 20000
  ```

### 해시 조인이 빠른 이유
* Hash Area에 생성한 해시 테이블을 이용한다는 점만 다르고, 조인 프로세싱 자체는 NL 조인과 같다.
* 하지만 결정적인 차이는, 해시 테이블이 PGA에 있고, 조인 과정이 PGA 안에서 이루어진다는 점이다.
  * NL 조인은 Outer 테이블 레코드마다 Inner 쪽 테이블 레코드를 읽기 위해 래치 획득 및 캐시버퍼 체인 스캔 과정을 반복하지만, 해시 조인은 그러한 과정 없이 PGA에서 빠르게 데이터를 탐색하고 조인할 수 있다.
* 해시 조인도 Build Input, Probe Input 각 테이블을 읽을 때는 DB 버퍼캐시를 경유하며, 이 때 인덱스를 이용하기도 한다.
  * 따라서 이 과정에서 생기는 버퍼캐시 탐색 비용과 랜덤 액세스 부하는 해시 조인도 피할 수 없다.

#### 해시 조인 vs 소트 머지 조인
* 해시 조인과 소트 머지 조인 둘 다 오퍼레이션을 PGA에서 처리한다는 공통점이 있다.

1. 소트 머지 조인
   * **양 쪽 집합을** 조인 키 기준으로 **정렬 후 PGA에 저장**한다.
   * PGA가 부족할 시 Temp를 사용하는데, 두 집합 중 하나만 커도 Temp를 사용하기 때문에 Temp에 Sort 결과를 쓰게 될 가능성이 크다.

2. 해시 조인
   * 양 쪽 집합 중 작은 집합(Build Input)만 읽어 해시 테이블을 생성한다.
   * 둘 중 하나만 PGA Hash Area에 들어가면 되기 때문에 Temp를 쓰게 될 가능성이 작다.

* 즉, 해시 조인은 NL조인처럼 **조인 과정에서 발생하는 랜덤 액세스 부하도 없고**, 소트 머지 조인처럼 **양쪽 집합을 미리 정렬하는 부하도 없다.**
  * 해시 테이블을 생성하는 비용이 수반되지만, 둘 중 작은 집합을 Build Input으로 선택하기 때문에 보통 부담이 크지 않다.
  * Temp 테이블스페이스를 쓰게 되더라도 대량 데이터 조인 시에는 일반적으로 NL 조인보다 해시 조인이 빠르다. 

### 대용량 Build Input 처리
* 모두 대용량 테이블인 T1, T2 테이블이 있고, 두 테이블 모두 대용량 테이블이여서 인메모리 해시 조인이 불가능한 상황이다.
* 이럴 때 DBMS는 아래 두 단계로 나눠서 진행한다. (분할 정복 방식)
  1) 파티션 단계
     * 조인하는 양쪽 집합의 조인 컬럼에 해시 함수를 적용하고, 반환된 해시 값에 따라 여러 개의 파티션으로 나눈다.
     * 서로 조인 가능한 짝(pair)들을 생성하는 단계이다.
     * 양쪽 집합을 읽어 디스크 Temp 공간에 저장해야 하므로 인메모리 해시 조인보다 비용이 많이 든다.
  2) 조인 단계
     * 파티션 단계를 완료하면 각 파티션 짝(pair)에 대해 하나씩 조인을 수행한다.
       * 이 때, 각각에 대한 Build Input과 Probe Input은 독립적으로 결정된다.
       * 파티션하기 전과 상관없이 각 **파티션 짝 별로 작은 쪽을 Build Input으로 선택**하고 해시 테이블을 생성한다.
     * 해시 테이블을 생성하고 나면 반대쪽 파티션 로우를 하나씩 읽으면서 해시 테이블을 탐색한다.
     * 모든 파티션 짝에 대한 처리를 마칠 때까지 이 과정을 반복한다.
* 즉, 조인해야 하는 양쪽 집합을 여러개의 작은 짝으로 자른 뒤, 짝들에 대해 작은 쪽을 Build로 선택해 해시 조인을 수행한다.

### 해시 조인 실행계획 제어
```
0    SELECT STATEMENT Optimizer=ALL_ROWS
1  0   HASH JOIN
2  1       TABLE ACCESS (BY INDEX ROWID) OF '사원' (TABLE)
3  2         INDEX (RAGE SCAN) OF '사원_X1' (INDEX)
4  3       TABLE ACCESS (BY INDEX ROWID) OF '고객' (TABLE)
5  4         INDEX (RAGE SCAN) OF '고 객_X1' (INDEX)
```
* 위 실행계획에서는 위쪽 데이터로 해시 테이블을 생성한 후, 아래쪽 테이블에서 읽은 조인 키값으로 해시 테이블을 탐색하면서 조인한다고 해석하면 된다.
* 해시 조인 실행계획을 제어할 때 `use_hash` 힌트를 사용하면 된다.
  * 이 힌트만 사용하면 Build Input을 옵티마이저가 선택하는데, 일반적으로 둘 중 조건절에 대한 카디널리티가 작은 테이블을 선택한다.
* 오라클은 기본적으로 `leading`이나 `ordered` 과같은 힌트로 지시한 순서에 따라 가장 먼저 읽는 테이블을 Build Input으로 선택한다.
* `swap_join_inputs` 힌트로 Build Input을 명시적으로 선택할 수도 있다.
  ```oracle
  select /*+ leading(e) use_hash(c) swap_join_inputs(c) */
         e.사원번호, e.사원명, e.입사일자
       , c.고객번호, c.고객명, c.전화번호, c.최종주문금액
  from 사원 e, 고객 c
  where c.고객사원번호 = e.사원번호
    and e.입사일자 >= '19960101'
    and e.부서코드 = 'Z123'
    and c.최종주문금액 >= 20000
  ```

#### 세 개 이상 테이블 해시 조인
* A, B, C 세 개의 테이블이 있을 때 이 세 개의 테이블을 조인하는 경로가 아래와 같다고 가정한다.
  * 경로1 : A와 B, B,와 C
  * 경로2 : A와 B, A 와 C
* 이 경우를 결국 정리해보면 아래와 같다.
  * A <-> B <-> C
  * B <-> A <-> C
* 결국 T1 <-> T2 <-> T3의 형태가 된다는 것이다.


#### 세 개 이상 테이블 해시조인 유도 예시
```oracle
select /*+ leading(T1, T2, T3) use_hash(T2) use_hash(T3) */
       ...
from T1, T2, T3
where T1.key = T2.key
  and T2.key = T3.key
```
* `leading` 힌트 첫 번째 파라미터로 지정한 테이블은 무조건 Build Input으로 선택된다.
* 따라서 위와 같이 힌트를 지정했을 때에는 아래와 같은 실행계획이 나올 수 있다.
  ```
  0     SELECT STATEMENT Optimizer=ALL_ROWS
   1  0    HASH JOIN
   2  1      HASH JOIN
   3  2        TABLE ACCESS (FUL) OF 'T1' (TABLE)
   4  2        TABLE ACCESS (FUL) OF 'T2' (TABLE)
   5  1      TABLE ACCESS (FULL) OF 'T3' (TABLE)
  ```
  ```
   0     SELECT STATEMENT Optimizer=ALL_ROWS
   1  0    HASH JOIN
   2  1      TABLE ACCESS (FULL) OF 'T3' (TABLE)
   3  1      HASH JOIN
   4  3        TABLE ACCESS (FUL) OF 'T1' (TABLE)
   5  3        TABLE ACCESS (FUL) OF 'T2' (TABLE)
  ```
* T2를 Build Input으로 선택하고 싶다면 아래와 같이 `swap_join_inputs`힌트를 사용하면 된다.
  ```
  select /*+ leading(T1, T2, T3) swap_join_inputs(T2) */
  ```
  ```
  0     SELECT STATEMENT Optimizer=ALL_ROWS
   1  0    HASH JOIN
   2  1      HASH JOIN
   3  2        TABLE ACCESS (FUL) OF 'T2' (TABLE)
   4  2        TABLE ACCESS (FUL) OF 'T1' (TABLE)
   5  1      TABLE ACCESS (FULL) OF 'T3' (TABLE)
  ```
  ```
   0     SELECT STATEMENT Optimizer=ALL_ROWS
   1  0    HASH JOIN
   2  1      TABLE ACCESS (FULL) OF 'T3' (TABLE)
   3  1      HASH JOIN
   4  3        TABLE ACCESS (FUL) OF 'T2' (TABLE)
   5  3        TABLE ACCESS (FUL) OF 'T1' (TABLE)
  ```
* 여기서 패턴 1을 패턴 2로 바꾸고 싶다면, T3를 Build Input으로 선택하려는 것이므로 아래와 같이 힌트를 사용하면 된다.
  ```
  select /*+ leading(T1, T2, T3) swap_join_inputs(T3) */
  select /*+ leading(T1, T2, T3) swap_join_inputs(T2) swap_join_inputs(T3) */
  ```

* 패턴 2를 패턴 1로 바꾸고 싶을 때에는, `no_swap_join_inputs` 힌트를 사용하면 된다.
  * T3를 Probe Input으로 선택해주는 방식이다.
  ```oracle
  select /*+ leading(T1, T2, T3) no_swap_join_inputs(T3) */
  ```
  * 이 힌트는 오라클 10.1.0.3 버전부터 제공한다.

### 조인 메소드 선택 기준
* 해시 조인이 워낙 빠르다 보니, **웬만하면 해시조인으로 처리하려는 유혹에 빠지기 쉽다.**
  * 인덱스 설계에 공들이지 않아도 되니 편하기까지 하기 때문에 더더욱 그렇다.
* **이러한 사고는 굉장히 위험하고, 수행빈도가 매우 높은 쿼리에 대해선 더더욱 위험하다.**
* 일반적인 조인 메소드 선택 기준은 아래와 같다.
  * **소량 데이터 조인 시** : **NL 조인**
  * **대량 데이터 조인 시** : **해시 조인**
  * **대량 데이터 조인인데 해시조인으로 처리할 수 없을 때** (조인 조건식이 등치(`=`) 조건이 아닐 때) : **소트 머지 조인**
  * 대량의 기준은, NL 조인 기준으로 최적화 했는데도 랜덤 액세스가 많아 만족할만한 성능을 낼 수 없는 경우를 의미한다.
* 수행 빈도가 매우 높은 쿼리에 대해서는 아래와 같은 기준을 적용할 수 있다.
  * NL 조인과 해시 조인 성능이 같은 경우 : NL 조인
  * 해시 조인이 약간 더 빠른경우 : NL 조인
  * NL 조인보다 해시 조인이 매우 빠른 경우 : 해시 조인

#### NL조인을 가장 먼저 고려해야 하는 이유
* NL 조인 위주로 처리하려면 인덱스를 세밀하게 설계해야 하는데에도 **NL 조인을 먼저 고려해야 하는 이유가 있다.**
* NL 조인에 사용하는 **인덱스는 영구적으로 유지**하면서 다양한 쿼리를 위해 **공유 및 재사용**하는 자료구조이다.
* 하지만 해시 테이블은 단 하나의 쿼리를 위해 생성하고 조인이 끝나면 소멸하는 자료 구조이다.
  * 같은 쿼리를 100개 프로세스가 동시에 수행하면 해시 테이블도 100개가 만들어진다.
  * 따라서 수행시간이 짧으면서 수행빈도가 매우 높은 쿼리를 해시 조인으로 처리하면 CPU와 메모리 사용률이 크게 증가하며, 또한 해시 맵을 만드는 과정에 여러 래치 경합도 발생한다.
* 해시 조인은 아래 세 가지 조건을 만족하는 SQL문에 사용해야 한다.
  * 수행빈도가 낮다.
  * 쿼리 수행 시간이 오래 걸린다.
  * 대량 데이터를 조인한다.
  * 위 세 가지 조건은 배치프로그램, DW, OLAP성 쿼리의 특징이기도 하다.
* 결과적으로, OLTP 환경에서 해시 조인을 사용할 순 있지만 3가지 조건에 해당하는지 점검해봐야 한다.
  * 예시로, **OLTP환경에서 NL 조인으로 0.1초 걸리는 쿼리를 0.01초로 단축할 목적으로 해시조인을 쓰는건 자제**해야 한다.
  * 만약 이렇게 단축해 달라는 요청이 온 경우엔 더더욱 해시조인 사용을 자제해야 한다. 
    * 이 경우엔 수행빈도가 아주 높은 쿼리임이 분명하다.

## 서브 쿼리 조인
* 옵티마이저가 서브쿼리 조인을 어떻게 처리하는지 이해하고, 원하는 방식으로 실행계획을 제어할 수 있어야 튜닝도 가능하다.
* 특히 옵티마이저는 **서브쿼리에 대해 다양한 형태로 쿼리변환을 시도**하기 때문에, 서브쿼리 조인을 이해하는 출발점을 쿼리 변환에서 찾아야 한다.

### 서브쿼리 변환이 필요한 이유
* 최근 옵티마이저는 비용을 평가하고 실행계획을 생성하기에 앞서, 사용자로부터 전달받은 SQL을 **최적화에 유리한 형태로 변환하는 작업인 쿼리 변환부터 진행**한다.
  * SQL을 여러 가지 다른 형태로 표현할 수 있고, 어떤 것을 선택하느냐에 따라 성능도 다를 수 있기 때문이다.
  * 애초에 사용자가 최적화에 유리한 형태로 SQL을 작성하면 좋지만, 그렇지 않기 때문에 옵티마이저가 대신 해주는 것이다.
  * 이로인해 옵티마이저 엔진은 점점 무거워지며 최적화에 소요되는 시간도 늘고 있다.
  * SQL 성능과 관련해 새로 개발되는 핵심 기능 대부분이 쿼리 변환 영역에 속한다.
* 서브쿼리는 하나의 SQL문 안에 괄호로 묶은 별도의 쿼리 블록을 말한다.
  * 쿼리에 내장된 또 다른 쿼리이다. 
* 서브 쿼리를 DBMS마다 조금씩 다르게 분류하는데, **오라클은 아래 세 가지로 분류**한다.
  1) **인라인 뷰(Inline View)** : FROM 절에 사용한 서브쿼리를 말한다.
  2) **중첩된 서브쿼리(Nexted Subquery)** : 결과 집합을 한정하기 위해 WHERE 절에 사용한 서브쿼리를 말한다. 
     * 이 때, 서브쿼리가 메인쿼리를 참조하는 형태를 **상관관계(Correlated) 있는 서브쿼리** 라고 부른다.
  3) **스칼라 서브쿼리(Scalar Subquery)** : 한 레코드당 정확히 하나의 값을 반환하는 서브쿼리다. 
     * 주로 SELECT-LIST에서 사용하지만, 몇가지 예외사항을 제외하면 컬럼이 올 수 있는 대부분에 위치할 수 있다.
* 서브쿼리를 참조하는 메인쿼리도 하나의 쿼리 블록이며, 옵티마이저는 쿼리 블록 단위로 최적화를 수행한다.
* **서브쿼리별로 최적화한 쿼리가 전체적으로도 최적화되었다고 볼 순 없다.**
  * 예시로, 서브쿼리 블록만 보면 좋아 보이지만, 전체 쿼리 관점에서 보면 조인 순서가 비효율적일 수 있다.
  * 따라서 전체적으로 어떻게 돌아가는지를 이해하려면, 서브쿼리를 풀어서 조인 구조로 바꾸어 볼 수 있는 시각이 필요하다.

### 서브쿼리와 조인
#### 필터 오퍼레이션
* `no_unnest` 힌트는 서브쿼리를 풀어내지 말고 그대로 수행하라고 옵티마이저에 지시하는 힌트이다.

```oracle
select c.고객번호, c.고객명
from 고객 c
where c.가입일시 >= trunc(add_months(sysdate, -1), 'mm')
  and exists (
        select /*+ no_unnest */ 'x'
        from 거래
        where 고객번호 = c.고객번호
          and 거래일시 >= trunc(sysdate, 'mm'))
```
```
0     SELECT STATEMENT Optimizer=ALL_ROWS (Cost=289 Card=1 Bytes=39)
1  0    FILTER
2  1      TABLE ACCESS (BY INDEX ROWID) OF '고객' (TABLE) (Cost=4 Card=190 ...)
3  2        INDEX (RANGE SCAN) OF '고객_X01' (INDEX) (Cost=2 Card=190)
4  1      INDEX (RANGE SCAN) OF '거래_X01' (INDEX) (Cost=3 Card=4K Bytes=92K)
```
* 위와같은 실행계획에서는 `FILTER`를 `NESTED LOOPS`로 치환하고 처리 루틴을 해석해보면 된다.
* NL 조인과 같이 부분범위 처리도 가능하다.
* 하지만, **NL 조인과의 차이점**은 아래와 같다.
  1) 멈추는 지점
     * 필터는 조건을 만족하는 로우를 하나라도 찾는 순간 진행을 멈추고, 메인쿼리의 다음 로우를 계속 처리한다.
  2) 캐싱 기능
     * 서브쿼리 입력값에 따른 반환값(true/false)을 캐싱하는 기능이 있다.
     * 이미 true/false가 캐시에 있다면 서브쿼리 자체를 다시 수행하지 않는다.
  3) 조인 순서 고정
     * 필터는 구조상 항상 메인쿼리가 드라이빙 집합이다.
     * 즉, 서브쿼리쪽 테이블을 먼저 읽고 메인 테이블을 나중에 조인할 수 없다.

#### 서브쿼리 Unnesting
* 서브쿼리 Unnesting은 **메인쿼리와 서브쿼리 간의 계층 구조를 풀어 서로 같은 레벨의 조인 구조로 만드는것**이며, 서브쿼리 Flattening 이라고 부르기도 한다.
* **서브쿼리를 그대로 두면 필터 방식을 사용할 수 밖에 없지만, Unnesting 하고 나면 일반 조인문처럼 다양한 최적화 기법을 사용할 수 있다.**
  * Unnesting된 서브쿼리는 NL 세미조인 외에도 다양한 방식으로 실행될 수 있다.
  * 또한, Unnesting된 서브쿼리는 메인쿼리보다 먼저 처리될 수 있다.
* `unnest` 힌트를 사용해 제어할 수 있다.
  ```oracle
  select /*+ leading(거래@subq) use_nl(c) */ c.고객번호, c.고객명
  from 고객 c
  where c.가입일시 >= trunc(add_months(sysdate, -1), 'mm')
    and exists (
          select /*+ qb_name(subq) unnest */ 'x'
          from 거래
          where 고객번호 = c.고객번호
            and 거래일시 >= trunc(sysdate, 'mm')) 
  ```
  ```
  0    SELECT STATEMENT Optimizer=ALL_ROWS (Cost=253K Card=190 Bytes=11K)
  1  0   NESTED LOOPS
  2  1    NESTED LOOPS (Cost=253K Card=190 Bytes=11K)
  3  2      SORT (UNIQUE) (Cost=2K Card=427K Bytes=9M)
  4  3        TABLE ACCESS (BY INDEX ROWID) OF '거래' (TABLE) (Cost=2K ...)
  5  4          INDEX (RANGE SCAN) OF '거래_X02' (INDEX) (Cost=988 Card 427K)
  6  2      INDEX (RANGE SCAN) OF '고객_X01' (INDEX) (Cost=1 Card=190)
  7  1    TABLE ACCESS (BY INDEX ROWID) OF '고객' (TABLE) (Cost=3 Card=1 ...)
  ```
  * 서브쿼리를 풀어서 조인하면, 메인쿼리 결과집합이 서브쿼리 쪽 집합수준으로 확장될 수 있으므로 서브쿼리 집합에 대한 Sort Unique 오퍼레이션이 수행되었다.
  * 즉, 서브쿼리 집합에서 고객번호 중복을 제거하기 위해 쿼리를 아래와 같이 변환한 것이다.
    ```oracle
    select /* no_merge(t) leading(t) ues_nl(c) */ c.고객번호, c.고객명
    from (select distinct 고객번호
          from 거래
          where 거래일시 >= trunc(sysdate, 'mm')) t, 고객 c
    where c.가입일시 >= trunc(add_months(sysdate, -1), 'mm')
      and c.고객번호 = t.고객번호
    ```
* 즉 서브쿼리를 **Unnesting하면 서브쿼리는 별도의 조인용 집합**이 되고, 메인 테이블과 조인할 수 있게 된다.
* 옵티마이저는 많은 조인 테크닉을 가지기 때문에 조인 형태로 변환했을 때 **필터 오퍼레이션보다 더 좋은 실행경로를 찾을 가능성이 높아진다.**

> 서브쿼리에서 ROWNUM
> * 서브쿼리를 Unnesting 하면 필터 오퍼레이션보다 더 좋은 실행경로를 찾을 가능성이 커진다.
> * 하지만 **서브쿼리에 rownum을 사용하면 이 중요한 옵티마이징 기능을 사용하지 못하게 막는 부작용**이 있다.
>   * rownum이 들어간 블록은 함부로 건드리면 의미가 바뀔 수 있기 때문에 옵티마이저 입장에선 변형하면 안되는 블록으로 보게 된다.
>   * 즉, 옵티마이저에게 이 서브쿼리 블록을 손대지 말라고 선언한것과 같다.
> * 따라서 서브쿼리에서 Unnesting을 활용하고 싶다면, 특별한 의도가 없는 한 rownum은 서브쿼리에 함부로 쓰지 않는게 좋다.

#### 서브쿼리 Pushing
* **Unnesting 되지 않은 서브쿼리는** 항상 필터 방식으로 처리되며, 보통 **실행계획 맨 마지막 단계에서 처리**된다.
* 하지만 서브쿼리 필터링을 먼저 처리함으로써 조인 단계로 넘어가는 로우 수를 크게 줄일 수 있는 상황이 생길 수 있다.
* Pushing 서브쿼리는 서브쿼리 필터링을 가능한 한 앞 단계에서 처리하도록 강제하는 기능이다.
  * `push_subq`, `no_push_subq` 힌트로 제어할 수 있다.
* 이 기능은 **Unnesting 되지 않은 서브쿼리에만 작동**한다.
  * 서브쿼리가 Unnesting 되면 필터가 아닌 다양한 조인 방식으로 실행되기 때문이다.
  * 따라서 **`push_subq` 힌트는 항상 `no_unnest` 힌트와 같이 기술하는 것이 좋다.**

### 뷰(View)와 조인
* 옵티마이저의 기본 최적화 단위가 쿼리블록이므로, 옵티마이저가 뷰 쿼리를 변환(메인쿼리와 머징)하지 않으면 뷰 쿼리 블록을 독립적으로 최적화한다.
  * 이때, 메인 쿼리의 조건이 인라인 뷰 안으로 들어가지 못한다면, 불필요하게 큰 집합을 먼저 만든 뒤 나중에 조인해 잘라내는 구조가 될 수 있다.
```oracle
select c.고객번호, c.고객명, t.평균거래, t.최소거래, t.최대거래
from 고객 c
    ,(select 고객번호, avg(거래금액) 평균거래
           , min(거래금액) 최소거래, max(거래금액) 최대거래 
      from 거래
      where 거래일시 >= trunc(sysdate, 'mm')                -- 당월 발생한 거래
      group by 고객번호) t
where c.가입일시 >= trunc(add_months(sysdate, -1), 'mm')    -- 전월 이후 가입 고객
  and t.고객번호 = c.고객번호
```
  * 위 예시에서 당월 `거래` 전체를 읽어 고객번호 수준으로 Group By 하는 실행계획이 수립될 수 있다.
  * **문제는** `고객` 테이블에서 전월 이후 가입한 `고객`을 **필터링하는 조건이 인라인 뷰 바깥**에 있다는 점이다.
  * 즉 이 조건이 있는데에도 **인라인 뷰 안에서는 당월에 거래한 모든 `고객`의 거래 데이터를 읽어야 한다.**
* `merge`힌트를 이용해 뷰를 메인 쿼리와 머징하도록 할 수 있다. (뷰 머징 방지엔 `no_merge` 힌트)
  ```oracle
  select c.고객번호, c.고객명, t.평균거래, t.최소거래, t.최대거래
  from 고객 c
      ,(select /*+ merge */ 고객번호, avg(거래금액) 평균거래
             , min(거래금액) 최소거래, max(거래금액) 최대거래 
        from 거래
        where 거래일시 >= trunc(sysdate, 'mm')                -- 당월 발생한 거래
        group by 고객번호) t
  where c.가입일시 >= trunc(add_months(sysdate, -1), 'mm')    -- 전월 이후 가입 고객
    and t.고객번호 = c.고객번호
  ```
  * 위와같이 변경한 경우, 아래와 같은 실행 계획을 선택할 수 있다.
    * 인덱스를 이용해 전월 이후 가입한 `고객`만 읽는다.
    * 그 `고객`들의 고객번호만 가지고 `거래` 테이블과 조인한다.
    * 조인된 `거래`에 대해서만 당월 조건과 Group By를 수행한다. 
  * 단점은 조인에 성공한 전체 집합을 Group By 하고서야 데이터를 출력할 수 있다는 점이다.
    * 부분범위 처리가 불가능하다는 의미이다.
    * 만약 전월 이후 가입한 `고객`이 매우 많고 당월 `거래`도 매우 많다면 부분범위 처리 불가능한 상황에서 NL 조인은 좋은 선택은 아니며 해시 조인이 빠르다.

#### 조인 조건 Pushdown
* 오라클 11g 이후로 조인 조건 Pushdown이라는 쿼리 변환 기능이 작동한다.
* 메인 쿼리를 실행하면서 조인 조건절 값을 건건이 뷰 안으로 밀어 넣는 작업이다.
* 실행계획에서 `VIEW PUSHED PREDICATE` 오퍼레이션을 통해 이 기능의 작동 여부를 알 수 있다.
* 이 기능을 제어하는 힌트는 `push_pred`이며, 옵티마이저가 뷰를 머징하면 힌트가 작동하지 않기 때문에 `no_merge` 힌트를 함께 사용하는게 좋다.
  ```oracle
  select c.고객번호, c.고객명, t.평균거래, t.최소거래, t.최대거래
  from 고객 c
      ,(select /*+ no_merge push_pred */ 
               고객번호, avg(거래금액) 평균거래
             , min(거래금액) 최소거래, max(거래금액) 최대거래 
        from 거래
        where 거래일시 >= trunc(sysdate, 'mm')                -- 당월 발생한 거래
        group by 고객번호) t
  where c.가입일시 >= trunc(add_months(sysdate, -1), 'mm')    -- 전월 이후 가입 고객
    and t.고객번호 = c.고객번호
  ```
  ```
  0    SELECT STATEMENT Optimizer=ALL_ROWS (Cost=4 Card=1 Bytes=61)
  1  0   NESTED LOOPS (Cost=4 Card=1 Bytes=61)
  2  1     TABLE ACCESS (BY INDEX ROWID BATCHED) OF '고객' (TABLE) (Cost=2 ...)
  3  2       INDEX (RANGE SCAN) OF '고객_X01' (INDEX) (Cost=1 Card=1)
  4  1     VIEW PUSHED PREDICATE (Cost=2 Card=1 Bytes=7)
  5  4       SORT (GROUP BY) (Cost=2 Card=1 Bytes=7)
  6  5         TABLE ACCESS (BY INDEX ROWID BATCHED) OF '거래' (TABLE) (Cost=2 ...)
  7  6           INDEX (RANGE SCAN) OF '거래_X02' (INDEX) (Cost=1 Card=5)    
  ```
  * 이 방식을 사용하면 전월 이후 가입한 `고객`을 대상으로 건건이 당월 `거래` 데이터만 읽어서 조인하고 Group By를 수행한다.
  * 중간에 멈출수도 있으며, **부분범위 처리가 가능**하다.
* 부분범위 처리 가능한 NL 구조를 유지하면서도, 인라인 뷰의 작업 범위를 줄이고 싶을 때 유용하다.


> Lateral 인라인 뷰, Cross/Outer Apply 조인
> * 인라인 뷰 안에서 메인쿼리 테이블 컬럼을 참조하면 ORA-00904(invalid identifier) 에러가 발생한다.
> * 오라클 12c부터 인라인 뷰를 Lateral로 선언하면 인라인 뷰 안에서 메인쿼리 테이블의 컬럼을 참조할 수 있다.
> * Outer 조인이 필요하면 12c의 Outer Apply 조인 구문을 사용할 수도 있다.
> * 12c에서는 Cross Apply 조인 구문도 지원하며,구문만 다를 뿐 Lateral 인라인 뷰와 동일하다.
> * 하지만 이러한 기능들은 유용해 보이지만 사용해야 할 이유를 찾기 힘들다
>   * **최근 오라클 버전에선 조인 조건 Pushdown 기능이 잘 동작**한다.
>   * 오히려 개발자들이 이러한 구문을 남용함으로 얽히고 설킨 복잡한 쿼리만 생겨날 수 있다.
> * 즉, **기본적으로 Lateral 인라인 뷰, Cross/Outer Apply 조인을 쓸 이유는 없다.**
> * 튜닝 과정에 알 수 없는 이유로 조인 조건 Pushdown 기능이 잘 작동하지 않을 때만 활용하면 된다.

### 스칼라 서브쿼리 조인
#### 스칼라 서브쿼리의 특징
* SELECT-LIST에 사용한 함수는, 함수 안에 있는 SELECT 쿼리를 메인쿼리 건수만큼 재귀적으로 반복 실행한다.
* 하지만 스칼라 서브쿼리는 건수만큼 해당 테이블을 반복해서 읽으며, 레코드마다 정확히 하나의 값만 반환한다.
  * 함수처럼 재귀적으로 실행하는 구조는 아니며, Outer 조인문처럼 하나의 문장으로 이해한다.
  * 조인과 차이가 있다면 스칼라 서브쿼리는 처리 과정에서 캐싱 작용이 일어난다는 것이다.

#### 스칼라 서브쿼리 캐싱 효과
* 스칼라 서브쿼리로 조인하면 오라클은 조인 횟수를 최소화하려고 입력 값과 출력 값을 내부 캐시에 저장해 둔다.
  * 스칼라 서브쿼리의 입력 값은, 서브쿼리 안에서 참조하는 메인쿼리의 컬럼 값이다.
  * 조인할 때마다 일단 캐시에서 입력값을 찾아보고 찾으면 저장된 출력값을 반환한다.
  * 캐시에서 찾지 못할 때만 조인을 수행하며, 결과는 버리지 않고 캐시에 저장해둔다.
* 스칼라 서브 쿼리 캐싱은 필터 서브쿼리 캐싱과 같은 기능이며, 조인 성능을 크게 올릴 수 있다.
  * 메인쿼리 집합이 아무리 커도 조인할 데이터를 대부분 캐시에서 찾는다면 조인 수행횟수를 최소화할 수 있다.
* 캐싱은 쿼리 단위로 이루어지며, 쿼리를 시작할 때 PGA 메모리에 공간을 할당하고 쿼리를 수행하면서 공간을 채워나가며 쿼리를 마치는 순간 공간을 반환한다.

* 아래 예시는, SELECT-LIST에서 사용한 함수에 캐싱 효과를 적용하는 법이다.
  ```oracle
  select empno, ename, sal, hiredate
       , (select GET_DNAME(e.deptno) from dual) dname
  from emp e
  where sal >= 2000
  ```
  * 위 예시와 같이 스칼라 서브쿼리를 덧씌우면 서브쿼리 캐싱효과로 인해 호출 횟수를 최소화할 수 있다.
  * 함수에 내장된 SELECT 쿼리도 그만큼 덜 수행된다.

#### 스칼라 서브쿼리 캐시 부작용
* 모든 캐시가 다 그렇듯이, 캐시 공간은 늘 부족하다.
* 스칼라 서브쿼리에 사용하는 캐시도 매우 작은 메모리 공간이다.
* 스칼라 서브쿼리 캐싱 효과는, **입력 값의 종류가 소수여서 해시 충돌 가능성이 작을 때 효과가 있다.**
  * 반대의 경우라면 캐시를 매번 확인하는 비용으로 인해 성능이 나빠지고 CPU 사용률만 높게 만들며, 메모리도 더 사용한다.

1) 입력값 종류가 적은 경우 (O)
   ```oracle
   select 거래번호, 고객번호, 영업조직ID, 거래구분코드
        , (select 거래구분명 from 거래구분 where 거래구분코드 = t.거래구분코드) 거래구분명
   from 거래 t
   where 거래일자 >= to_char(add_months(sysdate, -3), 'yyyymmdd') -- 50,000건
   ```
   * 위 예시에서, `거래구분코드`가 20개의 값이 존재한다면 **효율적인 캐싱**이 일어난다.

2) 입력값 종류가 너무 많은 경우 (X)
   ```oracle
   select 거래번호, 고객번호, 영업조직ID, 거래구분코드
        , (select 고객명 from 고객 where 고객번호 = t.고객번호) 거래구분명
   from 거래 t
   where 거래일자 >= to_char(add_months(sysdate, -3), 'yyyymmdd') -- 50,000건
   ```
   * 위 예시에서, `고객`이 매우 많다면 메인 쿼리에서 50000개의 `거래`를 읽는 동안 매번 탐색하지만, 대부분은 찾지 못해 결국 조인을 해야 한다.
   * 불필요한 캐시 탐색으로 인해 **일반 조인문보다 느리고 불필요하게 자원만 낭비**하게 된다.

3) 입력값 종류가 너무 적은경우 (X)
  ```oracle
  select 계좌번호, 계좌명, 고객번호, 개설일자, 계좌종류구분코드
       , (select brch_nm(관리지점코드) from dual) 관리지점명
       , (select brch_nm(개설지점코드) from dual) 개설지점명
  from 계좌
  where 고객번호 = :고객번호
  ```
  * 위 예시에서는 고객당 계좌가 많지 않고 보통은 한 개일 것이기 때문에 쓰지도 않을 캐시를 할당해 값을 채웠다가 바로 버리는 결과가 생긴다.
  * 즉, **메인쿼리 집합이 매우 작은 경우에는 스칼라 서브쿼리 캐싱이 성능에 도움을 주지 못한다.**

#### 두 개 이상의 값 반환
* 스칼라 서비쿼리에는 가장 큰 제약은, **두 개 이상의 값을 반환할 수 없다**는 제약이다.
* 이러한 문제를 해결하기 위한 오래된 튜닝 패턴 중 하나는, 아래와 같이 여러 문자열로 합쳐서 한번에 가져오고 바깥에서 분리하는 방식이다.
  ```oracle
  select 고객번호, 고객명
       , to_number(substr(거래금액, 1, 10)) 평균거래금액
       , to_number(substr(거래금액, 11, 10)) 최소거래금액
       , to_number(substr(거래금액, 21)) 최대거래금액
  from (
      select c.고객번호, c.고객명
           , (select lpad(avg(거래금액), 10) || lpad(min(거래금액), 10) || max(거래금액)
              from 거래
              where 거래일시 >= trunc(sysdate, 'mm')
                and 고객번호 = c.고객번호) 거래금액
      from 고객 c
      where c.가입일시 >= trunc(add_months(sysdate, -1), 'mm')
  )
  ```
  * 또한 오브젝트 **TYPE을 사용하는 방법도 있지만**, TYPE을 미리 선언해두어야 하기 때문에 **불편해서 잘 쓰이지 않는다.**
* 두 개 이상 값을 반환하고 싶을 때에는 인라인 뷰를 사용하면 편하긴 하다.
  ```oracle
  select c.고객번호, c.고객명, t.평균거래, t.최소거래, t.최대거래
  from 고객 c
      , (select 고객번호, avg(거래금액) 평균거래
              , min(거래금액) 최소거래, max(거래금액) 최대거래
         from 거래
         where 거래일시 >= trunc(sysdate, 'mm')
         group by 고객번호) t
  where c.가입일시 >= trunc(add_months(sysdate, -1), 'mm')
    and t.고객번호(+) = ㅊ.rhrorqjsgh
  ```
  * 하지만 뷰를 사용하면, 뷰가 머징되지 않았을 때 당월 거래 전체를 읽어야 하거나, 뷰가 머징될 때 Group By 때문에 부분범위처리가 안되는 문제가 있다.
* 따라서 두 개 이상의 값을 반환해야 할 때 스칼라 서브쿼리와 인라인 뷰 사이에서 많은 고민을 하던 시기가 있었다.
  * 11g 이후로는 조언 조건 Pushdown 기능이 잘 작동하므로 **인라은 뷰를 마음편히 사용**할 수 있게 되었다.

#### 스칼라 서브쿼리 Unnesting
* **스칼라 서브쿼리도 NL 방식으로 조인**하므로 **캐싱효과가 크지 않으면 랜덤 I/O 부담**이 있다.
* 그래서 다른 조인방식을 선택하기 위해 스칼라 서브쿼리를 일반 조인문으로 변환하고 싶을 때가 있다.
  * 대량/병렬 쿼리에선 될 수 있으면 스칼라 서브쿼리를 사용하지 않아야 한다.
  * 대량 데이터를 처리하는 병렬 쿼리는 해시 조인으로 처리해야 효과적이기 때문이다.
* 사용자가 직접 쿼리를 변환해야 하는 상황에서, 길고 복잡한 스칼라 서브쿼리가 여러 개 섞여 있으면 직접 조인문으로 풀어 쓰는 것은 매우 번거롭고 실수 위험도 크다.
* 오라클 12c부터 스칼라 서브쿼리도 Unnesting이 가능해졌는데, 옵티마이저가 스칼라 서브쿼리를 일반조인 형태로 변환해 줄 수 있다.
* 하지만 스칼라 서브쿼리 Unnesting이 항상 좋은 결과만 주는 것은 아니여서, 특정 쿼리에서는 예상치 못한 성능 문제가 생기기도 했다.
  * `_optimizer_unnest_scalar_sq` 파라미터를 false로 설정함으로써 일단 문제를 해결하거나, `no_unnest` 힌트를 이용해 부분적으로 해결하기도 했다.
  * `_optimizer_unnest_scalar_sq` 파라미터를 true로 설정하면, 스칼라 서브쿼리를 Unnesting 할 지 여부를 옵티마이저가 결정한다.
    * false로 설정하면 옵티마이저가 이 기능을 사용하진 않지만, 사용자가 `unnest`힌트로 유도할 순 있다.

# 소트 튜닝
## 소트 연산에 대한 이해
* SQL 수행 도중, 가공된 데이터 집합이 필요할 때 오라클은 **PGA의 Sort Area와 Temp 테이블스페이스를 활용**한다.
  * 소트 머지 조인, 해시 조인, 데이터 소트와 그룹핑 등이 대표적이다.

### 소트 수행 과정
* 소트는 기본적으로 **PGA에 할당한 Sort Area에서 이루어진다.**
* 메모리 공간인 Sort Area가 **다 차면 디스크 Temp 테이블스페이스를 활용**한다.
* Sort Area에서 작업을 완료할 수 있는 지에 따라 소트를 두가지 유형으로 나눈다.
1) 메모리 소트(In-Memory Sort / Internal Sort)
   * 전체 데이터의 정렬 작업을 메모리 내에서 완료한다.
2) 디스크 소트(To-Disk Sort / External Sort)
   * 할당받은 Sort Area 내에서 정렬을 완료하지 못해 디스크 공간까지 사용한다.
   * 디스크 소트 과정은 아래와 같다.
     * 소트할 대상 집합을 SGA 버퍼캐시를 통해 읽어오고, 일차적으로 Sort Area에서 정렬을 시도한다.
     * 양이 많을 경우 정렬된 중간 집합을 Temp 테이블스페이스에 임시 세그먼트를 만들어 저장한다.
       * 이 중간 정렬 결과를 Sort Run 이라고 부른다.
     * 여러 개의 Sort Run을 다시 읽어 Merge해 최종 정렬 결과를 만든다.
       * 각 Sort Run 내에서는 이미 정렬된 상태이므로 Merge과정은 어렵지 않다.

* **소트 연산은 메모리 집약적일 뿐만 아니라, CPU 집약적**이기도 하다.
* 처리할 데이터량이 많을 때는 디스크 I/O까지 발생하므로, 쿼리 성능을 좌우하는 매우 중요한 요소이다.
* 또한, 소트는 **부분범위 처리를 불가능하게 함으로써 OLTP 환경에서 애플리케이션 성능을 저하시키는 주 요인이 되기도 한다.**
* 따라서 가능하면 불필요한 소트를 만들지 말고, 불가피한 소트라면 메모리 내에서 수행을 완료할 수 있도록 해야 한다.

### 소트 오퍼레이션
#### Sort Aggregate
* Sort Aggregate는 전체 로우를 대상으로 집계를 수행할 때 나타난다.
* **Sort 라는 표현을 사용하지만 실제로 데이터를 정렬하진 않고, Sort Area를 사용한다는 의미**로 이해하면 된다.
```oracle
select deptno, sum(sal), max(sal), min(sal), avg(sal) from emp 
```
* 데이터를 정렬하지 않고 SUM, MAX, MIN, AVG 값을 구하는 법
  * Sort Area에 SUM, MAX, MIN, COUNT 값을 위한 변수를 각각 하나씩 할당한다.
  * 테이블 첫 레코드에서 읽은 해당 컬럼의 값을 SUM, MAX, MIN 변수에 저장하고, COUNT 변수에는 1을 저장한다.
  * 테이블에서 레코드를 하나씩 읽어내려가며 SUM 변수에는 값을 누적하고, MAX 변수에는 기존보다 큰 값이 나타날 때만 대체하고, MIN 변수도 기존보다 작은 값이 나타날 때만 값을 대체하며, COUNT 변수에는 NULL이 아닌 레코드를 만날때마다 1씩 증가시킨다.
  * 레코드를 다 읽고 나면 SUM, MAX, MIN, COUNT 변수에 각각 해당하는 값들이 저장되어 있으며, 그 값을 그대로 출력하면 되고 AVG는 SUM 값을 COUNT로 나눈 값을 출력한다.

#### Sort Order By
* Sort Order By는 **데이터를 정렬할 때** 나타난다.
```oracle
select * from emp order by sal desc 
```

#### Sort Group By
* **Sort Group By는 소팅 알고리즘을 이용해 그룹별 집계를 수행할 때** 나타난다.
```oracle
select deptno, sum(sal), max(sal), min(sal), avg(sal)
from emp
group by deptno
order by deptno
```
* 즉, 위 예시에서 `deptno`을 기준으로 Sort Area에 그룹별 작업할 공간을 만들고, 그룹별 집계 값을 채워 나간다.

> 그룹핑 결과의 정렬 순서
> * **Sort Group By는 정렬이 보장되지 않는다.**
> * 실제로는 **소팅 알고리즘을 사용해 값을 집계한다는 뜻일 뿐이고, 결과의 정렬을 의미하진 않는다.**
> * Order By 절이 없으면 오라클 입장에선 반드시 정렬된 순서로 출력할 의무가 없다.
> * 따라서 실행계획에 Sort Group By라고 표시되더라도, **정렬이 필요하면 반드시 Order By를 명시해야 한다.**

#### Hash Group By
* 오라클 10gR2 버전에서 도입되었으며, Group By 절 뒤에 Order By 절을 명시하지 않으면 대부분 Hash Group By 방식으로 처리한다.
```oracle
select deptno, sum(sal), max(sal), min(sal), avg(sal)
from emp
group by deptno
```
* Sort Group By가 소트 알고리즘으로 그룹 공간을 관리했다면, **Hash Group By는 해싱 알고리즘으로 그룹 버킷을 관리**한다.

#### Sort Unique
* **중복 레코드를 제거할 때** 나타나는 오퍼레이션이다.
* Unnesting된 서브쿼리가 M쪽 집합이면(혹은 1쪽 집합이더라도 조인 컬럼에 Unique 인덱스가 없으면) 메인 쿼리와 조인하기 전 중복 레코드부터 제거해야 한다. 
  * 이럴 때 Sort Unique 오퍼레이션이 나타난다.
* 만약 PK/Unique 제약 또는 Unique 인덱스를 통해 **Unnesting된 서브쿼리의 유일성이 보장된다면, Sort Unique 오퍼레이션은 생략**된다.
* Union, Minus, Intersect 같은 집합(Set) 연산자를 사용할 때도 Sort Unique 오퍼레이션이 나타난다.
* Distinct 연산자를 사용해도 Sort Unique 오퍼레이션이 나타난다.
  * 오라클 10gR2부터는 Distinct 연산에도 Hash Unique 방식을 사용하는데, Order By를 생략할 때 그렇다.
    
#### Sort Join
* **Sort Join 오퍼레이션은 소트 머지 조인을 수행할 때** 나타난다.

#### Window Sort
* **Window Sort는 윈도우 함수(분석함수)를 수행할 때** 나타난다.
```oracle
select empno, ename, job, mgr, sal
     , avg(sal) over (partition by deptno)
from emp
```
* `over (partition by deptno ...)` 구간을 처리하기 위해, 파티션 기준 또는 파티션 + order 기준으로 데이터를 정리하고, 그 위에서 윈도우 함수를 계산하기 위해 Window Sort가 사용된다.

## 소트가 발생하지 않도록 SQL 작성
* **Union, Minus, Distinct 연산자는** 중복 레코드를 제거하기 위한 **소트 연산을 발생**시키기 때문에 곡 필요한 경우에만 사용해야 한다.

### Union vs Union All
* **Union을 사용하면** 옵티마이저는 상단과 하단 두 집합 간 중복을 제거하려고 **소트 작업을 수행**한다.
* 하지만 **Union All은** 중복을 확인하지 않고 두 집합을 단순히 결합하기 때문에 **소트 작업을 수행하지 않는다.**
* 따라서 될 수 있으면 Union All을 사용해야 한다.
* 하지만 자칫 결과 집합이 달라질 수 있기 때문에 Union All로 무작정 변경하면 안된다.
  * 데이터 모델에 대한 이해와 집합적 사고가 필요하다.
  * 그렇지 않으면 알 수 없는 데이터 중복, 혹시 모를 데이터 중복을 우려해 중복 제거용 연산자를 불필요하게 자주 사용하게 될 수 있다.

### Exists 활용
* 중복 레코드를 제거할 목적으로 Distinct를 종종 사용하는데, **Distinct를 사용하면 조건에 해당하는 데이터를 모두 읽어서 중복을 제거**해야 한다.
  * **부분범위 처리는 당연히 불가능**하고, 모든 데이터를 읽는 과정에 **많은 I/O가 발생**한다.
```oracle
select DISTINCT p.상품번호, p.상품명, ...
from 상품p, 계약 c
where p.상품유형코드 = :pclscd
  and c.상품번호 = p.상품번호
  and c.계약일자 between :dt1 and :dt2
  and c.계약구분코드 = :ctpcd
```
* Exists를 사용하면 이러한 문제를 해결할 수 있다.
```oracle
select p.상품번호, p.상품명, ...
from 상품 p
where EXISTS(select 'x' from 계약 c
             where c.상품번호 = p.상품번호
               and c.계약일자 between :dt1 and :dt2
               and c.계약구분코드 = :ctpcd)
```
* 위 예시를 보면 Exists 서브쿼리를 통해 데이터 존재여부만 확인하면 되기 때문에 만족하는 데이터를 모두 읽지 않는다.
* 이와 같이 **Distinct, Minus 연산자를 사용한 쿼리는 대부분 Exists 서브쿼리로 변환 가능**하다.

### 조인 방식 변경
* 조인문일 때는 조인 방식도 잘 선택해줘야 한다.
```oracle
select c.계약번호, c.상품코드, p.상품명, p.상품구분코드, c.계약일시, c.계약금액
from 계약 c, 상품 p
where c.지점ID = :brch_id
  and p.상품코드 = c.상품코드
order by c.계약일시 desc
```
```
0    SELECT STATEMENT Optimizer=ALL_ROWS
1  0   SORT (ORDER BY)
2  1     HASH JOIN
3  2       TABLE ACCESS (FULL) OF '상품' (TABLE)
4  2       TABLE ACCESS (BY INDEX ROWID) OF '계약' (TABLE)
5  4         INDEX (RAGE SCAN) OF '계약_X01' (INDEX)
```
* 위 예시는 `계약_X01` 인덱스가 `[지점ID + 계약일시]` 순이면 소트 연산을 생략할 수 있지만, 해시 조인이기 때문에 Sort Order By가 나타난 예시이다.
* 즉 이러한 상황에서는, 아래와 같이 NL조인하도록 조인방식을 변경하면 소트 연산을 생략할 수 있으며, `지점ID` 조건을 만족하는 데이터가 많고 부분범위 처리 가능한 상황에서 큰 성능 개선 효과를 얻을 수 있다.
```oracle
select /*+ leading(c) use_nl(p)*/
       c.계약번호, c.상품코드, p.상품명, p.상품구분코드, c.계약일시, c.계약금액
from 계약 c, 상품 p
where c.지점ID = :brch_id
  and p.상품코드 = c.상품코드
order by c.계약일시 desc
```
```
0    SELECT STATEMENT Optimizer=ALL_ROWS
1  0   NESTED LOOPS
2  1     NESTED LOOPS
3  2       TABLE ACCESS (BY INDEX ROWID) OF '계약' (TABLE)
4  3         INDEX (RAGE SCAN DESCENDING) OF '계약_X01' (INDEX)
5  2       INDEX (UNIQUE SCAN) OF '상품_PK' (INDEX (UNIQUE))
6  1     TABLE ACCESS (BY INDEX ROWID) OF '상품' (TABLE)
```

## 인덱스를 이용한 소트 연산 생략
* **인덱스의 정렬된 상태를 이용하면 Order By 또는 Group By 절이 있어도 소트 연산을 생략할 수 있다.**
* **여기에 Top N 쿼리 특성을 결합하면 OLTP 시스템에서 대량 데이터를 조회할 때 매우 빠른 응답 속도를 낼 수 있다.**

### Sort Order By 생략
```oracle
select 거래일시, 체결건수, 체결수량, 거래대금
from 종목거래
where 종목코드 = 'KR123456'
order by 거래일시
```
* 위 예시에서, 인덱스 선두 컬럼을 `종목코드 + 거래일시` 순으로 구성하면 소트 연산을 생략할 수 있다.
  * 즉, 실행계획에서 Sort Order By 오퍼레이션이 생략된다.
* 또한 소트 연산을 생략함으로써, 전체 레코드를 읽지 않고도 바로 결과집합 출력을 시작할 수 있게 된다.
  * 즉, 부분범위 처리 가능한 상태가 되었다.

> 부분범위 처리 아직도 유효한가?
> * 요즘 DB 애플리케이션은 대부분 3-Tier 환경에서 작동하므로 부분범위 처리는 의미 없다고 생각할 수 있다.
> * 단위 작업을 마치면 DB 커넥션을 바로 커넥션 풀에 반환해야 하므로, 그 전에 쿼리 조회 결과를 클라이언트에게 모두 전송하고 커서를 닫아야만 하기 때문이다.
> * 하지만 **부분범위 처리 원리는 이러한 3-Tier 환경에서도 여전히 유효한데, 바로 Top N 쿼리 때문**이다.

### Top N 쿼리
* Top N 쿼리는 **전체 결과집합 중 상위 N개 레코드만 선택하는 쿼리**이다.
* SQL Server나 Sybase는 Top N 쿼리를`TOP 10`과 같이, IBM DB2는 `FETCH FIRST 10 ROWS ONLY`와 같이, 오라클은 인라인 뷰로 한번 감싸고 `where rownum <= 10`과 같이 사용할 수 있다.
```oracle
select * from (
    select 거래일시, 체결건수, 체결수량, 거래대금
    from 종목거래
    where 종목코드 = 'KR123456'
      and 거래일시 >= '20180304'
    order by 거래일시
)
where rownum <= 10
```
* 여기에 소트 연산을 생략할 수 있는 인덱스(위 예시에선, `종목코드 + 거래일시`)를 사용하면, 옵티마이저는 소트 연산을 생략하고 인덱스를 스캔하다 10개의 레코드를 읽는 순간 바로 멈춘다.
```
0    SELECT STATEMENT Optimizer=ALL_ROWS
1  0   COUNT (STOPKEY)
2  1     VIEW
3  2       TABLE ACCESS (BY INDEX ROWID) OF '종목거래' (TABLE)
4  3         INDEX (RANGE SCAN) OF '종목거래_PK' (INDEX (UNIQUE))
```
* 위 실행계획을 보면, Sort Order By 오퍼레이션은 보이지 않고 `COUNT(STOPKEY)`가 보인다.
* **`COUNT(STOPKEY)`는 조건절에 부합하는 레코드가 많아도 그 중 `ROWNUM`으로 지정한 건수만큼 결과 레코드를 얻고 거기서 멈춘다는 뜻**이다.
* 이러한 기능을 **TOP N Stopkey 알고리즘**이라 한다.

#### 페이징 처리
* Top N 쿼리를 이용해 3-Tier 환경에서 페이징 처리를 할 수 있다.
```oracle
select *
from (
    select rownum no, a.*
    from
        (
         /* SQL Body */
        ) a
    where rownum <= (:page * 10)
)
where no >= (:page-1)*10 + 1
```
* 3-Tier 환경에서 부분범위 처리를 활용하기 위해 해야할 일은 다음과 같다.
  * **부분범위 처리 가능하도록 SQL을 작성**하고, 부분범위 처리가 잘 작동하는지 쿼리 툴에서 테스트한다.
  * 작성한 SQL 문을 페이징 처리용 표준 패턴 SQL Body에 붙여넣는다.
* 부분범위 처리가 가능하도록 SQL을 작성한다는 것은 아래와 같은 것을 의미한다.
  * **인덱스를 사용 가능하도록 조건절을 작성**한다.
  * **조인은 NL 조인 위주로 처리**한다. (룩업을 위한 작은 테이블은 해시조인 Build Input으로 처리해도 됨)
  * **Order By 절이 있어도 소트 연산을 생략할 수 있도록 인덱스를 구성**한다.

#### 페이징 처리 ANTI 패턴
```oracle
select *
from (
    select rownum no, a.*
    from
        (
         /* SQL Body */
        ) a
)
where no between (:page-1)*10 + 1 and (:page * 10)
```
* 간혹 간결하게 표현하고 싶어서 `ROWNUM` 조건 없이 위와같은 식으로 페이징 처리를 하곤 한다.
* 하지만 **`ROWNUM` 조건이 없어지면 Top N Stopkey 알고리즘이 동작하지 않아**, 실행계획 상에서도 `COUNT` 옆에 `(STOPKEY)`가 없을 것이다.
  * 즉 이런 경우에 SQL 툴에서 해당 쿼리를 확인해보면, 첫 번째 페이지는 금방 출력되긴 하지만 하드디스크가 계속 돌아가는 것을 확인할 수 있다. 

### MIN/MAX 구하기
* MIN/MAX를 구하는 SQL 실행계획을 보면, 아래와 같이 Sort Aggregate 오퍼레이션이 나타난다.
* **Sort Aggregate를 위해 전체 데이터를 정렬하진 않지만, 전체 데이터를 읽으면서 값을 비교**한다.
* **인덱스는 정렬되어 있으므로**, 이를 이용하면 전체 데이터를 읽지 않고도 최솟값 최댓값을 쉽게 찾을 수 있다.**

#### 인덱스 이용해 MIN/MAX를 구하기 위한 조건
* 데이터를 읽지 않고 **인덱스만을 이용해 MIN/MAX를 구하려면** 조건절 컬럼과 MIN/MAX 함수인자 컬럼이 모두 인덱스에 포함되어 있어야 한다.
  * 즉 테이블 액세스가 발생하지 않아야 한다.
* 위와같은 조건에 만족한 경우, 실행계획에 `FIRST ROW`와 같은 오퍼레이션이 나타난다.
* **`FIRST ROW`는 조건을 만족하는 레코드 하나를 찾았을 때 바로 멈춘다는 것을 의미**하며, 이것을 **First Row Stopkey 알고리즘**이라고 한다.

* 인덱스 구성에 따른 실행계획의 차이
  1) ```oracle
     -- 인덱스 : DEPTNO + MGR + SAL
     SELECT MAX(SAL) FROM EMP WHERE DEPTNO = 30 AND MGR = 7698;
     ```
     ```
     0    SELECT STATEMENT Optimizer=ALL_ROWS (Cost=1 Card=1 Bytes=8)
     1  0  SORT (AGGREGATE) (Card=1 Bytes=8)
     2  1    FIRST ROW (Cost=1 Card=1 Bytes=8)
     3  2      INDEX(RANGE SCAN (MIN/MAX)) OF 'EMP_X1' (INDEX) (Cost=1 Card=1 ...)
     ```
     * `DEPTNO`, `MGR`을 만족하는 가장 큰 `SAL`의 레코드를 하나 읽으면 끝이다.
     * 선두컬럼들이 WHERE 조건과 잘 맞고, 마지막에 `SAL`이 있기 때문에 조건 구간 내에서 `SAL의 MAX를 바로 찾는 구조이다.
  2) ```oracle
     -- 인덱스 : DEPTNO + SAL + MGR
     SELECT MAX(SAL) FROM EMP WHERE DEPTNO = 30 AND MGR = 7698;
     ```
     ```
     0    SELECT STATEMENT Optimizer=ALL_ROWS (Cost=1 Card=1 Bytes=8)
     1  0  SORT (AGGREGATE) (Card=1 Bytes=8)
     2  1    FIRST ROW (Cost=1 Card=1 Bytes=8)
     3  2      INDEX(RANGE SCAN (MIN/MAX)) OF 'EMP_X1' (INDEX) (Cost=1 Card=1 ...)
     ```
     * `DEPTNO`를 만족하는 가장큰 `SAL`값부터 내려오면서, `MGR = 7698`을 만족하는 레코드를 찾으면 끝이다.
     * `MGR`이 뒤쪽에 있어, `DEPTNO` 구간의 `SAL` 최대값 부터 내려오면서 `MGR` 조건을 필터링하는 구조이다.
  3) ```oracle
     -- 인덱스 : SAL  + DEPTNO + MGR
     SELECT MAX(SAL) FROM EMP WHERE DEPTNO = 30 AND MGR = 7698;
     ```
     ```
     0    SELECT STATEMENT Optimizer=ALL_ROWS (Cost=1 Card=1 Bytes=8)
     1  0  SORT (AGGREGATE) (Card=1 Bytes=8)
     2  1    FIRST ROW (Cost=1 Card=1 Bytes=8)
     3  2      INDEX(FULL SCAN (MIN/MAX)) OF 'EMP_X1' (INDEX) (Cost=1 Card=1 ...)
     ```
     * 인덱스 전체에서 가장 큰 `SAL`값부터 내려오면서, `DEPTNO = 30`, `MGR = 7698` 조건을 만족하는 레코드를 찾으면 끝이다.
     * Index Range Scan은 불가능하다.
  4) ```oracle
     -- 인덱스 : DEPTNO + SAL
     SELECT MAX(SAL) FROM EMP WHERE DEPTNO = 30 AND MGR = 7698;
     ```
     ```
     0    SELECT STATEMENT Optimizer=ALL_ROWS (Cost=1 Card=1 Bytes=8)
     1  0  SORT (AGGREGATE) (Card=1 Bytes=8)
     2  1    TABLE ACCESS (BY INDEX ROWID) OF 'EMP' (TABLE) (Cost=2 Card=1 Bytes=8)
     3  2      INDEX(RANGE SCAN) OF 'EMP_X1' (INDEX) (Cost=1 Card=5)
     ```
     * `DEPTNO = 30` 조건을 만족하는 전체 레코드를 읽어 `MGR = 7698`을 필터링한 후 `MAX(SAL)`값을 구한다.
     * 인덱스 + 테이블 액세스 + 전체 집합에 대한 집계가 필요해 비효율적이다.

### Top N 쿼리를 이용해 MIN/MAX 구하기  
```oracle
-- 인덱스 : DEPTNO + SAL
SELECT *
FROM (
    SELECT sal
    FROM emp
    WHERE DEPTNO = 30
      AND MGR = 7698
    ORDER BY SAL DESC
)
WHERE ROWNUM <= 1;
```
```
0    SELECT STATEMENT Optimizer=ALL_ROWS (Cost=2 Card=1 Bytes=13)
1  0   COUNT (STOPKEY)
2  1     VIEW (Cost=2 Card=1 Bytes=13)
3  2       TABLE ACCESS (BY INDEX ROWID) OF 'EMP' (TABLE) (Cost=2 Card=1 ...)
4  3         INDEX (RANGE SCAN DESCENDING) OF 'EMP_X1' (INDEX) (Cost=1 Card=5)  
```
* 위 예시는 **Top N 쿼리를 이용해 최댓값을 구하는 방법**이다.
* `DEPTNO = 30` 조건을 만족하는 가장 오른쪽부터 역순으로 스캔하면서 테이블 액세스를 하다가, `MGR = 7698` 조건을 만족하는 레코드 하나를 찾았을 때 멈추면 된다.
* 실행계획 상에서는 `COUNT(STOPKEY)가 나타난다.
* 인라인 뷰를 사용하기 때문에 쿼리가 **더 복잡하긴 하지만, 성능 측면에서는 더 좋다.**

### 이력 조회
* 값이 어떻게 변경되어 왔는지 **과거 이력을 조회할 필요가 있다면, 이력 테이블을 따로 관리**해야 한다.
* 일반적으로는 이 이력 테이블에는 **현재 데이터도 저장하는데, 변경 이력을 완벽하게 다룰 수 있기 때문**이다.
  * 예시로, 특정 컬럼이 특정 값으로 바뀐 날짜를 알고 싶을 때, 이력 테이블에서 확인할 수 있다.


#### 가장 단순한 이력 조회
* 이력 데이터를 조회할 때, First Row Stopkey, Top N Stopkey 알고리즘이 동작할 수 있도록 인덱스 설계 및 SQL을 구현하는 일은 중요하다.
* 아래 예시는 상태코드가 현재 값으로 변경된 날짜를 상태변경이력 테이블을 통해 조회하는 방법이다.
```oracle
-- 상태변경이력 PK인덱스 : 장비번호 + 변경일자 + 변경순번 
SELECT 장비번호, 장비명, 상태코드
     , (SELECT MAX(변경일자)
        FROM 상태변경이력
        WHERE 장비번호 = P.장비번호) 최종변경일자
FROM 장비 P
WHERE 장비구분코드 = 'A001'
```
* 인덱스가 `[장비번호 + 변경일자 + 변경순번]` 순으로 구성되어 있었기 때문에, First Row Stopkey 알고리즘이 작동할 수 있다.

#### 좀 더 복잡한 이력 조회
* 위 예시에서 최종 변경 순번까지 읽어야 하는 상황이라면, 쿼리가 좀 더 복잡해진다.
1) `변경일자`와 `변경순번`을 문자열로 합쳐서 MAX를 한번에 구하는 패턴
   ```oracle
   -- 상태변경이력 PK인덱스 : 장비번호 + 변경일자 + 변경순번
   SELECT 장비번호, 장비명, 상태코드
        , SUBSTR(최종이력, 1, 8) 최종변경일자
        , TO_NUMBER(SUBSTR(최종이력, 9, 4)) 최종변경순번
   FROM (
       SELECT 장비번호, 장비명, 상태코드
            , (SELECT MAX(H.변경일자 || LPAD(H.변경순번, 4))
               FROM 상태변경이력 H
               WHERE 장비번호 = P.장비번호) 최종이력
       FROM 장비 P
       WHERE 장비구분코드 = 'A001'
   ) 
   ```
   * 위와같이 쿼리를 작성하면, **인덱스 컬럼을 가공했기 때문에 First Row Stopkey 알고리즘이 작동하지 않는다.**
   * 즉, `장비`별 `상태변경이력`이 많지 않을 경우엔 문제가 안될 순 있지만, 많으면 문제가 된다.

2) `변경일자`와 `변경순번`을 각각 MAX로 구하는 패턴
   ```oracle
   -- 상태변경이력 PK인덱스 : 장비번호 + 변경일자 + 변경순번
   SELECT 장비번호, 장비명, 상태코드
        , (SELECT MAX(H.변경일자)
           FROM 상태변경이력 H
           WHERE 장비번호 = P.장비번호) 최종변경일자
        , (SELECT MAX(H.변경순번)
           FROM 상태변경이력 H
           WHERE 장비번호 = P.장비번호
             AND 변경일자 = (SELECT MAX(H.변경일자)
                            FROM 상태변경이력 H
                            WHERE 장비번호 = P.장비번호)) 최종변경순번
   FROM 장비 P
   WHERE 장비구분코드 = 'A001'
   ```
   * 장비별 `상태변경이력`이 많은 경우, 오히려 위와같이 쿼리를 작성하는게 좋다.
   * `상태변경이력`을 **세 번 조회하는 비효율은 있지만, First Row Stopkey 알고리즘이 잘 작동하므로 성능은 비교적 좋다.**
   * 하지만, 이력테이블에서 읽어야 할 컬럼이 더 많다면 쿼리는 점점 더 복잡해진다.
     ```oracle
     -- 상태변경이력 PK인덱스 : 장비번호 + 변경일자 + 변경순번
     SELECT 장비번호, 장비명, 상태코드
          , (SELECT MAX(H.변경일자)
             FROM 상태변경이력 H
             WHERE 장비번호 = P.장비번호) 최종변경일자
          , (SELECT MAX(H1.변경순번)
             FROM 상태변경이력 H1
             WHERE 장비번호 = P.장비번호
               AND 변경일자 = (SELECT MAX(H2.변경일자)
                              FROM 상태변경이력 H2
                              WHERE 장비번호 = P.장비번호)) 최종변경순번
          , (SELECT H1.상태코드
             FROM 상태변경이력 H1
             WHERE 장비번호 = P.장비번호
               AND 변경일자 = (SELECT MAX(H2.변경일자)
                              FROM 상태변경이력 H2
                              WHERE 장비번호=P.장비번호)
               AND 변경순번 = (SELECT MAX(H3.변경순번)
                              FROM 상태변경이력 H3
                              WHERE 장비번호 = P.장비번호
                                AND 변경일자 = (SELECT MAX(H4.변경일자)
                                               FROM 상태변경이력 H4
                                               WHERE 장비번호 = P.장비번호))) 최종상태코드
     FROM 장비 P
     WHERE 장비구분코드 = 'A001'
     ```

#### INDEX_DESC 힌트 활용
* 쿼리를 단순하게 작성하면서도, 성능을 높이기 위해, **Top N + 인덱스 힌트를 사용하는 방법**이 있다.
```oracle
-- 상태변경이력 PK인덱스 : 장비번호 + 변경일자 + 변경순번
SELECT 장비번호, 장비명
     , SUBSTR(최종이력, 1, 8) 최종변경일자
     , TO_NUMBER(SUBSTR(최종이력 9, 4)) 최종변경순번
     , SUBSTR(최종이력, 13) 최종상태코드
FROM (
    SELECT 장비번호, 장비명
         , (SELECT /*+ INDEX_DESC(X 상태변경이력_PK) */ 
                   변경일자 || LPAD(변경순번, 4) || 상태코드
            FROM 상태변경이력 X
            WHERE 장비번호 = P.장비번호
              AND ROWNUM <= 1) 최종이력
    FROM 장비 P
    WHERE 장비구분코드 = 'A001'
) 
```
* 이 방식은 성능은 좋긴 한데, **인덱스 구성이 완벽해야만 쿼리가 잘 작동**한다.
* **따라서, 다른 대안이 있다면 그 방법을 사용하는게 좋다.**

#### 11g/ 12c 신기능 활용
* 바로 이전 예시는 **11g 이하 버전에서 실행해보면 ORA-00904 오류가 발생**한다.
* **메인쿼리 컬럼을 서브쿼리 내 인라인 뷰에서 참조했기 때문**이다.
* 이럴경우 아래와 같이, 정렬된 인라인 뷰를 한 단계 더 감싸서 해결하면 된다.

  ```oracle
  -- 상태변경이력 PK인덱스 : 장비번호 + 변경일자 + 변경순번
  SELECT 장비번호, 장비명
       , SUBSTR(최종이력, 1, 8) 최종변경일자
       , TO_NUMBER(SUBSTR(최종이력 9, 4)) 최종변경순번
       , SUBSTR(최종이력, 13) 최종상태코드
  FROM (
      SELECT 장비번호, 장비명
           , (SELECT 변경일자 || LPAD(X.변경순번, 4) || 상태코드
              FROM (SELECT 장비번호, 변경일자, 변경순번, 상태코드
                    FROM 상태변경이력
                    ORDER BY 변경일자 DESC, 변경순번 DESC)
              WHERE 장비번호 = P.장비번호
                AND ROWNUM <= 1) 최종이력
      FROM 장비 P
      WHERE 장비구분코드 = 'A001'
  ) 
  ```
  * **Predicate Pushing이라는 쿼리변환이 작동**하여 장비번호 조건이 인라인 뷰 안쪽으로 파고들어가게 된다.
* 12c에서 부터는 파싱 오류 없이 Top N Stopkey 알고리즘이 잘 작동한다.

#### 상황에 따라 달라져야 하는 이력 조회 패턴
* **전체 장비의 이력을 대상으로 조회할 때는 인덱스를 활용한 Stopkey 기능 작동여부가 튜닝의 핵심요소는 아니다.**
  * 인덱스를 활용한 패턴은 랜덤 I/O 발생량만큼 성능도 비례해서 느려지므로, 대량데이터 조회 시 좋은 솔루션은 되지 못한다.
1) 윈도우함수 (ROW_NUMBER) 활용
   ```oracle
   SELECT *
   FROM 장비 P
      , (SELECT 장비번호, 변경일자, 변경순번, 상태코드
              , ROW_NUMBER() OVER (PARTITION BY 장비번호
                                   ORDER BY 변경일자 DESC, 변경순번 DESC) RNUM
         FROM 상태변경이력) H
   WHERE H.장비번호 = P.장비번호
     AND H.RNUM = 1;
   ```
   * 위 예시는, Full Scan과 해시 조인을 이용하기 때문에 오랜 과거 이력까지 모두 읽지만, 인덱스를 이용하는 방식보다 빠르다.
2) KEEP 절 활용
   ```oracle
   SELECT P.장비번호, P.장비명
        , H.변경일자 AS 최종변경일자
        , H.변경순번 AS 최종변경순번
        , H.상태코드 AS 최종상태코드
   FROM 장비 P
      , (SELECT 장비번호
              , MAX(변경일자) 변경일자
              , MAX(변경순번) KEEP (DENSE_RANK LAST ORDER BY 변경일자, 변경순번) 변경순번
              , MAX(상태코드) KEEP (DENSE_RANK LAST ORDER BY 변경일자, 변경순번) 상태코드
         FROM 상태변경이력
         GROUP BY 장비번호) H
   WHERE H.장비번호 = P.장비번호
   ```
   * 위 예시는, `KEEP (DENSE_RANK LAST …)` 를 활용해각 장비번호 별로 가장 나중의 레코드를 한번에 뽑아온다.
   * 대량 이력 조회에서 효과적이다.

#### 선분이력
* 선분(라인 세그먼트)이력 모델을 사용하면 아래와 같이 간단한 쿼리로 이력을 쉽게 조회할 수 있다.
```oracle
SELECT P.장비번호, P.장비명
     , H.상태코드, H.유효시작일자, H.유효종료일자, H.변경일자
FROM 장비 P, 상태변경이력 H
WHERE P.장비구분코드 = 'A001'
  AND H.장비번호 = P.장비번호
  AND H.유효종료일자 = '99991231'
```
* 가장 최신 이력 1건 찾기 쿼리를, 복잡한 Top N / 윈도우 함수 없이도 단순조건으로 해결할 수 있다.

### Sort Group By 생략
* 인덱스를 이용해 소트 연산을 생략할 수 있는데, 그루핑 연산에도 인덱스를 활용할 수 있다.
* 실행계획에 Sort Gorup By Nosort 가 있다.
```oracle
select region, avg(age), count(*)
from customer
group by region
```
```
0  SELECT STATEMENT Optimizer=ALL_ROWS (Cost=30142 ...)
1  0  SORT GROUP BY NOSORT (Cost=30142 ...)
2  1    TABLE ACCESS (BY INDEX ROWID) OF 'CUSTOMER' (TABLE) (Cost=30142 ...)
3  2      INDEX (FULL SCAN) OF 'CUSTOMER_X01' (INDEX) (Cost=2337 ...)
```

* 위 예시는 `region`이 선두컬럼인 인덱스가 있는 예시이며, 동작 방식은 아래와 같다.
  * 컬럼의 `region` 값별로 구간을 스캔하면서 테이블을 액세스하다가 **다음 값을 만나는 순간, 집계한 내용을 Array에 적재**한다.
  * Array Size 만큼 집계 결과가 쌓였으면 클라이언트에 전송 하고, Fetch Call이 올 때까지 기다린다. (Fetch Call이 안오면 종료)
  * Fetch Call이 오면 다시 다음 값부터 작업을 반복한다.
* 같은 `region` 값들이 인덱스 상에서 연속 구간을 이루기 때문에 위와같은 방식이 가능하다.
* 따라서 Nosort 방식으로 Group By를 처리하면 부분범위 처리가 가능해진다.

## Sort Area를 적게 사용하도록 SQL 작성
* 소트 연산이 불가피하다면 메모리 내에서 처리를 할 수 있도록 노력해야 한다.
* **Sort Area 크기를 늘리는 방법도 있지만, 일단 Sort Area를 적게 사용할 방법부터 찾는것이 좋다.**

### 소트 데이터 줄이기
1) 정렬 전에 가공하는 방식 
   ```oracle
   select lpad(상품번호, 30) || lpad(상품명, 30) || lpad(고객ID, 10)
       || lpad(고객명, 20) || to_char(주문일시, 'yyyymmdd hh24:mi:ss')
   from 주문상품
   where 주문일시 between :start and :end
   order by 상품번호
   ```

2) 정렬 후 가공하는 방식
   ```oracle
   select lpad(상품번호, 30) || lpad(상품명, 30) || lpad(고객ID, 10)
       || lpad(고객명, 20) || to_char(주문일시, 'yyyymmdd hh24:mi:ss')
   from (
       select 상품번호, 상품명, 고객ID, 고객명, 주문일시
       from 주문상품
       where 주문일시 between :start and :end
       order by 상품번호
   )
   ```
* 두 예시에서, 위 1번 SQL은 레코드당 107바이트로 가공한 결과집합을 Sort Area에 담는다.
* 하지만 2번 SQL은 가공하지 않은 상태로 원본 컬럼만 정렬하고, 최종 출력 시 가공한다.
* 즉, 2번 SQL이 Sort Area를 적게 사용한다.

1) 전체 컬럼 정렬
   ```oracle
   select *
   from 예수금원장
   order by 총예수금 desc
   ```

2) 특정 컬럼만 정렬
   ```oracle
   select 계좌번호, 총예수금
   from 예수금원장
   order by 총예수금 desc
   ```
* 위 예시에선 당연히 2번 SQL이 Sort Area를 적게 사용한다.
* 상황에 따라 읽은 데이터량이 크게 달라져 성능도 크게 달라질 수 있다.

### Top N 쿼리의 소트 부하 경감 원리
* 특정 컬럼으로 인덱스가 설계되어 있는 경우엔, 그 컬럼에서 가장 큰 값 10개를 찾는것은 간단하다. (Top N Stopkey)
* 하지만 이러한 인덱스가 없는 상황에서는 아래와 같은 방법이 가장 효과적일 수 있다.
  * 처음 10개의 레코드를 읽어 후보들로 저장해둔다.
  * 그리고 나머지 레코드들을 읽으며, 현재 후보 중 최솟값보다 큰 값이 나타나면 그 최솟값을 제거하고 그 값을 넣는다.
  * 끝까지 반복해서 진행하면, 상위 N개만 남는다..
* 이러한 알고리즘을 **Top N 소트 알고리즘**이라 한다.
```oracle
select *
from (
    select rownum no, a.*
    from (
        select 거래일시, 체결건수, 체결수량, 거래대금
        from 종목거래
        where 종목코드 = 'KR123456'
          and 거래일시 >= '20180304'
        order by 거래일시
         ) a
    where rownum <= (:page * 10)
     )
where no >= (:page-1)*10 + 1
```
```
    0 STATEMENT
   10   COUNT STOPKEY (cr=690 pr=0 pw=0 time=83318 us)
   10     VIEW (cr=690 pr=0 pw=0 time=83290 us)
   10       SORT ORDER BY STOPKEY (cr=690 pr=0 pw=0 time=83264 us)
49857         TABLE ACCESS FULL 종목거래 (cr=690 pr=0 pw=0 time=299191 us)   
```
* 위 예시는 인덱스로 소트연산을 생략할 수 없어 Table Full Scan 방식으로 처리할 때의 예시이다.
* 실행계획에 **Sort Order By 오퍼레이션이 나타나며, 해당 오퍼레이션 옆에 Stopkey가 표시**되어 있다.
  * **소트연산을 피할 수 없어 Sort Order By 오퍼레이션을 수행하지만, Top N 소트 알고리즘이 작동한다는 의미**이다.
* 이 알고리즘이 작동하면 소트연산 횟수와 Sort Area 사용량을 최소화 해준다.
  * 이 방식을 이용하면 **대상 집합이 아무리 커도 많은 메모리 공간을 사용하지 않으며, 최솟값 최댓값 등의 레코드를 찾을 수 있다.**
  * 즉, Top N 소트 알고리즘이 소트 연산횟수와 Sort Area 사용량을 줄여주는 원리이다.
* 또한, 위 예시에서는 Physical Read(pr)와 Physical Write(pw)가 전혀 발생하지 않은 것을 확인할 수 있다.
  * 위 예시에서 `rownum <= (:page * 10)` 조건을 빼서 Top N 소트 알고리즘이 작동하지 않도록 하면 Physical Read(pr)와 Physical Write(pw)가 발생할 수 있다.
  * **메모리 내에서 정렬하지 못해 디스크를 이용하는 일이 발생하는 것**이다.

### 분석함수에서의 Top N 소트
* 윈도우 함수 중, **`rank`나 `row_number` 함수는 `max`함수보다 소트 부하가 적다.**
  * **Top N 소트 알고리즘이 작동하기 때문**이다.
* 즉, `max` 대신 `rank`를 사용하면 디스크를 사용하는 비중이 감소하는 상황이 생길 수 있다.

# DML 튜닝
## 기본 DML 튜닝
### DML 성능에 영향을 미치는 요소
* **인덱스**
* **무결성 제약**
* **조건절**
* **서브쿼리**
* **Redo 로깅**
* **Undo 로깅**
* **Lock**
* **커밋**

#### 인덱스
* **INSERT** 할 때 테이블에 레코드를 입력하면, **인덱스에도 입력해야 한다.**
  * 테이블은 Freelist(테이블마다 데이터 입력이 가능한 블록목록을 관리)를 통해 입력할 블록을 할당받지만, **인덱스는 정렬된 자료구조이기 때문에 수직적 탐색을 통해 입력할 블록을 찾아야 한다.**
  * 즉, 인덱스에 입력하는 과정이 복잡하기 때문에 DML 성능에 미치는 영향도 크다.
* **DELETE** 할 때에도 마찬가지로, 테이블에서 레코드를 하나 삭제하면 **인덱스 레코드도 모두 찾아서 삭제해줘야 한다.**
* **UPDATE** 할 때에는 **변경된 컬럼을 참조하는 인덱스만 찾아서 변경**해주면 된다.
  * 인덱스는 정렬된 자료구조이기 때문에 저장위치가 달라지므로 삭제 후 삽입하는 방식으로 처리한다. (두개의 오퍼레이션이 발생)
* 이러한 이유로 인덱스 개수가 DML 성능에 미치는 영향이 매우 크기 때문에, **DML 튜닝에서 인덱스 개수와 구성은 가장 직접적인 성능 지표 중 하나이다.**

#### 무결성 제약
* 데이터베이스에 논리적으로 의미 있는 자료만 저장되게 하는 데이터 무결성 규칙은 아래 네가지가 있다.
  * 개체 무결성(Entity Integrity)
  * 참조 무결성(Referential Integrity)
  * 도메인 무결성(Domain Integrity)
  * 사용자 정의 무결성(또는 업무 제약 조건)
* 이러힌 규칙은 DBMS에서 PK, FK, Check, Not Null 같은 제약을 설정하면 완벽하게 데이터 무결성을 지켜낼 수 있다.
* **PK, FK 제약은** Check, Not Null 제약보다 **성능에 더 큰 영향을 미친다.**
  * Check, Not Null은 해당 제약 조건을 준수하는지만 확인해보면 되지만, **PK, FK 제약은 실제 데이터를 조회해 봐야 알기 때문**이다.
* 즉, DML이 많은 테이블에서 FK를 너무 많이 걸면 성능에 큰 영향을 줄 수 있다.

#### 조건절
* 조건절만 포함하는 기본적인 DML문의 경우, SELECT 문과 실행계획이 다르지 않기 때문에 인덱스 튜닝 원리를 그대로 적용할 수 있다.

#### 서브쿼리
* 서브쿼리를 포함하는 DML의 경우, SELECT 문과 실행계획이 다르지 않기 때문에 조인튜닝 원리를 그대로 적용할 수 있다. (특히 서브쿼리 조인)

#### Redo 로깅
* 오라클은 **데이터 파일과 컨트롤 파일**에 가해지는 **모든 변경 사항을 Redo 로그에 기록**한다.
* Redo 로그는 트랜잭션 데이터가 유실되었을 때, 트랜잭션을 재현함으로써 유실 **이전 상태로 복구하는데 사용**된다.
* 따라서 **DML을 수행할 때마다 Redo 로그를 생성**해야 하기 때문에 Redo 로깅은 **DML 성능에 영향을 미친다.**
  * INSERT 작업에 대해서는 Redo 로깅 생략기능을 제공한다.

> Redo 로그의 용도
> 1) **Database Recovery** (Media Recovery)
>   * 물리적으로 디스크가 깨지는 등의 Media Fail 발생 시 **데이터베이스를 복구하기위해 사용**한다.
>   * 온라인 Redo 로그를 백업해 둔 **Archived Redo 로그를 이용**하게 된다.
> 2) **Cache Recovery**  (Instance Recovery 시 roll forward 단계)
>    * 모든 DBMS가 I/O 성능을 높이기 위해 버퍼캐시를 도입하는데, **버퍼 캐시는 휘발성**이다.
>    * 인스턴스가 비정상적으로 종료되면 캐시 내용이 사라진다.
>    * 이 때 Redo 로그를 읽어 마지막 커밋까지의 변경내용을 다시 적용한다.
> 3) **Fast Commit**
>    * 변경된 메모리 버퍼블록을 **디스크 상의 데이터 블록에 반영하는 작업은 랜덤 액세스 방식으로 이루어지기 때문에 매우 느리다.**
>    * 반면 **로그는 Append 방식으로 기록하므로 상대적으로 빠르다.**
>    * 따라서 먼저 Redo 로그에 변경 내용을 기록 후, 데이터 파일 반영은 나중에 배치방식으로 일괄처리한다. (DBWR, Checkpoint 이용)
>    * 즉, Redo 로그를 믿고 빠르게 커밋을 완료한다는 의미에서 Fast Commit이라고 부른다.

#### Undo 로깅
* 과거엔 Rollback이라는 용어를 주로 사용했지만, 9i부터 오라클은 Undo라는 용어를 사용하고 있다.
* **Redo는 트랜잭션을 재현함으로 과거를 현재 상태로 재생할 때 사용**되고, **Undo는 트랜잭션을 롤백함으로써 현재를 과거 상태로 되돌릴 때 사용**한다.
* 따라서, **Undo에는 변경 전 값으로 되돌리기 위한 정보를 로깅**한다.
* DML을 수행할 때마다 Undo를 생성해야 하기 때문에 Undo 로깅은 DML 성능에 영향을 미친다.
* 가장 오래 전에 커밋한 Undo 공간부터 재사용하기 때문에, 곧바로 사라지진 않지만 언젠가 다른 트랜잭션으로 덮어쓰여지며 사라진다.

> Undo 로그의 용도
> 1) **Transaction Rollback**
>    * **트랜잭션에 의한 변경사항을 최종 커밋하지 않고 롤백하고자 할 때** Undo 데이터를 이용한다.
> 2) **Transaction Recovery** (Instance Recovery 시 rollback 단계)
>    * Instance Crash 발생 후 Redo를 이용해 Roll forward 단계가 완료되면 최종 커밋되지 않은 변경사항까지 모두 복구된다.
>    * 이 때, **Undo를 이용해 커밋되지 않은 부분을 다시 롤백**한다.
> 3) **Read Consistency (읽기 일관성)**
>    * SQL 튜닝 관점에서 중요한 것은 읽기 일관성이다.
>    * 오라클은 읽기 일관성을 위해, 쿼리 시작 시점 기준의 일관된 데이터를 보여주려고 한다.
>    * 이 때, Undo를 사용해 **쿼리 도중 변경된 블록에 대해서 쿼리 시작 시점의 버전을 재구성**한다.
>    * 동시 트랜잭션이 많을 수록 Undo를 이용한 I/O가 많아져 성능에 영향을 준다.

> MVCC(Multi-Version Concurrency Control) 모델
> * MVCC 모델을 사용하는 오라클은 데이터를 **두 가지 모드**로 읽는다.
> 1) **Current 모드**
>   * 디스크에서 **캐시로 적재된 원본(Current) 블록을 현재 상태 그대로 읽는 방식**이다.
> 2) **Consistent 모드**
>   * 쿼리가 시작된 이후에 다른 트랜잭션에 의해 변경된 블록을 만나면 원본 블록으로부터 복사본 블록을 만들고, 거기에 **Undo 데이터를 이용하여 쿼리가 시작된 시점으로 되돌려 읽는 방식**이다.
>     * 즉, **원본블록 하나에 여러 복사본이 캐시에 존재할 수 있다.**
> * 오라클은 **마지막 커밋이 발생한 시점정보를 SCN(System Commit Number)라는 Global 변수로 관리**한다.
>   * 이 값은 기본적으로 각 트랜잭션이 커밋할 때마다 1씩 증가하며, 오라클 백그라운드 프로세서에 의해서도 조금씩 증가한다.
>   * 오라클은 **각 블록이 마지막으로 변경된 시점을 관리하기 위해 모든 블록 헤더에 SCN을 기록**하는데, 이를 **블록 SCN**이라고 한다.
>   * 그리고 **모든 쿼리는 Global 변수인 SCN 값을 먼저 확인하고 읽기 작접을 시작하는데 이를 쿼리 SCN**이라고 한다.
>   * **Consistent 모드는 쿼리 SCN과 블록 SCN을 비교함으로써 쿼리 수행 도중 블록이 변경되었는지를 확인하며 데이터를 읽는 방식**이다.
> * **SELECT 문은** 몇가지 예외 케이스를 빼곤 항상 **Consistent 모드로 데이터를 읽는다.**
> * **DML문은 Consistent 모드로 대상 레코드를 찾고 Current 모드로 추가/변경/삭제 한다.**

#### Lock
* Lock은 **DML성능에 직접적이고 큰 영향**을 미친다.
* Lock은 성능과 데이터 품질 모두 챙기려면 매우 세심한 동시성 제어가 필요하다.
  * **Lock을 너무 자주, 길게 사용하거나 레벨을 높일수록 DML 성능은 느려진다.**
  * 하지만 **Lock을 너무 적게, 짧게 사용하거나 필요한 레벨 이하로 낮추면 데이터 품질이 나빠진다.**
* **동시성 제어**는 **동시에 실행되는 트랜잭션 수를 최대화** 하면서도 입력/수정/삭제/조회 시 **데이터 무결성을 유지**하기 위해 노력하는것을 말한다.

#### 커밋
* 커밋은 DML과 별개로 실행하지만 DML을 끝내려면 커밋까지 완료해야 하기 때문에 밀접한 관련이 있다.
* DML이 Lock에 의해 블로킹된 경우 커밋은 DML 성능과 직결되는데, **DML을 완료할 수 있게 Lock을 푸는 열쇠가 커밋**이기 때문이다.
* 모든 DBMS가 **Fast Commit을 구현하고 있지만, 결코 가벼운 작업은 아니다.**
1) DB 버퍼캐시
   * 서버 프로세스는 버퍼캐시를 통해 데이터를 읽고 쓴다.
   * **버퍼캐시에서 변경된 블록을 주기적으로 데이터파일에 일괄 기록하는 작업은 DBWR 프로세스**가 맡는다.
2) Redo 로그버퍼
   * 버퍼캐시는 휘발성이기 때문에 DBWR 프로세스가 Dirty 블록들을 데이터파일에 반영할 때까지 불안한 상태처럼 보인다.
   * 하지만, **버퍼캐시에 가한 변경사항을 Redo 로그에도 기록해두었기 때문에 안전하다.**
     * 버퍼캐시 데이터가 유실되더라도 Redo 로그를 이용해 언제든 복구할 수 있다.
   * 그러나 Redo 로그도 파일이기 때문에 Append 방식으로 기록하더라도 디스크 I/O는 느리다.
   * Redo 로깅 성능 문제를 해결하기 위해 오라클은 **Redo 로그버퍼를 이용**한다.
     * Redo 로그 파일에 기록하기 전에 로그버퍼에 기록하는 방식이다.
   * **Redo 로그버퍼에 기록한 내용은 나중에 LGWR(LogWriter) 프로세스가 Redo 로그 파일에 일괄기록한다.
3) 트랜잭션 데이터 저장 과정
   * 트랜잭션의 데이터 저장 과정은 아래와 같다.
     1) DML 문을 실행하면 Redo 로그버퍼에 변경사항을 기록한다.
     2) 버퍼블록에서 데이터를 변경하는데, 이 때 버퍼캐시에 블록이 없다면 데이터파일에서 읽어온다.
     3) 커밋한다.
     4) LGWR 프로세스가 Redo 로그버퍼 내용을 Redo 로그 파일에 일괄 저장한다.
     5) DBWR 프로세스가 변경된 버퍼블록들을 데이터 파일에 일괄 저장한다.
   * 오라클은 데이터를 변경하기 전 항상 로그를 기록하는 Write Ahead Logging 방식을 사용한다.
     * DBWR와 LGWR 프로세스는 주기적으로 깨어나 각각 Dirty 블록과 Redo 로그버퍼를 파일에 기록한다.
     * LGWR 프로세스는 서버 프로세스가 커밋을 했다고 신호를 보낼 때에도 동작한다.
       * **커밋 시점에는 Redo 로그버퍼 내용을 로그파일에 기록한다**는 뜻이며, 이것을 **Log Force at Commit**이라고 한다.
     * 따라서 서버 프로세스가 변경한 버퍼블록들을 디스크에 기록하지 않았더라도, **커밋 시점에 Redo 로그를 디스크에 안전하게 기록했다면 그 순간부터 트랜잭션의 영속성은 보장**된다.
4) 커밋 = 저장 버튼
   * 커밋은 지금까지 작업한 내용을 디스크에 저장하라는 명령이다.
   * 저장을 완료할 때까지 서버 프로세스는 다음 작업을 진행할 수 없다.
     * Redo 로그버퍼에 기록된 내용을 디스크에 기록하도록 LGWR 프로세스에 신호를 보낸 후, 작업을 완료했다는 신호를 받아야 다음 작업을 할 수 있다. (Sync 방식)
   * LGWR 프로세스가 Redo 로그를 기록하는 작업은 디스크 I/O 작업이므로, **커밋은 생각보다 느린 작업이다.**
   * 너무 오랫동안 **커밋하지 않은 채 데이터를 계속 갱신하면, Undo 공간이 부족**해져 시스템 장애 상황을 유발할 수 있다.
   * 반대로 루프를 돌면서 너무 **자주 커밋을 하면, LGWR을 지나치게 많이 동작시켜 프로그램 자체 성능이 매우 느려진다.**

### 데이터베이스 Call과 성능
* SQL 트레이스 Call 통계를 보면 알 수 있듯이, SQL은 아래 세 단계로 나누어 실행된다.
  1) Parse Call
     * **SQL 파싱과 최적화를 수행하는 단계**다. 
     * SQL과 실행계획을 라이브러리 캐시에서 찾으면, 최적화 단계는 생략할 수 있다.
  2) Execute Call
     * **SQL을 실행하는 단계**다. 
     * DML은 이 단계에서 모든 과정이 끝나지만, SELECT 문은 Fetch 단계를 거친다.
  3) Fetch Call
     * **데이터를 읽어서 사용자에게 결과집합을 전송하는 단계**로, **SELECT 문에서만** 나타난다.
     * 전송할 데이터가 많을 때는 Fetch Call이 여러 번 발생한다.
* Call이 어디서 발생하느냐에 따라 **User Call**과 **Recursive Call**로 나눌 수 있다.
  1) User Call
     * **네트워크를 경유해 DBMS 외부로부터 인입되는 Call**이다. 
     * 3-Tier 아키텍처에서 User Call은 WAS 서버에서 발생하는 Call이다.
  2) Recursive Call
     * **DBMS 내부에서 발생하는 Call**이다. 
     * SQL 파싱과 최적화 과정에서 발생하는 데이터 딕셔너리 조회, PL/SQL로 작성한 사용자 정의 함수/프로시저/트리거에 내장된 SQL을 실행할 때 발생하는 Call이다.
* User Call이든 Recursive Call이든 **SQL을 실행할 때마다 Parse, Execute, Fetch Call 단계를 거친다.**
* 데이터베이스 Call이 많으면 성능은 느릴 수밖에 없고, **특히 네트워크를 경유하는 User Call이 성능에 미치는 영향은 매우 크다.**

### 대량 Insert 예시
* `source` 테이블을 읽고 100만건을 `target` 테이블에 저장해야 하는 상황이다.

#### 절차적 루프 처리 예시
* 테이블을 읽고 100만건을 루프돌며 건건히 테이블에 저장하는 방법이다.
1) PL/SQL 내에서 처리
   * 네트워크를 경유하지 않는 Recursive Call이기 때문에 29초 정도만에 수행을 마쳤다.
2) JAVA 프로그램으로 처리
   * 네트워크를 경유하는 User Call이기 때문에, 성능히 급격히 나빠져 약 218초 정도 걸렸다.
* 데이터베이스 Call의 종류에 따라 성능에 미치는 영향이 다르다.
* **둘 다 느리지만, User Call로 인한 성능 저하가 더 크다.**

#### One SQL의 중요성
* 위 예시를 아래와 같이 Insert Into Select 구문으로 처리할 수 있다.
```oracle
insert into target
select * from source;
```
* 즉, 단 한번의 Call로 처리하니 1.46초만에 수행을 마쳤다.
* 따라서, 이러한 상황 같이 **업무로직이 복잡하지 않다면 가급적 One SQL로 구현해야 성능이 잘 나온다.**
  * Insert Into Select, 수정가능 조인 뷰, Merge 문 등

### Array Processing 활용
* 실무에서 복잡한 절차적 프로그램을 One SQL로 구현하는 일은 절대로 쉽지 않다.
* 이럴 때 **Array Processing 기능을 활용하면 One SQL로 구현하지 않고도 Call 부하를 획기적으로 줄일 수 있다.**

```oracle
declare 
  cursor c is select * from source;
  type typ_source is table of c%rowtype;
  l_source typ_source;
    
  l_array_size number default 10000;

  procedure insert_target(p_source in typ_source) is
  begin
    forall i in p_source.first .. p_source.last
      insert into target values p_source(i);
  end insert_target;

begin
  open c;
  loop 
    fetch c bulk collect into l_source limit l_array_size;
    
    insert_target(l_source);
    
    exit when c%notfound;
  end loop;
  
  close c;
  
  commit;
end;
```
* JAVA에서도 `addBatch()`로 여러건을 모은 후, `executeBatch()`로 한번에 처리하는 방식으로 구현 가능하다.
* 즉, 100만번 발생하던 Call을 많이 줄여서 성능 향상이 발생한다.
* 단순히 루프를 돌며 건건이 처리하는 방식보다 PL/SQL에서는 29초 -> 4초, JAVA에서는 218초 -> 12초로 많은 성능향상이 나타난다.

### 인덱스 및 제약 해제를 통한 대량 DML 튜닝
* 인덱스와 무결성 제약조건은, OLTP 시스템에서 안정성을 위해 많이 사용하지만, DML 성능에는 많은 영향을 끼친다.
  * 그렇다고 OLTP 시스템에서 이러한 기능을 섣불리 해제할 순 없다.
* 동시 트랜잭션 없이 **대량 데이터를 적재하는 배치 프로그램에서는, 이 기능을 해제함으로 써 큰 성능개선 효과**를 얻을 수 있다.

#### PK 제약과 인덱스 해제 1 - PK 제약에 Unique 인덱스를 사용한 경우
1) PK제약을 비활성화 하면서 인덱스도 Drop한다.
   ```oracle
   alter table target modify constraint target_pk disable drop index;
   ```
2) 일반 인덱스는 Unusable 상태로 변경한다.
   ```oracle
   alter index T2_x1 unusable;
   ```
   * 인덱스가 Unusable인 상태에서 데이터를 입력하려면 skip_unusable_indexes 파라미터를 true로 설정해야 한다.
     ```oracle
     alter session set skip_unusable_indexes = true;
     ```
3) 대량 입력 작업 후, 작업이 완료되면 PK 제약을 활성화하고, 일반 인덱스를 재생성한다.
   ```oracle
   alter table target modify constraint target_pk enable NOVALIDATE;
   alter index target_x1 rebuild;
   ```
* 이렇게 하면 데이터 입력 시간과 제약 활성화 및 인덱스 재생성 시간을 합쳐도 더 빠른 경우가 많다.
* 즉, 인덱스 및 무결성 제약이 DML 성능에 미치는 영향은 아주 크다.

#### PK 제약과 인덱스 해제 2 - PK 제약에 Non-Unique 인덱스를 사용한 경우
* PK 인덱스를 Drop하지 않고 Unusable 상태에서 데이터를 입력하고 싶다면, PK 제약에 Non-Unique 인덱스를 사용하면 된다.
1) PK 및 PK 인덱스 제거한다.
   ```oracle
   alter table target drop primary key drop index;
   ```
2) PK 컬럼들에 대해 Non-Unique 인덱스 생성한다.
   ```oracle
   create index target_pk on target(no, empno);
   ```
3) 해당 인덱스를 사용하도록 PK 제약을 다시 생성한다.
   ```oracle
   alter table target 
     add constraint target_pk 
     primary key(no, empno) 
     using index target_pk;
   ```
4) 그 후, 아래와 같이 PK 제약을 비활성화하고, 인덱스를 Unusable 상태로 변경 후 대량 입력 작업을 진행한다.
   ```oracle
   alter table target modify constraint target_pk disable keep index;
   alter index target_pk unusable;
   alter index target_x1 unusable;
   ```
5) 입력 작업이 종료되면 인덱스를 재생성하고 PK 제약을 다시 활성화한다.
   ```oracle
   alter index target_x1 rebuild;
   alter index target_pk rebuild;
   alter table target modify constraint target_pk enable novalidate;
   ```

### 수정가능 조인 뷰
#### 전통적인 방식의 UPDATE
* **각 컬럼을 건건이 서브쿼리를 이용해 set 하는 방식은 비효율적**이기 때문에, 아래와 같이 쿼리를 작성할 수 있다.
  ```oracle
  update 고객 c
  set (최종거래일시, 최근거래횟수, 최근거래금액) = 
      (select max(거래일시), count(*), sum(거래금액)
       from 거래
       where 고객번호 = c.고객번호
         and 거래일시 >= trunc(add_months(sysdate, -1)))
  where exists (select 'x' from 거래
                where 고객번호 = c.고객번호
                  and 거래일시 >= trunc(add_months(sysdate, -1)))
  ```
  * 하지만 이러한 방법도 한달 이내 `고객`별 `거래` 데이터를 두 번 조회하기 때문에 비효율은 있다.
  * 즉, 총 `고객` 수와 한달 이내 `거래`를 발생시킨 `고객` 수에 따라 성능이 좌우된다.
* 총 `고객` 수가 아주 많다면 **Exists 서브쿼리를 해시 세미조인으로 유도하는 것을 고려**할 수 있다.
  ```oracle
  update 고객 c
  set (최종거래일시, 최근거래횟수, 최근거래금액) = 
      (select max(거래일시), count(*), sum(거래금액)
       from 거래
       where 고객번호 = c.고객번호
         and 거래일시 >= trunc(add_months(sysdate, -1)))
  where exists (select /*+ unnest hash_sj */ 'x' from 거래
                where 고객번호 = c.고객번호
                  and 거래일시 >= trunc(add_months(sysdate, -1)))
  ```
* 한달 이내 `거래`를 발생시킨 `고객`이 많아 UPDATE 발생량이 많다면 아래와 같이 변경하는 것을 고려할 수 있다.
  ```oracle
  update 고객 c
  set (최종거래일시, 최근거래횟수, 최근거래금액) = 
      (select nvl(max(거래일시), c.최종거래일시)
            , decode(count(*), 0, c.최근거래횟수, count(*))
            , nvl(sum(거래금액), c.최근거래금액)
       from 거래
       where 고객번호 = c.고객번호
         and 거래일시 >= trunc(add_months(sysdate, -1)))
  ```
  * 하지만 **모든 레코드에 LOCK**이 걸리는 것은 물론, **이전과 같은 값으로 갱신되는 비중이 높을수록 Redo 로그 발생량이 증가**해 오히려 비효율적일 수 있다.
* 즉, **다른 테이블과 조인이 필요한 경우 전통적인 UPDATE문을 사용하면 비효율을 완전히 없앨 순 없다.**

#### 수정가능 조인 뷰 활용
* **수정가능 조인 뷰를 활용하면 참조 테이블과 두 번 조인하는 비효율을 없앨 수 있다.**
  ```oracle
  update
  (select /*+ ordered use_hash no_merge(t) */
          c.최종거래일시, c.최근거래횟수, c.최근거래금액
        , t.거래일시, t.거래횟수, t.거래금액
   from (select 고객번호
              , max(거래일시) 거래일시, count(*) 거래횟수, sum(거래금액) 거래금액
         from 거래
         where 거래일시 >= trunc(add_months(sysdate, -1))
         group by 고객번호) t
       , 고객 c
   where c.고객번호 = t.고객번호
  )
  set 최종거래일시 = 거래일시
    , 최근거래횟수 = 거래횟수
    , 최근거래금액 = 거래금액
  ```
  * 해당 쿼리는 12c 이상에서만 정상적으로 실행되며, 10g 이하 버전은 UPDATE 옆에 `bypass_ujvc` 힌트를 사용해야 하고, 11g에서는 실행되지 않는다. 
* **조인 뷰는 FROM 절에 두 개 이상 테이블을 가진 뷰**를 가리키며, **수정가능 조인뷰는 말 그대로 입력, 수정, 삭제가 허용되는 조인뷰를 의미**한다.
* 하지만 수정가능 조인 뷰를 사용하려면 **조건이 필요**하다.
* **조인뷰에서 DML이 허용되려면, 그 테이블이 키 보존 테이블 이어야 한다.**

#### 키 보존 테이블이란?
* 키 보존 테이블이란 **조인 결과 집합에서도 해당 테이블의 키(또는 ROWID)가 여전히 중복 없이 유지되는 테이블**이다.
  * 즉, 조인된 결과를 통해서도 그 테이블의 한 행을 Unique하게 식별할 수 있는 경우이다.
  * 해당 조인 뷰에 대해 ROWID를 제공해 줄 수 있는 테이블이며, 보통 1쪽(Unique/PK 보장) 집합이 키 보존 테이블이 된다.
* 수정가능 조인 뷰는, 이러한 키 보존 테이블에 대해서만 DML을 허용한다는 제약이 있다.

#### ORA-01779 오류 회피
```oracle
update
(select d.deptno
      , d.avg_sal as d_avg_sal
      , e.avg_sal as e_avg_sal
 from (select deptno, round(avg(sal), 2) avg_sal 
        from emp 
        group by deptno) e
     , dept d
 where d.deptno = e.deptno)
set d_avg_sal = e.avg_sal
```
* 11g 이하 버전에서는 위 UPDATE문을 실행하면 ORA-01779 에러가 발생한다.
  * `emp` 테이블을 `detpno` 로 group by 했기 때문에, `deptno` 컬럼으로 조인한 `dept` 테이블은 키가 보존되더라도 옵티마이저가 수정 불가능한 조인 뷰로 간주하고 불필요한 제약을 가한 것이다.
* 이럴 경우엔 10g에선 아래와 같이 `bypass_ujvc` 힌트를 이용해 제약을 회피할 수 있다.
  ```oracle
  update /* bypass_ujvc */
  (select d.deptno
        , d.avg_sal as d_avg_sal
        , e.avg_sal as e_avg_sal
   from (select deptno, round(avg(sal), 2) avg_sal 
         from emp 
         group by deptno) e
      , dept d
   where d.deptno = e.deptno)
  set d_avg_sal = e.avg_sal
  ```
  * `bypass_ujvc`는 **Updatable Join View Check를 생략하라**고 옵티마이저에 지시하는 힌트이다.
* 하지만 11g부터 이 힌트를 사용할 수 없게 되었기 때문에 MERGE문으로 바꿔줘야 한다.
  * `bypass_ujvc`힌트 사용이 중단되었을 뿐 수정가능 조인 뷰는 사용할 수 있다.
  * 11g에서도 1쪽집합에 Unique 인덱스가 있으면 수정가능 조인 뷰를 이용한 UPDATE가 가능하다.
* 12c부터는 수정가능 조인 뷰가 개선이 되었고, 위 예시가 힌트 없이도 잘 동작하게 되었다.

### MERGE 문 활용
* DW(Data Warehouse)에서 가장 흔히 발생하는 오퍼레이션은, 기간계 시스템에서 가져온 신규 트랜잭션 데이터를 반영함으로써 두 시스템 간 데이터를 동기화하는 작업이다.
* 고객 테이블에 발생한 변경분 데이터를 DW에 반영하는 프로세스는 아래와 같다.
  1) 전일 발생한 변경데이터를 기간계 시스템으로부터 추출(Extraction)
  ```oracle
  create table customer_delta
  as
  select * from customer
  where mod_dt >= trunc(sysdate) - 1
    and mod_dt < trunc(sysdate)
  ```
  2) CUSTOMER_DELTA 테이블을 DW 시스템으로 전송(Transportation)
  3) DW 시스템으로 적재(Loading)
  ```oracle
  merge into customer t 
  using customer_delta s 
    on (t.cust_id = s.cust_id)
  when matched then 
    update set t.cust_nm = s.cust_nm, t.email = s.email, t.tel_no = s.tel_no, t.addr = s.addr, t.reg_dt = s.reg_dt
  when not matched then 
    insert (cust_id, cust_nm, email, tel_no, region, addr, reg_dt) 
    values (s.cust_id, s.cust_nm, s.email, s.tel_no, s.region, s.addr, s.reg_dt);
  ```
* 이 중 3번 데이터 **적재 작업을 효과적으로 지원하기 위해 오라클 9i에서 MERGE문이 도입**되었다.
* MERGE문은 `source` 테이블 기준으로 `target` 테이블과 Left Outer Join 한 뒤 조인에 성공하면 UPDATE, 실패하면 INSERT한다.
  * MERGE문을 **UPSERT라고도 부르는 이유**이다.

#### Optional Clauses (UPDATE/INSERT 선택적 처리)
* MERGE는 **`WHEN MATCHED`, `WEHN NOT MATCHED`를 선택적으로 처리할 수 있다.**
* 이 기능을 통해 **수정가능 조인 뷰 기능을 대체**할 수 있게 되었다.
```oracle
-- 수정가능 조인 뷰
update
(select d.deptno
      , d.avg_sal as d_avg_sal
      , e.avg_sal as e_avg_sal
 from (select deptno, rount(avg(sal), 2) avg_sal 
       from emp 
       group by deptno) e
    , dept d
 where d.deptno = e.deptno)
set d_avg_sal = e_avg_sal;

-- Merge 문
merge into dept d
using (select deptno, rount(avb(sal), 2) avg_sal from emp group by deptno) e
on (d.deptno = e.deptno)
when matched then
  update set d.avg_sal = e.avg_sal;
```

#### Conditional Operations (조건부 UPDATE/INSERT 처리)
* **ON 절에 기술한 조인문 외에 추가로 조건절을 기술할 수 있다.**
```oracle
merge into customer t using customer_delta s on (t.cust_id = s.cust_id)
when matched then 
  update set t.cust_nm = s.cust_nm, t.email = s.email, t.tel_no = s.tel_no, t.addr = s.addr, t.reg_dt = s.reg_dt
  where reg_dt >= to_date('20000101', 'yyyymmdd') -- 추가 조건
when not matched then 
  insert (cust_id, cust_nm, email, tel_no, region, addr, reg_dt) 
  values (s.cust_id, s.cust_nm, s.email, s.tel_no, s.region, s.addr, s.reg_dt)
  where reg_dt < trunc(sysdate); -- 추가 조건
```

#### DELETE Clause (조건부 삭제)
* **이미 저장된 데이터를 조건에 따라 지울 수 있다.**
```oracle
merge into customer t using customer_delta s on (t.cust_id = s.cust_id)
when matched then
  update set t.cust_nm = s.cust_nm, t.email = s.email, t.tel_no = s.tel_no, t.addr = s.addr, t.reg_dt = s.reg_dt
  delete where t.withdraw_dt is not null -- 탈퇴일시가 null이 아닌 레코드 삭제
when not matched then 
  insert (cust_id, cust_nm, email, tel_no, region, addr, reg_dt) 
  values (s.cust_id, s.cust_nm, s.email, s.tel_no, s.region, s.addr, s.reg_dt);
```
* 위 예시에서 기억해야할 부분은, UPDATE가 이루어진 결과로 `탈퇴일시`가 Null이 아닌 레코드만 삭제한다.
  * 즉, **UPDATE 결과를 보고 적용**한다.
* MERGE 문 DELETE절은 **조인에 성공한 데이터만 삭제**할 수 있다.
  * `WHEN MATCHED` 블록 안에서만 사용 가능하다. 
  * `source` 테이블에 존재하지 않는 데이터(조인에 실패)는 `target` 데이터에서 지울 수 없다.
  * 즉, **조인에 실패한 데이터는 UPDATE도 할 수 없고 DELETE도 할 수 없다.**

#### Merge Into 활용 예
* **저장하려는 레코드가 기존에 있던 것이면 UPDATE를 수행하고, 그렇지 않으면 INSERT를 하려고 할 때, MERGE 문을 활용**하면 SQL을 한번만 수행하도록 할 수 있다.

> 수정가능 조인 뷰 vs MERGE 문
> * UPDATE 대신 MERGE 문을 사용하는 개발자가 늘고 있다.
> * 실행계획만 같다면 UPDATE문을 사용하던 MERGE문을 사용하던 상관은 없다.
> * 하지만 아래와 같은 케이스는 문제가 된다.
>   ```oracle
>   MERGE INTO EMP T2
>   USING (SELECT T.ROWID AS RID, S.ENAME
>          FROM EMP T, EMP_SRC S
>          WHERE T.EMPNO = S.EMPNO
>            AND T2.ENAME <> S.ENAME) S
>   ON (T2.ROWID = S.RID)
>   WHEN MATCHED THEN UPDATE SET T2.ENAME = S.ENAME; 
>   ```
>   * UPDATE 대상 테이블인 EMP를 두 번 액세스 하기 때문에 성능에 좋지 않다.
>   ```oracle
>   MERGE INTO EMP T
>   USING EMP_SRC S
>   ON (T.EMPNO = S.EMPNO)
>   WHEN MATCHED THEN UPDATE SET T.ENAME = S.ENAME
>   WHERE T.ENAME <> S.ENAME; 
>   ```
>   * 위와같이 작성하는게 좋고, 차라리 아래와 같이 수정가능 조인 뷰를 이용한 UPDATE 문을 사용하는게 편할 수 있다.
>   ```oracle
>   UPDATE (
>     SELECT S.ENAME AS S_ENAME, T.ENAME AS T_ENAME
>     FROM EMP T, EMP_SRC S
>     WHERE T.EMPNO = S.EMPNO
>       AND T.ENAME <> S.ENAME
>   )
>   SET T_ENAME = S_ENAME
>   ```
>   * 물론 `EMP_SRC` 테이블 `EMPNO` 컬럼에 Unique 인덱스가 생성되어있어야 한다. 
>     * 그래야 조인 결과에서 `emp`가 키 보존테이블이 된다.

## Direct Path I/O 활용
* OLTP 시스템에선 버퍼캐시가 성능 향상에 도움을 준다.
* 정보계 시스템(DW/OLAP 등)이나 배치 프로그램에서 사용하는 SQL은 **주로 대량 데이터를 처리**하기 때문에 **버퍼캐시를 경유하는 I/O 메커니즘이 오히려 성능을 떨어뜨릴 수 있다.**

### Direct Path I/O
* **일반적으로 블록 I/O는 DB 버퍼캐시를 경유**한다.
  * 읽고자 하는 블록을 먼저 버퍼캐시에서 찾아보고, 찾지 못할 때만 디스크에서 읽는다.
  * 데이터를 변경할 때도 먼저 블록을 버퍼캐시에서 찾는다.
    * 찾은 버퍼블록에 변경을 가하고 나면, DBWR 프로세스가 변경된 블록들을 주기적으로 찾아 데이터파일에 반영해준다.
* 자주 읽는 블록에 대한 반복적인 I/O Call을 줄임으로써 시스템 전반적인 성능을 높이려고 버퍼캐시를 이용하지만, **대량 데이터를 읽고 쓸 때 건건이 버퍼캐시를 탐색한다면 성능에는 오히려 안좋다.**
  * Full Scan 의주로 가끔 수행되는 **대용량 처리 프로그램이 읽어들인 데이터는 대개 재사용성이 낮다.**
  * 대량 블록을 건건이 디스크로부터 버퍼캐시에 **적재하고서 읽어야 하는 부담도 크다.**
* 이러한 데이터 블록들이 버퍼캐시를 점유한다면 다른 프로그램 성능에도 나쁜 영향을 미친다.
* 그래서 오라클은 **버퍼캐시를 경유하지 않고 곧바로 데이터 블록을 읽고 쓸 수 있는 Direct Path I/O 기능을 제공**한다.
  * **병령로 Full Scan을 수행할 때**
  * **병렬 DML을 수행할 때**
  * **Direct Path Insert를 수행할 때**
  * Temp 세그먼트 블록들을 읽고 쓸 때
  * direct 옵션을 지정하고 export를 수행할 때
  * nocache 옵션을 지정한 LOB 컬럼을 읽을 때

> 병렬쿼리
> * 쿼리문에 `parallel` 또는 `parallel_index` 힌트를 사용하면 지정한 병렬도 만큼 병렬 프로세스가 떠서 동시에 작업을 진행한다.
> ```oracle
> select /*+ full(t) parallel(t 4) */ * 
> from big_table t;
> ```
> ```oracle
> select /*+ index_ffs(t big_table_x1) parallel_index(t big_table_x1 4) */ count(*) 
> from big_table t;
> ```
> * 위처럼 **병렬도를 4로 지정하면 성능이 4배 빨라지는게 아니라 수십 배 빨라진다.**
> * **Direct Path I/O 때문에 나타나는 효과**로, 버퍼캐시를 탐색하지 않고 디스크로부터 버퍼캐시에 적재하는 부담도 없으니 빠른 것이다.
> * **Order by, Group By, 해시 조인, 소트 머지 조인 등을 처리할 때는 힌트로 지정한 병렬도보다 두 배 많은 프로세스가 사용**된다.

### Direct Path Insert
* **일반적인 INSERT가 느린 이유**는 다음과 같다.
  * Freelist에서 데이터를 넣을 수 있는 블록을 찾는다.
    * Freelist : 테이블 HWM(High-Water-Mark) 아래쪽에 있는 블록 중 데이터 입력이 가능한 블록을 목록으로 관리
  * Freelist에서 찾은 블록을 버퍼캐시에서 찾거나, 없으면 디스크에서 읽어 캐시에 적재 한다.
  * INSERT 내용을 Undo 세그먼트에 기록한다.
  * INSERT 내용을 Redo 로그에 기록한다.
* **Direct Path Insert 방식이 빠른 이유**는 다음과 같다.
  * Freelist를 참조하지 않고, HWM 바깥 영역에 데이터를 순차적으로 입력한다.
  * 블록을 버퍼캐시에서 탐색하지 않으며, 버퍼캐시에도 적재하지 않고 **데이터파일에 직접 기록**한다.
  * **Undo 로깅을 안한다.**
    * 필요 시 Redo 로깅을 안 하게 할 수 있다. (Direct Path Insert가 아닌 일반 Insert 문에 로깅하지 않게 하는 방법은 없음)
      ```
      alter table t NOLOGGING
      ```
* Direct Path Insert 방식으로 동작하는 케이스는 아래와 같다.
  * INSERT ... SELECT 문에 `append` 힌트 사용
  * `parallel` 힌트를 이용해 병렬모드로 INSERT
  * direct 옵션을 지정하고 SQL Loader(sqlldr)로 데이터 적재
  * CTAS(create table ... as select) 문 수행

* Array Processing도 Dircet Path Insert 방식으로 처리할 수 있다.
  * `append_values` 힌트를 사용하면 된다.
    ```oracle
    ...
      insert /*+ append_values */ into target values p_source(i);
    ...
    ```

#### Direct Path Insert 주의사항
* 성능은 매우 빨라지지만, **테이블에는 Exclusive 모드 TM Lock**이 걸린다.
  * 커밋하기 전까지 **다른 트랜잭션은 해당 테이블에 DML을 수행하지 못한다.**
  * 따라서, **트랜잭션이 빈번한 주간에 이 옵션은 절대 사용하면 안된다.**
* Freelist를 조회하지 않고 HWM 바깥 영역에 입력하므로 **테이블 여유 공간이 있어도 재활용하지 않는다.**
  * **Range 파티션 테이블**이라면 과거 데이터를 DELETE가 아닌 DROP 방식으로 지워야 공간 반환이 제대로 이루어진다.
  * **비파티션 테이블**이라면 주기적으로 **Reorg(재구성) 작업을 수행해 줘야 한다.**

### 병렬 DML
* **병렬 쿼리와 병렬 DDL은 기본적으로 활성화**되어 있어 언제든 바로 병렬 처리 가능하다.
* 하지만 **병렬 DML은 기본적으로 비활성화** 되어있다.
* 따라서 DML을 병렬로 처리하려면 아래와 같이 **병렬 DML을 활성화해야 한다.**
  ```oracle
  alter session enable parallel dml;
  ```
* 그리고 `parallel` 힌트를 사용하면, 대상 레코드를 찾는 작업(INSERT는 SELECT 쿼리, UPDATE/DELETE는 조건절 검색)은 물론 데이터 추가/변경/삭제도 병렬로 진행한다.
  ```oracle
  insert /*+ parallel(c 4) */ into 고객 c
  select /*+ full(o) parallel(o 4) */ * from 외부가입고객 o;
  
  update /*+ full(c) parallel(c 4) */ 고객 c 
  set 고객상태코드 = 'WD'
  where 최종거래일시 < '20100101';
  
  delete /*+ full(c) parallel(c 4) */ from 고객 c
  where 탈퇴일시 < '20100101';
  ```

* **힌트를 사용했지만 병렬 DML을 활성화하지 않은 경우** 대상 레코드를 **찾는 작업은 병렬로 진행**하지만, **추가/변경/삭제는 QC(Query Coordinator)가 혼자 담당**하기 때문에 병목이 생긴다.
  * 오라클은 DML 문에 두 단계 전략을 사용하는데, Consistent 모드로 대상 레코드를 찾고 Current 모드로 추가/변경/삭제 한다.
* **병렬 INSERT는 `append` 힌트를 지정하지 않아도 Direct Path Insert 방식을 사용**한다.
  * 하지만 병렬 INSERT에도 **`append`힌트를 같이 사용하는게 좋다.**
  * **벙렬 DML이 동작하지 않는 상황에서도 QC가 Direct Path Insert를 사용**할 수 있어 어느 정도 성능을 확보할 수 있다.
* 12c부터는 `enable_parallel_dml` 힌트도 지원한다.
  ```oracle
  insert /*+ enable_parallel_dml parallel(c 4) */ into 고객 c
  select /*+ full(o) parallel(o 4) */ * from 외부가입고객 o;
      
  update /*+ enable_parallel_dml full(c) parallel(c 4) */ 고객 c 
  set 고객상태코드 = 'WD'
  where 최종거래일시 < '20100101';
  ```
* **병렬 DML을 사용하면 테이블에 Exclusive 모드 TM Lock이 걸리기 때문에 트랜잭션이 빈번한 주간에 이 옵션은 절대 사용하면 안된다.**

#### 병렬 DML이 잘 작동하는지 확인하는 방법
* DML 작업을 각 병렬 프로세스가 처리하는지, QC가 처리하는지를 실행계획에서 확인할 수 있다.
* **UPDATE(또는 DELETE/INSERT)가 `PX COORDINAOTR` 아래쪽에 나타나면 UPDATE를 각 병렬 프로세스가 처리**한다.
* 하지만 UPDATE(또는 DELETE/INSERT)가 `PX COORDINAOTR` 위쪽에 나타나면 UPDATE를 QC가 처리한다.

## 파티션을 활용한 DML 튜닝
### 테이블 파티션
* 파티셔닝은 **테이블 또는 인덱스 데이터를 특정 컬럼 값에 따라 별도 세그먼트에 나눠서 저장**하는 것을 말한다.
* 파티션이 필요한 이유를 관리적 측면과 성능적 측면으로 요약하면 아래와 같다.
  * 관리적 측면 : 파티션 단위 백업/추가/삭제/변경 -> **가용성, 운영 편의성 향상**
  * 성능적 측면 : 파티션 단위 조회 및 DML -> **I/O범위 축소, 경합 또는 부하 분산**
* 파티션에는 Range, 해시, 리스트 세 종류가 있다.

#### Range 파티션
* Range 파티션은 오라클 8 버전부터 제공된 가장 기초적인 방식으로 **주로 날짜 컬럼을 기준으로 파티셔닝**한다.
```oracle
create table 주문 ( 주문번호 number, 주문일자 varchar2(8), 고객ID varchar2(5)
                  , 배송일자 varchar2(8), 주문금액 number, ... )
partition by range(주문일자) (
  partition P2017_Q1 values less than ('20170401')
, partition P2017_Q2 values less than ('20170701')
, partition P2017_Q3 values less than ('20171001')
, partition P2017_Q4 values less than ('20180101')
, partition P2018_Q1 values less than ('20180401')
, partition P9999_MX values less than (MAXVALUE)
);
```
* 각 레코드를 파티션 키 값에 따라 분할 저장하고, 읽을 때에도 검색 조건을 만족하는 파티션만 골라서 읽을 수 있어 **이력성 데이터를 Full Scan 방식으로 조회할 때 성능을 크게 향상**한다.
* 오래된 파티션만 따로 백업/삭제가 가능하기 때문에 보관주기 정책 적용을 하기 쉽다.
* 조건절을 분석해 읽지 않아도 되는 파티션 세그먼트를 액세스 대상에서 제외하는 파티션 Pruning이 작동하여 SQL 성능이 향상된다.
* 파티션과 병렬처리가 만나면 그 효과는 배가 된다.
* 파티션도 클러스터, IOT와 마찬가지로 관련 있는 데이터가 흩어지지 않고 물리적으로 인접하도록 저장하는 클러스터링 기술에 속한다.
  * 클러스터와 다른 점은 세그먼트 단위로 모아서 저장한다는 점이다.
  * 클러스터는 데이터를 블록 단위로 모아 저장한다.
  * IOT는 데이터를 정렬된 순서로 저장하는 구조이다.
    
#### 해시 파티션
* 해시 파티션은 오라클 8i부터 제공하기 시작했다.
* **파티션 키 값을 해시 함수에 입력해 반환받은 값이 같은 데이터를 같은 세그먼트에 저장하는 방법**이다.
* 파티션 개수만 사용자가 결정하며, 데이터를 분산하는 알고리즘은 오라클 내부 해시함수가 결정한다.
* 해시 파티션은 **변별력이 좋고 데이터 분포가 고른 컬럼을 파티션 기준으로 선정해야 효과적**이다.
```oracle
create table 고객 ( 고객ID varchar2(5), 고객명 varchar2(10), ... )
partition by hash(고객ID) partitions 4;
```
* 검색할 때는 조건절 비교 값에 똑같은 해시 함수를 적용함으로써 읽을 파티션을 결정한다.
* 해시 알고리즘 특성 상 **등치조건 또는 IN-List 조건으로 검색할 때만 파티션 Pruning이 작동**한다.

#### 리스트 파티션
* 리스트 파티션은 오라클 9i부터 제공하기 시작했다.
* **사용자가 정의한 그룹핑 기준에 따라 데이터를 분할 저장**하는 방식이다.
```oracle
create table 인터넷매물 ( 물건코드 varchar2(5), 지역분류 varchar2(4), ... )
partition by list(지역분류) (
  partition P_지역1 values ('서울')
, partition P_지역2 values ('경기', '인천')
, partition P_지역3 values ('부산', '대구', '대전', '광주')
, partition P_기타 values (DEFAULT)
);
```
* Range 파티션에서는 값의 순서에 따라 저장할 파티션이 결정되지만, **리스트 파티션에서는 순서와 상관없이 불연속적인 값의 목록에 의해 결정**된다.
* 해시 파티션은 오라클이 정한 해시 알고리즘에 따라 임의로 분할하지만, **리스트 파티션은 사용자가 정의한 논리적인 그룹에 따라 분할**한다.
* 업무적인 친화도에 따라 그룹핑 기준을 정하되, **될 수 있으면 각 파티션에 값이 고르게 분산되도록 해야** 한다.

### 파티션 인덱스
* 파티션 테이블은 아래와 같이 구분된다.
  * 비파티션 테이블
  * 파티션 테이블
* 파티션 인덱스는 아래와 같이 구분되는데, 각 파티션이 커버하는 커버하는 테이블 파티션 범위에 따라 로컬과 글로벌로 나뉜다.
  * 비파티션 인덱스
  * 파티션 인덱스
    * 로컬 파티션 인덱스
    * 글로벌 파티션 인덱스

#### 로컬 파티션 인덱스
* 로컬 파티션 인덱스는 **각 파티션 테이블과 파티션 인덱스가 서로 1:1 대응관계**가 되도록 **오라클이 자동으로 관리**하는 파티션 인덱스를 말한다.
  * 로컬 인덱스라고 줄여서 부르기도 한다.
* **`CREATE INDEX` 문 뒤에 `LOCAL` 옵션을 추가**하면 된다.
  ```oracle
  create index 주문_x01 on 주문 ( 주문일자, 주문금액 ) LOCAL;
  create index 주문_x02 on 주문 ( 고객ID, 주문일자 ) LOCAL;
  ```
* 각 로컬 파티션 인덱스는 파티션 테이블의 속성을 그대로 상속받는다.
  * 파티션 테이블 키가 `주문일자`이면, 로컬 파티션 인덱스 키도 `주문일자`가 된다.
* 로컬 파티션 인덱스는 파티션 테이블과 정확히 1:1 대응 관계를 갖도록 오라클이 파티션을 자동으로 관리해준다.
* 파티션 테이블의 **파티션 구성을 변경(add, drop, exchange 등)하더라도 인덱스를 재생성할 필요가 없다.**
  * 변경작업이 순식간에 끝나기 때문에, **피크 시간대만 피하면 서비스를 중단하지 않고도 작업할 수 있다.**

#### 글로벌 파티션 인덱스
* **글로벌 파티션 인덱스는 파티션을 테이블과 다르게 구성한 인덱스**이다.
  * 로컬이 아닌 파티션 인덱스는 모두 글로벌 파티션 인덱스이다.
  * 파티션 유형이 다르거나, 파티션 키가 다르거나, 파티션 기준값 정의가 다른 경우이다.
* 비파티션 테이블이어도 인덱스는 파티셔닝할 수 있다.
```oracle
create index 주문_x03 on 주문( 주문금액, 주문일자 ) GLOBAL
partition by range(주문금액) (
  partition P_01 values less than ( 10000 )
, partition P_MX values less than ( MAXVALUE )   
);
```
* 글로벌 파티션 인덱스는 **테이블 파티션 구성 변경 시 Unusable 상태로 바뀌기 때문에 인덱스를 바로 재생성해줘야 한다.**
  * 그동안 해당 테이블을 사용하는 서비스를 중단해야 한다.

#### 비파티션 인덱스
* 비파티션 인덱스는 말 그대로 파티셔닝하지 않은 인덱스다.
  * 비파티션 인덱스는 여러 테이블 파티션을 가리키기 때문에, 글로벌 비파티션 인덱스라고 부르기도 한다.
* 비파티션 인덱스는 **파티션 테이블의 구성 변경 시 Unusable 상태로 바뀌기 때문에 인덱스를 바로 재생성해줘야 한다.**
  * 그동안 해당 테이블을 사용하는 서비스를 중단해야 한다.

#### Prefixed vs Nonprefixed
* 파티션 인덱스를 Prefixed와 Nonprefixed로 나눌 수 있다.
  * **Prefixed** : **인덱스 파티션 키 컬럼이 인덱스 키 컬럼 왼쪽 선두에 있다.**
  * **Nonprefixed** : **인덱스 파티션 키 컬럼이 인덱스 키 컬럼 왼쪽 선두에 위치하지 않는다.** (아예 속하지 않는 경우도 포함)
* 인덱스 파티션 키 컬럼이 인덱스 구성상 왼쪽 선두 컬럼에 위치하는지에 따른 구분이다.
* **글로벌 파티션은 Nonprefixed를 사용할 수 없다.**
  * 글로벌 파티션 인덱스는 Prefixed 파티션만 지원되므로, 전체 유형은 4가지로 정리된다.
    * 로컬 Prefixed 파티션 인덱스
    * 로컬 Nonprefixed 파티션 인덱스
    * 글로벌 Prefixed 파티션 인덱스
    * 비파티션 인덱스

#### Unique 인덱스와 파티션 설계 시 중요한 제약
* **Unique 인덱스를 파티셔닝하려면, 파티션 키가 모두 인덱스 구성 컬럼이어야 한다.**
  * 파티션 키 컬럼 없이 Unique를 보장하려면 Unique 체크를 위해 여러 파티션을 돌아 다녀야 하는 비효율이 생긴다.
  * 파티션 키 조건 없이 PK 인덱스로 액세스하는 수많은 쿼리 성능을 위해서도 필요하다.
  * 즉, 이 제약은 DML 성능 보장을 위해 당연히 있어야 할 제약조건이다.
* **이 제약으로 인해 PK 인덱스를 로컬 파티셔닝하지 못하면** 파티션 Drop, Truncate, Exchange, Split, Merge 같은 **파티션 구조 변경 작업도 쉽지 않다.**
  * 이 작업을 하는 순간 PK 인덱스가 Unusable 상태로 바뀌기 때문이다.
  * 곧바로 Rebuild를 하면 되지만 그동안 해당 테이블을 사용하는 서비스를 중단해야 한다.
* 따라서 **서비스 중단 없이 파티션 구조를 빠르게 변경하려면, PK를 포함한 모든 인덱스가 로컬 파티션 인덱스이어야 한다.**
* 파티션을 활용한 대량 UPDATE/DELETE/INSERT는 파티션 구조 변경 작업을 수반하며, ILM(보관주기관리, Information Lifecycle Management)를 지원하는 아주 중요한 기능이다.
  * 이 기능을 활용해 **ILM 관리체계를 효과적으로 운영하려면 가급적 인덱스를 로컬 파티션으로 구성**해야 한다.
  * 즉, **대량으로 데이터를 추가/변경/삭제하는 기준 컬럼을 PK에 포함하려고 노력**해야 한다.
    * 설계할 때부터 PK를 잘 구성해 줘야 한다.

### 파티션을 활용한 대량 UPDATE 튜닝
* 대량 데이터를 입력/수정/삭제할 때는 인덱스를 Drop 하거나 Unusable 상태로 변경하고 작업하는 방법을 많이 활용하지만, 초대용량 테이블일 경우엔 쉽지 않다.
  * 이 방법의 손익 분기점은 약 5% 정도(테이블이 클수록 더 낮아지긴 함)이다.
  * 하지만 초대용량 테이블일 경우 인덱스 재생성 비용이 커져 쉽지 않아진다.
* **테이블이 파티셔닝 되어 있고, 인덱스도 로컬 파티션**이라면 좋은 방법이 생긴다.
* **수정된 값을 갖는 임시 세그먼트를 만들어 원본 파티션과 바꿔치기**하는 방식이다.

1. **임시테이블을 생성**한다. 가능하다면 nologging 모드로 생성한다.
  ```oracle
  create table 거래_t
  nologging
  as
  select * from 거래 where 1 = 2;
  ```
2. 데이터를 읽어 **임시 테이블에 입력하면서 상태코드 값을 수정**한다.
  ```oracle
  insert /*+ append */ into 거래_t
  select 고객번호, 거래일자, 거래순번, ...
       , (case when 상태코드 <> 'ZZZ' then 'ZZZ' else 상태코드 end) 상태코드
  from 거래
  where 거래일자 < '20250101';
  ```
3. **임시 테이블에 원본 테이블과 같은 구조로 인덱스를 생성**한다. 가능하다면 nologging 모드로 생성한다.
  ```oracle
  create unique index 거래_t_pk on 거래_t(고객번호, 거래일자, 거래순번) nologging;
  create index 거래_t_x1 on 거래_t(거래일자, 고객번호) nologging;
  create index 거래_t_x2 on 거래_t(상태코드, 거래일자) nologging;
  ```
4. **특정 파티션과 임시 테이블을 Exchange** 한다.
  ```oracle
  alter table 거래
  exchange partition p202412 with table 거래_t
  including indexes without validation;
  ```
5. **임시 테이블을 Drop** 한다.
  ```oracle
  drop table 거래_t;
  ```
6. nologging 모드로 작업했다면, 파티션을 logging 모드로 전환한다.
  ```oracle
  alter table 거래 modify partition p202412 logging;
  alter index 거래_pk modify partition p201412 logging;
  alter index 거래_x1 modify partition p201412 logging;
  alter index 거래_x2 modify partition p201412 logging;
  ```

### 파티션을 활용한 대량 DELETE 튜닝
* 조건절로 수천만 건 데이터를 삭제할 때도, 인덱스를 실시간으로 관리하려면 엄청난 시간이 소요된다.
* 그렇다고 초대용량 테이블 인덱스를 모두 Drop 했다가 다시 생성하는것도 쉽지 않다.

> DELETE가 느린 이유
> 1. 테이블 레코드 삭제
> 2. 테이블 레코드 삭제에 대한 Undo Logging
> 3. 테이블 레코드 삭제에 대한 Redo Logging
> 4. 인덱스 레코드 삭제
> 5. 인덱스 레코드 삭제에 대한 Undo Logging
> 6. 인덱스 레코드 삭제에 대한 Redo Logging
> 7. Undo(2, 5번)에 대한 Redo Logging

#### 파티션 Drop을 이용한 대량 데이터 삭제
* 테이블이 삭제 조건절 컬럼 기준으로 파티셔닝 되어 있고, 인덱스도 로컬 파티션이라면 아래와 같이 대량 데이터를 순식간에 삭제할 수 있다.
  ```oracle
  alter table 거래 drop partition p202412;
  -- 11g부턴 아래와 같이도 대상 지정 가능
  alter table 거래 drop partition for('20241201');
  ```
* DML DELETE가 아닌, 세그먼트 자체를 떼어 버리는것이라 매우 빠르다.

#### 파티션 Truncate를 이용한 대량 데이터 삭제
* 파티셔닝 기준 컬럼 조건 외 또 다른 삭제 조건이 있는 경우가 있다.
```oracle
delete from 거래
where 거래일자 < '20250101'
  and (상태코드 <> 'ZZZ' or 상태코드 is null);
```
* 해당 조건을 만족하는 데이터가 대다수라면, 대량 데이터를 지울게 아닌 **남길 데이터만 백업하고 재입력하는 방식이 빠르다.**
1. **임시 테이블을 생성하고 남길 데이터만 복제**한다.
  ```oracle
  create table 거래_t
  as
  select *
  from 거래
  where 거래일자 < '20150101'
    and 상태코드 = 'ZZZ'
  ```
2. **삭제 대상 테이블 파티션을 Truncate** 한다.
  ```oracle
  alter table 거래 truncate partition p201412;
  -- 11g부터는 아래와 같이도 대상 지정 가능
  alter table 거래 truncate partition for ('20141201');
  ```
3. **임시 테이블에 복제해 둔 데이터를 원본 테이블에 입력**한다.
  ```oracle
  insert into 거래
  select * from 거래_t;
  ```
4. **임시 테이블을 Drop**한다.
  ```oracle
  drop table 거래_t;
  ```

* 서비스 중단 없이 파티션을 Drop 또는 Truncate 하려면 아래 조건을 모두 만족해야 한다.
  * **파티션 키와 커팅 기준 컬럼이 일치**해야 함
    * ex) 파티션 키와 커팅 기준 컬럼이 모두 `신청일자`
  * **파티션 단위와 커팅 단위가 일치**해야 함
    * ex) 월 단위 파티션을 월 단위로 데이터 커팅
  * **모든 인덱스가 로컬 파티션 인덱스**이어야 함 
    * 즉, **PK 인덱스 구성도 로컬 파티션**이어야 한다.
    * ex) 파티션 키는 `신청일자`, PK는 `신청일자 + 신청순번`
    * 따라서 **PK 구성 컬럼에 파티션키를 포함하는 설계가 중요**하다.
    * PK 인덱스는 삭제기준 컬럼이 인덱스 구성 컬럼이어야 로컬 파티셔닝이 가능하다.

### 파티션을 활용한 대량 INSERT 튜닝
#### 비파티션 테이블일 때
* 비파티션 테이블에 손익분기점을 넘는 대량 데이터를 INSERT 하려면 인덱스를 Unusable 시켰다가 재생성하는 방식이 더 빠를 수 있다.
1. **테이블을 nologging 모드로 전환**한다.
  ```oracle
  alter table target_t nologging; 
  ```
2. **인덱스를 Unusable 상태로 전환**한다.
  ```oracle
  alter index target_t_x01 unusable; 
  ```
3. **가능하다면 Direct Path Insert 방식으로 대량 데이터를 입력**한다.
  ```oracle
  insert /*+ append */ into target_t
  select * from source_t
  ```
4. **nologging 모드로 인덱스를 재생성**한다.
  ```oracle
  alter index target_t_x01 rebuild nologging; 
  ```
5. **logging 모드로 전환**한다.
  ```oracle
  alter table target_t logging;
  alter table target_t_x01 logging;
  ```

#### 파티션 테이블일 때
* 초대용량 인덱스를 재생성하는 부담이 만만치 않기 때문에, 시간이 더 오래 걸리더라도 실무에서는 웬만하면 인덱스를 그대로 둔 상태로 INSERT 한다.
* 만약 테이블이 파티셔닝되어있고, 인덱스도 다행이 로컬 파티션이라면 좋은 방법이 생긴다.
  * 파티션 단위로 인덱스를 재생성할 수 있다는 점을 이용한다.

1. **작업 대상 파티션 테이블을 nologging 모드로 전환**한다.
  ```oracle
  alter table target_t modify partition p_201712 nologging;
  ```
2. **작업대상 파티션 테이블과 매칭되는 파티션 인덱스를 Unusable 상태로 전환**한다.
  ```oracle
  alter index target_t_x01 modify partition p_201712 unusable;
  ```
3. **가능하다면 Direct Path Insert 방식으로 대량 데이터를 입력**한다.
  ```oracle
  insert /*+ append */ into target_t
  select * from source_t where dt between '20171201' and '20171231'
  ```
4. **nologging 모드로 인덱스 파티션을 재생성**한다.
  ```oracle
  alter index target_t_x01 rebuild partition p_201712 nologging;
  ```
5. **작업 파티션을 logging 모드로 전환**한다.
  ```oracle
  alter table target_t modify partition p_201712 logging;
  alter table target_t_x01 modify partition p_201712 logging;
  ```

## Lock과 트랜잭션 동시성 제어
### 오라클 Lock
* 오라클은 공유 리소스와 사용자 데이터를 보호할 목적으로 DML Lock, DDL Lock, 래치, 버퍼 Lock, 라이브러리 Lock/Pin 등 다양한 종류의 Lock을 사용한다.
* 애플리케이션 개발 측면에서 가장 중요하게 다루어야 할 Lock은 DML Lock이다.
* DML Lock은 다중 트랜잭션이 동시에 액세스하는 사용자 데이터의 무결성을 보호해준다.
* DML Lock에는 테이블 Lock과 로우 Lock이 있다.

#### DML 로우 Lock
* DML 로우 Lock은 두 개의 동시 트랜잭션이 같은 로우를 변경하는 것을 방지한다.
  * 하나의 로우를 변경하려면 로우 Lock을 먼저 설정해야 한다.
* 어떤 DBMS던지 DML 로우 Lock에는 배타적 모드를 사용하기 때문에, UPDATE 또는 DELETE를 진행중인 로우를 다른 트랜잭션이 UPDATE하거나 DELETE 할 수 없다.
* INSERT에 대한 로우 Lock 경합은 Unique 인덱스가 있을 때만 발생한다.
  * Unique 인덱스가 있는 상황에서 두 트랜잭션이 같은 값을 입력하려고 할 때, 블로킹이 발생한다.
  * 블로킹이 발생하면 후행 트랜잭션은 기다렸다가 선행 트랜잭션이 커밋하면 INSERT에 실패하고, 롤백하면 성공한다.
  * 두 트랜잭션이 서로 다른 값을 입력하거나 Unique 인덱스가 아예 없으면 INSERT에 대한 로우 Lock 경합은 발생하지 않는다.
* MVCC 모델을 사용하는 오라클은 SELECT 문에 로우 Lock을 사용하지 않는다.
  * 오라클은 다른 트랜잭션이 변경한 로우를 읽을 때 복사본 블록을 만들어 쿼리가 시작된 시점으로 되돌려서 읽는다.
  * 변경이 진행중인 로우를 읽을 때도 Lock이 풀릴 때까지 기다리지 않고 복사본을 만들어 읽는다.
  * 즉, SELECT 문에 Lock을 사용할 필요가 없다.
* **오라클에서는 DML과 SELECT는 서로 진행을 방해하지 않고, 물론 SELECT 끼리도 서로 방해하지 않는다.**
  * DML끼리는 서로 방해할 수 있는데 어떤 DBMS를 사용하더라도 마찬가지이다.
* MVCC 모델을 사용하지 않는 DBMS는 SELECT 문에 공유 Lock을 사용한다.
  * 공유 Lock끼리는 호환되며, 두 트랜잭션이 같이 Lock을 설정할 수 있다는 것이다.
  * 공유 Lock과 배타적 Lock은 호환되지 않기 때문에 DML과 SELECT가 서로 진행을 방해할 수 있다.
  * 다른 트랜잭션이 읽고 있는 로우를 변경하려면 다음 레코드로 이동할 떄까지 기다려야 하며, 다른 트랜잭션이 변경중인 로우를 읽으려면 커밋할 때까지 기다려야 한다.
* DML 로우 Lock에 의한 성능 저하를 방지하려면, 온라인 트랜잭션을 처리하는 주간에 Lock을 필요 이상으로 오래 유지하지 않도록 커밋 시점을 조절해야 한다.
  * Lock이 오래 지속되지 않도록 관련 SQL을 모두 튜닝해야 한다.

#### DML 테이블 Lock
* 오라클은 DML 로우 Lock을 설정하기 전, 테이블 Lock을 먼저 설정한다.
* 현재 트랜잭션이 갱신 중인 테이블 구조를 다른 트랜잭션이 변경하지 못하게 막기 위해서이다.
* 테이블 Lock을 TM Lock이라고 부르기도 한다.
* 오라클은 로우 Lock에 항상 배타적 모드를 사용하지만, 테이블 Lock에는 여러가지 Lock 모드를 사용한다.
  * RS(row share): 웬만하면 다 같이 써도 되는 제일 약한 락으로, X만 안 됨
  * RX(row exclusive): 일반 DML 모드. RS/RX끼리는 가능, S/SRX/X와는 충돌
  * S(share): 읽기 전용. S/RS끼리만 가능, 나머지(DML 포함)는 충돌
  * SRX(share row exclusive): 나만 쓰고, 남들은 읽기만 가능. 혼자 강력하게 잡는 용도
  * X(exclusive): 완전 독점, 아무도 같이 못 씀
* RS, RX간에는 DML문 수행 시 테이블 Lock에 의한 경합은 발생하지 않고, 같은 로우를 갱신하려고 할 때만 로우 Lock에 의한 경합이 발생한다. 
* DML을 수행하기 전 항상 테이블 Lock을 먼저 설정하는데, 자신이 해당 테이블에서 현재 어떤 작업을 수행 중인지 알리는 일종의 푯말이다.
* 테이블 Lock에는 여러 모드가 있기 때문에, 어떤 모드를 사용했는지에 따라 후행 트랜잭션이 수행할 수 있는 작업의 범위가 달라진다.
  * 예시로, DDL을 이용해 테이블 구조를 변경하려는 트랜잭션은 해당 테이블에 TM Lock이 설정되어있는지를 먼저 확인한다.
  * TM Lock을 RX모드로 설정한 트랜잭션이 하나라도 있으면 현재 테이블을 갱신중인 트랜잭션이 있다는 신호고, ORA-00054 메시지를 남기고 작업을 멈춘다.
  * DDL문이 먼저 수행 중일 때는, DML 문을 수행하려는 트랜잭션이 기다린다.

> 대상 리소스가 사용중일 때 진로 선택
> * Lock을 얻고자 하는 리소스가 사용 중일 때, 세 가지 방법 중 하나를 택한다.
> * 내부적으로 진로가 결정되어 있지만 사용자가 선택할 수 있는 경우도 있는데, 이 세 옵션을 모두 선택할 수 있는 문장이 SELECT FOR UPDATE문이다.
> 1. Lock이 해제될 때까지 기다린다. (`select * from t for update`)
> 2. 일정 시간만 기다리다 포기한다. (`select * from t for update wait 3`)
> 3. 기다리지 않고 작업을 포기한다. (`select * from t for update nowait`)
> * DML을 수행할 때 묵시적으로 테이블 Lock을 설정하는데, 이때는 1번을 선택한다.
> * Lock Table 명령을 이용해 명시적으로 테이블 Lock을 설정할 때도 기본적으로 기다리는 방법을 택하지만, `NOWAIT` 옵션을 이용해 바로 작업을 포기하도록 사용자가 지정할 수 있다.
>   * `lock table emp in exclusive mode NOWAIT`
> * DDL을 수행할 때도 내부적으로 테이블 Lock을 설정하는데, 이 때는 `NOWAIT` 옵션이 자동으로 지정된다.
>   * 오라클 11g부터 `ddl_lock_timeout` 파라미터를 0보다 크게 설정하면 설정한 시간(초단위) 만큼 기다리다 작업을 포기하게 할 수 있다.

#### Lock을 푸는 열쇠, 커밋
1) 블로킹(Blocking)
   * 블로킹은 선행 트랜잭션이 설정한 Lock 때문에 후행 트랜잭션이 작업을 진행하지 못하고 멈춰있는 상태를 의미한다.
   * 이것을 해소하는 방법은 커밋 뿐이다.
2) 교착상태(Deadlock)
   * 두 트랜잭션이 각각 특정 리소스에 Lock을 설정한 상태에서 맞은편 트랜잭션이 Lock을 설정한 리소스에 또 Lock을 설정하려고 진행하는 상황을 의미한다.
   * 교착상태가 발생하면 둘 중 하나가 뒤로 물러나지 않으면 영영 풀리지 않는다.
   * 오라클에서 교착상태가 발생하면 이를 먼저 인지한 트랜잭션이 문장 수준 롤백을 진행한 후 에러 메시지를 던지며, 교착상태를 발생시킨 문장 하나만 롤백한다.
     * `ORA-00060 deadlock detected while waiting for resource`
   * 교착상태는 해소되었지만 블로킹 상태에 놓이게 되고, 이 메시지를 받은 트랜잭션은 커밋이나 롤백을 결정해야만 한다.
* 오라클은 데이터를 읽을 때 Lock을 사용하지 않으므로 다른 DBMS에 비해 상대적으로 Lock 경합이 적게 발생한다.
* 읽는 트랜잭션의 진행을 막는 부담감이 없으므로, 필요한 만큼 트랜잭션을 충분히 길게 가져갈 수 있다.
* 하지만 트랜잭션이 너무 길면 트랜잭션을 롤백해야 할 때 너무 많은 시간이 걸려 고생할 수 있다.
  * Undo 세그먼트가 고갈되거나 Undo 세그먼트 경합을 유발할 수 있다.
* 따라서 DML Lock때문에 동시성이 저하되지 않도록 적절한 시점에 커밋해야 한다.
* 그렇다고 해서 커밋을 너무 자주 수행하면 서버 프로세스가 LGWR에 로그 버퍼를 비우도록 요청하고 동기 방식으로 기다리는 횟수가 늘기 때문에 성능이 느려진다.
* 잦은 커밋때문에 성능이 느리다면, 오라클 10gR2부터 제공하는 비동기식 커밋과 배치 커밋을 활용하는 방안을 검토할 수 있다.
  * WAIT(Default) : LGWR가 로그버퍼를 파일에 기록했다는 완료 메시지를 받을 때까지 기다린다. (동기식)
  * NOWAIT : LGWR의 완료 메시지를 기다리지 않고 바로 다음 트랜잭션을 진행한다. (비동기식)
  * IMMEDIATE(Default) : 커밋 명령을 받을 때마다 LGWR가 로그 버퍼를 파일에 기록한다.
  * BATCH : 세션 내부에 트랜잭션 데이터를 일정량 버퍼링 했다가 일괄 처리한다.
  ```oracle
  COMMIT WRITE IMMEDIATE WAIT;
  COMMIT WRITE BATCH NOWAIT;
  ```

### 트랜잭션 동시성 제어
1. 비관적 동시성 제어
   * 사용자들이 같은 데이터를 동시에 수정할 것으로 가정한다.
   * 한 사용자가 데이터를 읽는 시점에 Lock을 걸고 조회 또는 갱신처리가 완료될 때까지 이를 유지한다.
2. 낙관적 동시성 제어
   * 사용자들이 같은 데이터를 동시에 수정하지 않을 것으로 가정한다.
   * 데이터를 읽을 때 Lock을 설정하지 않는다.

#### 비관적 동시성 제어
```oracle
select 적립포인트, 방문횟수, 최근방문일시, 구매실적 from 고객
where 고객번호 = :cust_num;

-- 새로운 적립포인트 계산

update 고객 set 적립포인트 = :적립포인트 where 고객번호 = :cust_num
```
* 위 예시는 적립포인트를 계산하는 동안 다른 트랜잭션이 같은 고객의 실적정보를 변경하면 문제가 생길 수 있다.
* 이럴 경우 SELECT 문에 FOR UPDATE 사용하면 고객 레코드에 Lock을 설정하기 때문에 잘못 갱신되는 문제를 방지할 수 있다.
* FOR UPDATE에 WAIT 또는 NOWAIT 옵션을 사용하면 Lock을 얻기 위해 무한정 기다리지 않아도 된다.
  * `NOWAIT` : 대기없이 ORA-00054를 던짐
  * `WAIT 3` : 3초 대기 후  ORA-30006을 던짐
* 따라서 해당옵션을 사용하면 다른 트랜잭션에 의해 Lock이 걸렸을 때 Exception을 만나게 되므로 에러메시지를 출력하며 트랜잭션을 종료할 수 있다.

> skip locked
> * Lock이걸린 레코드는 생략하고 다음 레코드를 계속 읽도록 구현할 수 있다.
> ```oracle
> select cust_id, rcpt_amt from cust_rcpt_Q
> where yn_upd = 'Y' FOR UPDATE SKIP LOCKED
> ```


#### 낙관적 동시성 제어
```oracle
select 적립포인트, 방문횟수, 최근방문일시, 구매실적 into :a, :b, :c, :d
from 고객
where 고객번호 = :cust_num;

-- 새로운 적립포인트 계산

update 고객 set 적립포인트 = :적립포인트
where 고객번호 = :cust_num
  and 적립포인트 = :a
  and 방문횟수 = :b
  and 최근방문일시 = :c
  and 구매실적 = :d;

if sql%rowcount = 0 then
  alter('다른 사용자에 의해 변경되었습니다.');
end if;
```
* 위는 낙관정 동시성 제어 예시이다.
* 위 예시처럼 SELECT문에서 읽은 컬럼이 매우 많다면 UPDATE문에 조건절을 일일이 기술하는것은 매우 귀찮다.
* 만약 UPDATE 대상 테이블에 최종 변경일시를 관리하는 컬럼이 있다면 해당 레코드의 갱신여부를 더 간단히 판단할 수 있다.
```oracle
select 적립포인트, 방문횟수, 최근방문일시, 구매실적, 변경일시
into :a, :b, :c, :d, :mod_dt
from 고객
where 고객번호 = :cust_num;

-- 새로운 적립포인트 계산

update 고객 set 적립포인트 = :적립포인트, 변경일시 = SYSDATE
where 변경일시 = :mod_dt;

if sql%rowcount = 0 then
  alter('다른 사용자에 의해 변경되었습니다.');
end if;
```
* 낙관적 동시성 제어도 UPDATE전에 SELECT문을 한번 더 수행함으로 써 Lock에 대한 예외처리를 한다면 다른 트랜잭션이 설정한 Lock을 기다리지 않게 구현할 수 있다.
```oracle
select 고객번호
from 고객
where 고객번호 = :cust_num
  and 변경일시 = :mod_dt
for update nowait;
```

#### 동시성 제어 없는 낙관적 프로그래밍
* 낙관적 동시성 제어를 사용하면 Lock이 유지되는 시간이 매우 짧아지기 때문에 동시성을 높이는데 매우 유리하다.
* 하지만 다른 사용자가 같은 데이터를 변경했는지 검사하고 그에 따라 처리 방향성을 결정해야하는 귀찮은 절차가 필요하다.
* 무작정 낙관적으로 프로그래밍하지 않아야 하고, 특히나 중요한 로직에서는 동시성 제어를 제대로 해야 한다.

#### 데이터 품질과 동시성 향상을 위한 제언
* 데이터 품질인 굉장히 중요한데, 이를 위해 FOR UPDATE 사용을 두려워하지 말아야 한다.
* 다중 트랜잭션이 존재하는 데이터베이스 환경에서 공유 자원에 대한 액세스 직렬화는 필수다.
  * JAVA에서 멀티 쓰레드 프로그래밍 할때 `synchronized`키워드의 역할
* 데이터를 변경할 목적으로 읽는다면 당연히 Lock을 걸어야 한다.
* 따라서 FOR UPDATE가 필요한 상황이라면 정확하게 사용해야 하고, 코딩이 번거롭더라도 동시성이 나빠지지 않게 WAIT NOWAIT 옵션을 활용한 예외처리에 세심한 주의를 기울여야 한다.
* 낙관적, 비관적 동시성 제어를 같이 사용하는 방법도 있다.
  * 낙관적 동시성 제어를 시도 후 다른 트랜잭션에 의한 데이터가 변경된 사실이 발견되면 롤백하고 다시 시도 시 비관적 동시성 제어를 사용하는 방식이다.
* 불필요하게 Lock을 오래 유지하지 않고 트랜잭션의 원자성을 보장하는 범위 내에서 가급적 빨리 커밋해야 한다.
* 동시성을 향상하고자 할 때 SQL 튜닝은 기본이다.
  * 가장 효율적인 인덱스를 구성해주고, 데이터량에 맞는 조인 메소드를 선택해야 한다.
  * Lock에 대한 고민은 트랜잭션 내 모든 SQL을 완벽히 튜닝하고나서 해도 늦지 않다.

> 로우 Lock 대상 테이블 지정
> ```oracle
> select b.주문수량
> from 계좌마스터 a, 주문 b
> where a.고객번호 = :cust_no
>   and b.계좌번호 = a.계좌번호
>   and b.주문일자 = :ord_dt
> for update
> ```
> * 위와 같이 쿼리를 작성하면 양쪽 테이블 모두에 로우 Lock이 걸린다.
> * 아래와 같이 작성하면 `주문수량`이 있는 주문 테이블에만 로우 Lock이 걸린다.
> ```oracle
> select b.주문수량
> from 계좌마스터 a, 주문 b
> where a.고객번호 = :cust_no
>   and b.계좌번호 = a.계좌번호
>   and b.주문일자 = :ord_dt
> for update of b.주문수량
> ```

### 채번 방식에 따른 INSERT 성능 비교
* INSERT, UPDATE, DELETE, MERGE 중 가장 중요하고 튜닝 요소가 많은 것은 INSERT이다.
* 수행빈도도 높지만, 채번 방식에 따른 성능 차이가 매우 크다.

#### 채번 테이블
* 각 테이블 식별자의 단일컬럼 일련번호 또는 구분 속성별 순번을 채번하기 위해 별도 테이블을 관리하는 방식이다.
* 채번 레코드에서 읽어서 1을 더한 값으로 변경하고, 그 값을 새로운 레코드를 입력하는데 사용한다.
* 채번 레코드를 변경하는 과정에 자연스럽게 액세스 직렬화가 이루어지므로, 두 트랜잭션이 중복 값을 채번할 가능성을 원천적으로 방지해 준다.
1) 장점
   * 범용성이 좋다.
   * INSERT 과정에 중복 레코드 발생에 대비한 예외 처리에 크게 슨경쓰지 않다도 되므로, 채번 함수만 잘 정의하면 편리하게 사용할 수 있다.
   * INSERT 과정에 결번을 방지할 수 있다.
   * PK가 복합컬럼일 때도 사용할 수 있다.
2) 단점
   * 채번 레코드를 변경하기 위한 로우 Lock 경합으로 인해, 다른 채번방식에 비해 성능이 안좋다.
     * 동시 INSERT가 많으면 채번 레코드뿐만 아니라 채번 테이블 블록 자체에도 경합이 발생한다.
     * 다른 레코드를 변경하는 프로세스끼리도 경합을 할 수 있다. (버퍼 Lock과 ITL경합)

> 자율트랜잭션
> * 자율트랜잭션 기능을 이용하면, 메인 트랜잭션에 영향을 주지 않고 서브 트랜잭션에서 일부 자원만 Lock을 해제할 수 있다.
> * PL/SQL 선언부에 `paragma autonomous_transaction` 이라고 선언하기만 하면 된다.
> * PL/SQL 함수/프로시저를 자율 트랜잭션으로 선언하면, 그 내부에서 커밋을 수행해도 메인 트랜잭션은 커밋하지 않은 상태로 남는다.
> * 따라서 채번 테이블 로우 Lock은 해제한 상태이므로 다른 트랜잭션을 블록킹하지 않는다.

#### 시퀀스 오브젝트

1) 장점
    * 성능이 빠르다
    * INSERT 과정에 중복 레코드 발생에 대비한 예외처리에 신경쓰지 않아도 된다.
    * 테이블별로 시퀀스 오브젝트를 생성하고 관리하는 부담이 있지만 개발팀 입장에서는 사용하기 매우 편리하다.
    * 시퀀스 Lock에 의한 성능 이슈가 있지만, 캐시 사이즈를 적절히 설정하면 가장 빠른 성능을 제공한다.
    * 자율트랜잭션 기능이 기본적으로 구현되어 있다.

2) 단점
    * 기본적으로 PK가 단일컬럼일 때만 사용 가능하다.
      * 복합컬럼일 때에도 사용할 수는 있지만, 유일성을 보장하는 최소 컬럼으로 PK를 구성해야 한다는 최소성 요건을 위배하게 된다.
    * 신규 데이터를 입력하는 과정에 결번이 생길 수 있다. (업무적으로 문제가 생기진 않음)
      * 시퀀스 채번 이후 트랜잭션을 롤백하는 경우
      * CACHE 옵션을 설정한 시퀀스가 캐시에서 밀려나는 경우

> 시퀀스 Lock
> * 오라클이 시퀀스 오브젝트에 사용하는 Lock은 세가지가 있다.
> 1) 로우 캐시 Lock
>    * 딕셔너리 정보를 매번 디스크에서 읽고 쓰면 성능이 매우 느리기 때문에 오라클은 로우 캐시(딕셔너리 캐시)를 사용한다.
>    * 로우 캐시는 공유 캐시(SGA)의 구성요소이므로 정보를 읽고 쓸 때 액세스를 직렬화해야 한다.
>    * 이를 위해 사용하는 Lock이 로우 캐시 Lock이다.
>    * 이 로우 캐시 Lock을 사용하는 대표적인 오브젝트가 시퀀스이므로, 로우 캐시 Lock 경합이 나타날 수 있다.
>    * 시퀀스 채번으로 인한 로우 캐시 Lock 경합을 줄이기 위해 오라클은 기본적으로 CACHE 옵션을 사용한다.
>      * CACHE 옵션의 기본값은 20이고, 채번 빈도가 낮아 사용하고 싶지 않다면 NOCACHE 옵션을 지정하면 된다.
> 2) 시퀀스 캐시 Lock
>    * 시퀀스 캐시도 공유 캐시에 위치한다.
>    * 시퀀스 캐시에서 값을 얻을 때도 액세스 직렬화가 필요하며, 이것을 SQ Lock이라고 부른다.
> 3) SV Lock
>    * 시퀀스 캐시는 한 인스턴스 내에서 공유된다.
>    * nextval을 호출하는 순서대로 값을 제공하므로, 인스턴스 내에서는 번호 순서를 보장한다.
>    * 데이터베이스 하나에 인스턴스가 여러 개인 RAC 환경에서는 인스턴스마다 시퀀스 캐시를 따로 갖는다.
>    * 따라서 인스턴스 간 번호 순서를 기본적으로 보장하지 않는다.
>    * 업무적으로 순서를 보장해야 한다면, ORDER 옵션을 사용하면 된다.
>    * RAC 환경에서 ORDER 옵션을 사용하면 오라클은 SV Lock을 통해 시퀀스 캐시에 대한 액세스를 직렬화한다.
>      * RAC 각 노드는 네트워크 상에 서로 분리된 서버 이므로, 시퀀스 캐시를 네트워크를 통해 공유해 성능 문제가 있다.
>      * 그러나 순서를 보장해야 하는 경우, 다른 방식 (채번테이블 ,MAX + 1) 보다 성능은 빠르다. 

> 순환(cycle) 옵션을 가진 시퀀스 활용
> * PK가 복합컬럼인데 동시 트랜잭션이 높아 시퀀스가 꼭 필요하다면 순환 옵션을 가진 시퀀스 활용을 고려할 수 있다.
> * 하루에 도저히 도달할 수 없는 값으로 최댓값을 설정하고, 그 값에 도달하면 다시 1부터 시작하도록 순환옵션을 설정하는 것이다.
> * 일반적으로 권장할 솔루션은 아니지만, 채번 성능이 문제가 될 때 고려해볼 수 있다.

> 12c 시퀀스 신기능
> * 오라클 12.1 버전에서 시퀀스 관련 신기능이 많이 도입되었다.
> 1) 컬럼 기본값으로 시퀀스 지정
>    * 컬럼 기본값으로 시퀀스를 지정할 수 있는데, 만약 값을 입력하지 않으면 오라클이 시퀀스 nextval을 호출해 값을 입력한다.
>    ```oracle
>    create sequence my_seq;
>    
>    create table t (
>      c1 number default my_seq.nextval not not null
>    , c2 varchar2(5))
>    ```
> 2) IDENTITY 컬럼
>    * 특정 컬럼을 IDENTITY 컬럼으로 지정할 수 있다.
>    ```oracle
>    create table t (c1 number generated always as identity, c2 varchar2(5)); 
>    ``` 
>    * 사용자가 직접 값을 입력할 수도 있으려면 `GENERATED BY DEFAULT` 옵션을 지정하면 된다.
> 3) 세션 시퀀스 
>    * 글로벌 시퀀스는 여러 세션이 공유할 수 있는 시스템 레벨 시퀀스이다. (생성 시 `GLOBAL` 생략 가능)
>    ```oracle
>    create sequence g_seq GLOBAL;
>    ```
>    * 세션 시퀀스는 여러 세션이 공유할 수 없는 세션 레벨 시퀀스로 아래와 같이 생성한다.
>    ```oracle
>    create sequence s_seq SESSION 
>    ```
>    * 세션 시퀀스는 세션 내에서만 유효하고, 세션이 종료되면 초기화된다.
>    * Lock 메커니즘이 불필요하기 때문에 글로벌 시퀀스보다 성능이 좋다.

#### MAX + 1
* 테이블의 최종 일련번호를 조회하고, 거기에 1을 더해 INSERT하는 방식이다.

1) 장점
   * 시퀀스 또는 별도의 채번 테이블을 관리하는 부담이 없다.
   * 동시 트랜잭션에 의한 충돌이 많지 않으면 성능이 매우 빠르다.
   * PK가 복합컬럼인 경우에도 사용하기 좋다. (구분속성 별 순번을 채번)
2) 단점
   * 레코드 중복에 대비한 세밀한 예외처리가 필요하다.
   * 다중 트랜잭션에 의한 동시 채번이 심하면 시퀀스보다 성능이 많이 나빠질 수 있다.

#### 세가지 채번 방식 선택 기준
* 다중 트랜잭션에 의한 동시 채번이 많지 않으면 세 방식 중 어느것을 사용해도 크게 상관이 없다.
  * 채번 테이블이나 시퀀스 오브젝트 관리 부담을 고려한다면 MAX + 1 방식이 좋다.
* 다중 트랜잭션에 의한 동시 채번이 많고, PK가 단일컬럼 일련번호라면 시퀀스 방식이 가장 좋다.
* 다중 트랜잭션에 의한 동시 채번이 많고, PK 구분 속성에 값 종류 개수가 많으면 중복에 의한 로우 Lock 경합 및 재실행 가능성이 낮다.
  * 시퀀스보다 MAX + 1 방식이 구조적으로 좋다.
* 다중 트랜잭션에 의한 동시 채번이 많고 PK 구분 속성에 값 종류 개수가 적으면, MAX + 1 방식은 성능에 문제가 생길 수 있다.
  * 이럴 땐 순환 옵션을 가진 시퀀스 오브젝트 활용을 고려할 수 있다.

#### 시퀀스보다 좋은 솔루션
* 한 개 이상의 구분 속성과 함께 뒤쪽에 순번 대신 입력일시를 두는 방식으로 PK 구조를 설계하면 채번 또는 INSERT 과정에 생기는 Lock 이슈를 거의 해소할 수 있다.
* 채번 과정을 생략하고 `SYSDATE`, `SYSTIMESTAMP` 함수만 호출하면 되기 때문에 빠르고 간편하다.
  * 구분 속성에 값의 종류 개수가 많으면 입력일시에 DATE 타입을 써도 된다.
  * 값의 종류 개수가 적으면 TIMESTAMP 타입을 써야 할 수도 있다.
  * TIMESTAMP는 최대 소수점 9자리까지 가능하지만 OS 플랫폼에 따라 다르다.
  * 대부분 유닉스와 리눅스는 소수점 6자리까지 지원하고, MS 윈도우는 3자리까지만 지원하는데 이정도면 데이터 중복은 거의 발생하지 않는다.
* 적절한 데이터 타입을 선택하면 중복 가능성이 매우 희박하지만, 그래도 예외처리는 필요하다.
* 데이터 베이스에 데이터를 빠르게 입력하는것도 중요하지만, 빠르게 삭제하고 삭제한 공간을 시스템에 바로 반납할 수 있어야 좋은 시스템이 된다.
* 파티션과 관점에서 보면 이 솔루션은 아래 조건을 충족한다.
  * 서비스 중단없이 파티션 단위로 커팅하려면 기본적으로 PK 인덱스가 로컬 파티션이어야 한다.
  * PK 인덱스를 로컬 파티셔닝하려면 삭제 기준 컬럼이 PK에 포함되어 있어야 한다.
  * 삭제 기준은 대게 입력일시이다.

#### 인덱스 블록 경합
* INSERT 성능이 너무 빨라도 문제인데, 바로 인덱스 경합 때문이다.
* 인덱스 경합은 Right Growing 인덱스에서 가장 흔히 볼 수 있다.
* 인덱스에는 키순으로 정렬된 상태를 유지하며 값이 입력되는데, 일련번호나 입력일시/변경일시처럼 순차적으로 값이 증가하는 단일컬럼 인덱스는 항상 맨 우측 블록에만 데이터가 입력된다.
* 이러한 특징을 갖는 인덱스를 Right Growing 인덱스라고 부른다.
* Right Growing 인덱스에는 입력하는 값이 달라도 같은 블록을 갱신하려는 프로세스 간 버퍼 Lock 경합이 발생할 수 있다.
* 여러 프로세스에 의한 동시 INSERT가 많을 때 트랜잭션 성능을 떨어뜨리는 주범이며, 특히 RAC환경에서 심각한 성능 저하를 일으킨다.
* 구분속성을 앞에 두면 Right Growing 인덱스는 아니다.
* 그래도 동시성이 매우 높으면 인덱스 블록 경합은 생길 수 있다.
* 물론 구분속성의 값 종류 개수가 적을수록 경합도 심하다.
* 인덱스 블록 경합을 해소하는 가장 일반적인 방법은 인덱스를 해시 파티셔닝하는 것이다.
  * 인덱스를 해시 파티셔닝하면 값이 순차적으로 증가하더라도 해시 함수가 리턴한 값에 따라 서로 다른 파티션에 입력되므로 경합을 줄일 수 있다.
* 또한 인덱스를 리버스 키 인덱스로 전환하는 방법도 고려할 수 있다.

> 시퀀스 신기능 활용
> * 12c 시퀀스 신기능인 세션 시퀀스를 활용하면 일련번호에 대한 Right Growing 인덱스 성능 문제를 해결할 수 있다.
>   * 글로벌 시퀀스와 세션 시퀀스를 하나씩 만들고, 글로벌시퀀스 currval과 세션시퀀스 nextval을 조합한 값으로 INSERT하는 방식이다.
>   * 이렇게 하면 각 프로세스가 서로 다른 리프 블록에 값을 입력하므로 인덱스 경합이 발생하지 않는다.
> * 최근 공개된 오라클 18c 버전에서는 Scalable 시퀀스를 이용할 수 있다.
>   * 시퀀스 생성 시 `SCALE` 또는 `SCALE EXTEND`옵션을 지정하면 된다.
>   * Scalable 시퀀스에서 nextval을 호출하면 인스턴스 번호, 세션ID, 시퀀스 번호를 조합한 번호를 반환한다.
>   * EXTEND 옵션을 생략하면 리딩제로 없는 숫자를 반환한다.

# SQL 옵티마이저
## 통계정보와 비용 계산 원리
### 선택도와 카디널리티
* 선택도란 전체 레코드 중, 조건절에 의해 선택되는 레코드 비율을 말한다.
* 가장 단순한 `=` 조건으로 검색할 때의 선택도만 살펴보면, 컬럼 값 종류 개수(Number of Distinct Values, NDV)를 이용해 아래와 같이 구한다.
  ```
  선택도 = 1/NDV
  ```
* 카디널리티란, 전체 레코드 중 조건절에 의해 선택되는 레코드 개수이며, 아래 공식으로 구한다.
  ```
  카디널리티 = 총 로우 수 * 선택도 = 총 로우 수 / NDV
  ```
* 옵티마이저는 카디널리티를 구하고, 그 만큼의 데이터를 액세스하는 데 드는 비용을 계산해 테이블 액세스 방식, 조인 순서, 조인 방식 등을 결정한다.
* 비용을 계산하는 출발점은 선택도이다.
* 선택도를 계산할 때 NDV를 사용하므로, 통계정보 수집 과정에서 이 값을 정확히 구하는 것이 매우 중요하다.
* 통계정보 수집주기, 샘플링 비율 등을 잘 결정해야 하는 이유이다.

### 통계정보
* 통계정보의 종류는 아래와 같이 나뉜다.
  * 오브젝트 통계
    * 테이블 통계
    * 인덱스 통계
    * 컬럼 통계
  * 시스템 통계

#### 테이블 통계
* 테이블 통계를 수집하는 명령어는 다음과 같다.
```oracle
begin
  dbms_stats.gather_table_stats('scott', 'emp');
end;
```
* 수집된 테이블 통계 정보는 아래와 같이 조회할 수 있고, all_tab_statistics 뷰에서도 같은 정보를 확인할 수 있다.
  ```oracle
  select num_rows, blocks, avg_row_len, sample_size, last_analyzed
  from all_tables
  where owner = 'SCOTT'
    and table_name = 'EMP';
  ```
  * NUM_ROWS : 테이블에 저장된 총 레코드 개수
  * BLOCKS : 테이블 블록 수 = 사용된 익스텐트에 속한 총 블록수
    * 테이블에 할당된 총 블록 수는 dba_segments 또는 user_segments 뷰에서 확인 가능
  * AVG_ROW_LEN : 레코드당 평균 길이(Bytes)
  * SAMPLE_SIZE : 샘플링한 레코드 수
  * LAST_ANALYZED : 통계정보 수집일시

#### 인덱스 통계
* 인덱스 통계를 수집하는 명령어는 다음과 같다.
```oracle
-- 인덱스 통계만 수집
begin
  dbms_stats.gather_index_stats( ownname => 'scott', indname => 'emp_x01');
end;

-- 테이블 통계를 수집하면서 인덱스 통계도 같이 수집
begin
  dbms_stats.gather_table_stats('scott', 'emp', cascade=>true);
end;
```
* 수집된 인덱스 통계 정보는 아래와 같이 조회할 수 있고, all_ind_statistics 뷰에서도 같은 정보를 확인할 수 있다.
  ```oracle
  select blevel, leaf_blocks, num_rows, distinct_keys
       , avg_leaf_blocks_per_key, avg_data_blocks_per_key, clustering_factor
       , sample_size, last_analyzed
  from all_indexes
  where owner = 'SCOTT'
    and table_name = 'EMP'
    and index_name = 'EMP_X01'; 
  ```
  * BLEVEL : 브랜치 레벨의 약자로, 인덱스 루트에서 리프 블록에 도달하기 직전까지 읽게되는 블록 수 (인덱스 수직적 탐색 비용 계산)
  * LEAF_BLOCKS : 인덱스 리프 블록 총 개수 (인덱스 수평적 탐색 비용 계산)
  * NUM_ROWS : 인덱스에 저장된 레코드 개수 (인덱스 수평적 탐색 비용 계산)
  * DISTINCT_KEYS : 인덱스 키 값의 조합으로 만들어지는 값의 종류 개수 (인덱스 수평적 탐색 비용 계산)
    * 예시로, C1 + C2로 구성한 인덱스에서 C1 컬럼이 3개, C2 컬럼이 4개 값이 있으면 최대 12개 값의 종류가 만들어 질텐데, 인덱스에 저장된 데이터 기준으로 실제 입력된 값의 종류 개수를 구해놓은 수치
    * 인덱스 키 값을 모두 `=` 조건으로 조회할 때의 선택도를 계산하는 데 사용
  * AVG_LEAF_BLOCKS_PER_KEY : 인덱스 키값을 모두 `=` 조건으로 조회할 때 읽게 될 리프 블록 개수 (인덱스 수평적 탐색 비용 계산)
  * AVG_DATA_BLOCKS_PER_KEY : 인덱스 키값을 모두 `=` 조건으로 조회할 때 읽게 될 테이블 블록 개수 (테이블 액세스 비용 계산)
  * CLUSTERING_FACTOR : 인덱스 키값 기준으로 테이블 데이터가 모여 있는 정도 (테이블 액세스 비용 계산)
    * 인덱스 전체 레코드를 스캔하면서 테이블 레코드를 찾아 갈 때 읽게 될 테이블 블록 개수를 미리 계산해 놓은 수

#### 컬럼 통계
* 컬럼 통계는 테이블 통계 수집할 때 함께 수집된다.
* 수집된 컬럼 통계정보는 아래와 같이 조회할 수 있고, all_tab_col_statistics 뷰에서도 같은 정보를 확인할 수 있다.
  ```oracle
  select num_distinct, density, avg_col_len, low_value, high_value, num_nulls
       , last_analyzed, sample_size
  from all_tab_columns
  where owner = 'SCOTT'
    and table_name = 'EMP'
    and column_name = 'DEPTNO'
  ```
  * NUM_DISTINCT : 컬럼 값의 종류 개수(NDV)
  * DENSITY : `=` 조건으로 검색할 때의 선택도를 미리 구해 놓은 값
    * 히스토그램이 없거나, 있더라도 100% 균일한 분포를 갖는다면, 1 / NUM_DISTINCT 값과 일치
  * AVG_COL_LEN : 컬럼 평균 길이(Bytes)
  * LOW_VALUE : 최솟값
  * HIGH_VALUE : 최댓값
  * NUM_NULLS : 값이 NULL인 레코드 수

##### 컬럼 히스토그램
* `=` 조건에 대한 선택도는 `1 / NUM_DISTINCT` 공식으로 구하거나, 미리 구해놓은 `DENSITY` 값을 이용하면 된다.
* 일반적인 컬럼에는 이 공식이 비교적 잘 맞지만, 데이터 분포가 균일하지 않은 컬럼에는 그렇지 못한다.
* 그래서 옵티마이저는 일반적인 컬럼 통계 외에 히스토그램을 추가로 활용한다.
* 오라클 12c에서 사용하는 히스토그램 유형으로는 4가지가 있다.
  * 도수분포(FREQUENCY) : 값 별로 빈도수 저장
  * 높이균형(HEIGHT-BALANCED) : 각 버킷의 높이가 동일하도록 데이터 분포 관리
  * 상위도수분포(TOP-PREQUENCY) : 많은 레코드를 가진 상위 n 개 값에 대한 빈도수 저장(12c 이상)
  * 하이브리드(HYBRID) : 도수분포와 높이 균형 히스토그램의 특성 결합 (12c 이상)
* 히스토그램을 수집하려면 테이블 통계 수집 시 아래와 같이 `method_opt` 파라미터를 지정하면 된다.
```oracle
begin
  dbms_stats.gather_table_stats('scott', 'emp'
       , cascade=>false, method_opt=>'for columns ename size 10, deptno size 4');
end;

begin
    dbms_stats.gather_table_stats('scott', 'emp'
        , cascade=>false, method_opt=>'for all columns size 75');
end;

begin
    dbms_stats.gather_table_stats('scott', 'emp'
        , cascade=>false, method_opt=>'for all columns size auto');
end;
```
* 수집된 컬럼 히스토그램은 아래와 같이 조회할 수 있고, all_tabl_histograms 뷰에서도 같은 정보를 확인할 수 있다.
  ```oracle
  select endpoint_value, endpoint_number
  from all_histograms
  where owner = 'SCOTT'
    and table_name = 'EMP'
    and column_name = 'DEPTNO'
  order by endpoint_value;
  ```
  ```
  ENDPOINT_VALUE ENDPOINT_NUMBER
  -------------- ---------------
              10               3
              20               8
              30              14
  ```
  * 출력되는 결과값은 히스토그램 유형에 따라 해석하는 방법이 다르다.

#### 시스템 통계
* 시스템 통계는 애플리케이션 및 하드웨어 성능 특성을 측정한 것이며, 아래 항목들을 포함한다.
  * CPU 속도
  * 평균적인 Single Block I/O 속도
  * 평균적인 Multiblock I/O 속도
  * 평균적인 Multiblock I/O 개수
  * I/O 서브시스템의 최대 처리량(Throughput)
  * 병렬 Slave의 평균적인 처리량(Throughput)
* 시스템 통계는 아래와 같이 `sys.aux_stats$` 뷰에서 조회할 수 있다.
  ```oracle
  select sname, pname, pval1, pval2 from sys.aux_stats$;
  ```

### 비용 계산 원리
* 조인까지 포함한 모든 비용 계산 원리를 알아야 할 이유는 없다.
* 하지만 옵티마이저가 통계정보를 어떤 식으로 활용하는지는 이해할 필요가 있다. 
* 단일 테이블을 인덱스로 액세스할 때의 비용 계산 원리는 아래와 같다.
  * 인덱스 키 값을 모두 `=` 조건으로 검색할 때는 인덱스 통계만으로도 쉽게 비용을 계산할 수 있다.
    ```
    비용 = BLEVEL                     -- 인덱스 수직적 탐색 비용
        + AVG_LEAF_BLOCKS_PER_KEY    -- 인덱스 수평적 탐색 비용
        + AVG_DATA_BLOCKS_PER_KEY    -- 테이블 랜덤 액세스 비용
    ```
  * 인덱스 키값이 모두 `=`조건이 아닐 때는 아래와 같이 컬럼 통계까지 활용한다.
    ```
    비용 = BLEVEL                                 -- 인덱스 수직적 탐색 비용
        + LEAF_BLOCKS * 유효 인덱스 선택도          -- 인덱스 수평적 탐색 비용
        + CLUSTERING_FACTOR * 유효 테이블 선택도    -- 테이블 랜덤 액세스 비용
    ```
    * 유효 인덱스 선택도 : 전체 인덱스 레코드 중 액세스 조건에 의해 선택될 것으로 예상되는 레코드 비중(%)
    * 유효 테이블 선택도 : 전체 인덱스 레코드 중 인덱스 컬럼에 대한 모든 조건절에 의해 선택될 것으로 예상되는 레코드 비중(%)

> 비용(Cost)의 정확한 의미
> * 위에 비용 계산식은 I/O 비용 모델 기준이다.
> * I/O 비용 모델을 사용할 때 실행계획에 나타나는 Cost는 예상 I/O Call 횟수를 의미한다.
> * 최신 CPU 비용 모델에서 Cost는 Single Block I/O를 기준으로 한 상대적 시간을 표현한다.
>   * Cost가 100으로 표시된다면 Single Block I/O를 100번 하는 정도의 시간으로 해석 (상대적 시간 개념)
> * CPU 비용 모델을 개발한 이유는 같은 실행계획으로 같은 양의 데이터를 읽어도 애플리케이션 및 하드웨어 성능 특성에 따라 절대 소요시간이 다를 수 있어서이다.
> * 똑같이 I/O Call을 해도 그것이 Single Block I/O인지 Multiblock I/O인지에 따라 속도가 다르고, 시스템마다 Single Block I/O, Multiblock I/O 속도도 다르다. 

## 옵티마이저에 대한 이해
### 옵티마이저 종류
* 비용기반 옵티마이저(CBO)는 사용자 쿼리를 위해 후보군이 될만한 실행계획들을 도출하고, 데이터 딕셔너리에 미리 수집해 둔 통계정보를 이용해 각 실행계획의 예상비용을 산정하고, 그 중 가장 낮은 비용의 실행계획 하나를 선택하는 옵티마이저이다.
  * CBO가 사용하는 통계 정보로는 데이터량, 컬럼 값의 수, 컬럼 값 분포, 인덱스 높이, 클러스터링 팩터 등이 있다.
* 규칙기반 옵티마이저(RBO)는 각 액세스 경로에 대한 우선순위 규칙에 따라 실행계획을 만드는 옵티마이저이다.
  * 데이터 특성을 나타내는 통계정보를 전혀 활용하지 않고 단순한 규칙에만 의존하기 때문에 대량 데이터를 처리하는데 부적합하다.
  * RBO 규칙도 어느정도는 사용하기에 큰 무리가 없던 시절이 있었지만, 지금과 같은 대용량 데이터베이스 환경에서는 전혀 대안이 될 수 없다.

### 옵티마이저 모드
* 비용기반 옵티마이저에는 최적화 목표를 설정하는 기능으로서, 아래 세가지 모드가 있다.
  * ALL_ROWS : 전체 처리속도 최적화
    * 옵티마이저는 전체를 읽는 것을 전제로 시스템 리소스를 가장 적게 사용하는 실행계획을 선택한다.
  * FIRST_ROWS : 최초 응답속도 최적화
    * 최초 응답속도 최적화가 목표로, ALL_ROWS와 비교하면 Table Full Scan보다 인덱스를 더 많이 선택하고, 해시조인이나 소트머지조인보다 NL조인을 더 많이 선택한다.
    * 하지만 FIRST_ROWS_N으로 인해 사라지게 될 옵티마이저 모드이다.
  * FIRST_ROWS_N : 최초 N건 응답속도 최적화
    * 앞쪽 N개 로우만 읽고 멈추는 것을 전제로 응답속도가 가장 빠른 실행계획을 선택한다.
    * alter system 또는 alter session 명령어로 옵티마이저 모드를 설정할 때 N으로 지정할 수 있는 값은 1, 10, 100, 1000 네가지이다.
      ```oracle
      alter session set optimizer_mode = first_rows_1;
      ```
    * 힌트로 설정할 때에는 0보다 큰 어떤 정수값이라도 입력할 수 있다.
      ```oracle
      select /*+ first_rows(30 */ col1, col2 from t where ...
      ```
    * FIRST_ROWS와 달리 읽을 데이터 건수를 지정하였으므로, 더 정확한 비용 산정이 가능하다. (더 완벽한 CBO모드로 작동 함)

### 옵티마이저에 영향을 미치는 요소
#### SQL과 연산자 형태
* 결과가 같더라도 SQL을 어떤 형태로 작성했는지, 어떤 연산자를 사용했는지에 따라 옵티마이저가 다른 선택을 할 수 있으며 궁극적으로 쿼리 성능에 영향을 미친다.

#### 인덱스, IOT, 클러스터, 파티션, MV 등 옵티마이징 팩터
* 쿼리를 똑같이 작성해도 인덱스, IOT, 클러스터, 파티션, MV 등을 구성했는지, 어떤식으로 구성했는지에 따라 실행계획과 성능이 크게 달라진다.

#### 제약 설정
* DBMS에 설정한 PK, FK, Check, Not Null 같은 제약들은 데이터 무결성을 보장해 줄뿐만 아니라, 옵티마이저가 쿼리 성능을 최적화하는데 매우 중요한 메타 정보로 활용된다.

#### 통계정보
* 통계정보는 옵티마이저에 매우 강력한 영향을 미친다.
* 잘 작동하던 프로그램이 어느 날 갑자기 느려졌다면 대부분은 통계정보가 원인이다.
  * 특정 테이블 통계정보를 갑자기 삭제한다.
  * 대량 데이터를 지웠다가 다시 입력하기 전, 데이터가 없는 상태에서 자동으로 통계정보가 수집된다.
  * 3년간 갱신하지 않던 특정 테이블 통계정보를 어느날 갑자기 재수집한다. 
  * 통계정보 없이 관리하던 테이블에 인덱스를 재생성한다.
  * 테이블이나 인덱스를 재생성하면서 파티션 단위로만 통계정보를 수집한다.

#### 옵티마이저 힌트
* 옵티마이저에게 가장 절대적인 영향을 미치는 요소는 힌트다.
* 옵티마이저는 힌트를 명령어로 인식하고 그대로 따른다.
* 힌트가 잘 작동하지 않으면 아래 경우에 해당 할 가능성이 높다.
  * 문법적으로 맞지 않게 힌트를 기술
  * 잘못된 참조 사용
  * 의미적으로 맞지 않게 힌트를 기술
  * 논리적으로 불가능한 액세스 경로
  * 버그

#### 옵티마이저 관련 파라미터
* SQL, 데이터, 통계정보, 하드웨어 등 모든 환경이 같은데도 오라클 버전을 업그레이드하면 옵티마이저가 다르게 작동할 수 있다.
* 옵티마이저의 그런 행동 변화는 대개 파라미터 추가 또는 기본값 변경에 기인한다.
* 옵티마이저 모드 외, 옵티마이저 행동에 영향을 미치는 파라미터 목록은 아래와 같이 확인할 수 있다.
  ```oracle
  select name, value, isdefault, default_value
  from v$sys_optimizer_env
  ```
  * 종종 변경이 필요한 공식 파라미터 위주로 보여주며, 수많은 Hidden 파라미터 중에서는 관리자가 기본값을 변경한 것만 보여준다.

### 옵티마이저의 한계
* DBA가 통계정보를 아무리 잘 수집하고 개발자가 SQL을 아무리 잘 작성해도, 옵티마이저는 실수가 있기 마련이다.
* 아무리 기술이 발전해도 옵티마이저도 사람이 만든 소프트웨어이기 때문에 어쩔 수 없다.
* 옵티마이저에 가장 큰 영향을 미치는 통계정보 또한 필요한 만큼 충분히 확보하는 것부터가 불가능하다.
* 통계정보를 완벽하게 수집한다고 해도, 바인드 변수를 사용한 SQL에 컬럼 히스토그램을 활용할 수는 없다.

### 개발자의 역할
* 불완전한 옵티마이저에만 의존하지 말고, 개발자 스스로가 옵티마이저가 되어야 한다.
* 능력이 없어 옵티마이저에게 맡기는게 아닌 바빠서 맡긴다는 마인드를 갖고 있어야 한다.
* SQL 성능을 높이기 위해 개발자는 아래와 같은 일을 해야 한다.
  * 필요한 최소 블록만 읽도록 쿼리를 작성한다.
  * 최적의 옵타미이징 팩터를 제공한다.
  * 필요시, 옵티마이저 힌트를 사용해 최적의 액세스 경로로 유도한다.

#### 필요한 최소 블록만 읽도록 쿼리 작성
* 데이터베이스 성능은 I/O 효율에 달려있으므로, 동일한 레코드를 반복적으로 읽지 않고 필요한 최소 블록만 읽도록 해야 한다.
* 예시로, 페이징 쿼리에서 함수는 페이징 후의 결과에서 사용하도록 해야 한다.

#### 최적의 옵티마이징 팩터 제공
1) 전략적인 인덱스 구성
   * 전략적인 인덱스 구성은 옵티마이저를 돕는 가장 기본적인 옵티마이징 팩터이다.
   * 어떤 테이블에 어떤 조건으로 자주 액세스 하는지는 개발자가 더 잘 알기 때문에, 인덱스를 전략적으로 구성해 줄 책임은 DBA가 아닌 개발팀에 있다.
2) DBMS가 제공하는 다양한 기능 활용
   * 파티션, 클러스터, IOT, MV, Result Cache 등 DBMS가 제공하는 기능들을 적극적으로 활용해야 한다.
3) 옵티마이저 모드 설정
   * 전략과 목표가 중요하기 때문에, 옵티마이저 모드는 중요하다.
4) 정확하고 안정적인 통계정보

* 옵티마이저 모드를 포함해 각종 파라미터를 적절한 값으로 설정하고, 통계정보를 잘 수집해줘야 한다.
* 전략적인 인덱스 구성이 필수적으로 뒷받침 되어야 하며, DBMS가 제공하는 기능을 적극적으로 활용해 옵티마이저가 최적의 선택을 할 수 있도록 다양한 수단을 제공해주어야 한다. 

#### 필요시, 옵티마이저 힌트를 사용해 최적의 액세스 경로로 유도
* 옵티마이저는 생각만큼 완벽하지 않다.
* 따라서 옵티마이저가 최적의 실행계획을 수립하지 못할 때 개발자가 힌트를 이용해 직접 데이터 액세스 경로를 선택해 줄 수 있어야 한다.
* 이미 최적으로 실행되고 있더라도, 절대 다른 방식으로 바뀌지 않게 실행계획을 고정해야 하는 상황도 있다.

### 데이터베이스 튜닝
* 파일에 데이터를 읽고 쓰는 소프트웨어 애플리케이션인, DBMS의 성능 튜닝이라는 표현이 더 정확하다.
* SQL이 병목이나 지연 없이 빠르고 안정적으로 수행되도록 조치하는 모든 활동을 데이터베이스 튜닝이라고 할 수 있다.
  * SQL 튜닝 : I/O 효율화, DB Call 최소화, SQL 파싱 최소화 등
  * DB 설계 : 논리적 데이터 구조 설계, 물리적 저장 구조 설계 등
  * 인스턴스 튜닝 : Lock/Latch 모니터링 및 해소, 메모리 설정, 프로세스 설정 등
* 좋은 소리를 듣기 위해서는 소스와 공간이 중요하듯이, 데이터베이스에는 옵티마이저가 효율적으로 처리할 수 있게 작성한 SQL과 효과적인 데이터 구조가 중요하다.
* SQL 튜닝팀과 DBA팀은 알아야할 부분이 다르다
  1) DBA
     * 데이터베이스 자체에 대한 연구가 중요하다.
     * 데이터베이스 설치, 백업/복구, 오브젝트 생성/변경, 보안 등의 기본적인 기술을 익혀야 한다.
     * 데이터베이스 가키텍처를 완벽히 숙지해야 하고, 운영하면서 생기는 여러 장애 상황을 모니터링하고 해결하는 기술력이 있어야 한다.
  2) SQL 튜닝 팀
     * SQL 중심으로 공부해야 한다.
     * 옵티마이저가 SQL을 파싱하고 통계정보를 활용해 실행계획을 생성하는 원리
     * 옵티마이저가 쿼리변환 원리를 바탕으로 실행계획을 분석하는 방법
     * 옵티마이저 힌트를 이용해 실행계획을 제어하는 방법
     * 옵티마이저가 좋은 실행계획을 생성하도록 유도하기 위한 효과적인 SQL 작성법
     * 애플리케이션에서 SQL을 실행할 때 사용하는 프로그래밍 인터페이스
     * SQL을 빠르게 처리할 수 있는 좋은 데이터 구조와 파티션/인덱스 설계
     * 정확성과 안정성을 확보할 수 있는 통계정보 수집 정책
* 데이터베이스 튜닝은 SQL이 빠르고 안정적으로 수행되도록 조치하는 모든 활동을 의미하는데, SQL과 친숙해지지 않고 데이터베이스 튜닝을 하겠다는 생각은 버려야 한다.
* 즉, 데이터베이스 튜닝을 하려면 SQL과 좋은 데이터 구조를 설계하는 방법에 관심을 두고 공부해야 한다.

